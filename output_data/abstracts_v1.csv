articleID,year,journal,title_original,Title,abstract
100056,1990,international journal of industrial organization,cost overruns in long term projects,cost overruns in long term projects,"To evaluate the comparative efficacy and cost-effectiveness of various antihypertensive medications in persons aged 35 through 64 years with diastolic blood pressure of 95 mm Hg or greater and no known coronary heart disease, we used the Coronary Heart Disease Policy Model, which is a computer simulation of overall mortality as well as the mortality, morbidity, and cost of coronary heart disease in the US population. From the pooled literature, we estimated the antihypertensive and total cholesterol effects of various antihypertensive regimens. For 20 years of simulated therapy from 1990 through 2010, the cost per year of life saved was projected to be $10,900 for propranolol hydrochloride; $16,400 for hydrochlorothiazide; $31,600 for nifedipine; $61,900 for prazosin hydrochloride; and $72,100 for captopril. Doubling the cholesterol effects of the agents under study did not significantly change their effectiveness because, in general, lowering diastolic blood pressure by 1 mm Hg was equivalent to lowering the cholesterol level by 6%. Although any projection requires multiple estimates, each of which may be open to debate, propranolol appears to be the preferred initial option under most of a variety of alternative assumptions. Collapse"
100185,1990,journal of agricultural economics,cereal supply policy instruments:  an attitudinal survey among farmers in england,cereal supply policy instruments an attitudinal survey among farmers in england,ERR
100200,1990,journal of agricultural economics,value-adding activity and employment in the beef and sheep processing sector in northern ireland,value adding activity and employment in the beef and sheep processing sector in northern ireland,ERR
100218,1990,journal of the american statistical association,error models for official mortality forecasts,error models for official mortality forecasts,"""The Office of the Actuary, U.S. Social Security Administration, produces alternative forecasts of mortality to reflect uncertainty about the future.... In this article we identify the components and assumptions of the official forecasts and approximate them by stochastic parametric models. We estimate parameters of the models from past data, derive statistical intervals for the forecasts, and compare them with the official high-low intervals. We use the models to evaluate the forecasts rather than to develop different predictions of the future. Analysis of data from 1972 to 1985 shows that the official intervals for mortality forecasts for males or females aged 45-70 have approximately a 95% chance of including the true mortality rate in any year. For other ages the chances are much less than 95%."" Collapse"
100219,1990,journal of the american statistical association,density estimation with confidence sets exemplified by superclusters and voids in the galaxies,density estimation with confidence sets exemplified by superclusters and voids in the galaxies,"Abstract A method is presented for forming both a point estimate and a confidence set of semiparametric densities. The final product is a three-dimensional figure that displays a selection of density estimates for a plausible range of smoothing parameters. The boundaries of the smoothing parameter are determined by a nonparametric goodness-of-fit test that is based on the sample spacings. For each value of the smoothing parameter our estimator is selected by choosing the normal mixture that maximizes a function of the sample spacings. A point estimate is selected from this confidence set by using the method of cross-validation. An algorithm to find the mixing distribution that maximizes the spacings functional is presented. These methods are illustrated with a data set from the astronomy literature. The measurements are velocities at which galaxies in the Corona Borealis region are moving away from our galaxy. If the galaxies are clustered, the velocity density will be multimodal, with clusters correspond... Collapse"
100226,1990,journal of the american statistical association,fast evaluation of the distribution of the durbin-watson and other invariant test statistics in time series regression,fast evaluation of the distribution of the durbin watson and other invariant test statistics in time series regression,"Abstract A method is given for evaluating p values in O(n) operations for a general class of invariant test statistics that can be expressed as the ratio of quadratic forms in time series regression residuals. The best known of these is the Durbin-Watson statistic, although several others have been discussed in the literature. The method is numerically exact in the sense that the user specifies the error tolerance at the outset. As with existing exact methods, the problem is reexpressed in terms of the distribution function of a single quadratic form in independent normals, which is evaluated by numerically inverting its characteristic function. Existing methods, however, calculate the characteristic function by reducing the matrix defining the quadratic form to either eigenvalue or tridiagonal form, each of which requires O(n 3) operations for sample size n, whereas the new method uses a modification of the Kalman filter to do it in O(n) operations. Moreover, the new method has minimal storage requiremen... Collapse"
100227,1990,journal of the american statistical association,on the behavior of randomization tests without a group invariance assumption,on the behavior of randomization tests without a group invariance assumption,"Abstract Fisher's randomization construction of hypothesis tests is a powerful tool to yield tests that are nonparametric in nature in that their level is exactly equal to the nominal level in finite samples over a wide range of distributional assumptions. For example, the usual permutation t test to test equality of means is valid without a normality assumption of the underlying populations. On the other hand, Fisher's randomization construction is not applicable in this example unless the underlying populations differ only in location. In general, the basis for the randomization construction is invariance of the probability distribution of the data under a transformation group. It is the goal of this article to understand the robustness properties of randomization tests by studying their asymptotic validity in situations where the basis for their construction breaks down. Here, asymptotic validity refers to whether the probability of a Type I error tends asymptotically to the nominal level. In particula... Collapse"
100228,1990,journal of the american statistical association,construction of permutation tests,construction of permutation tests,"There is a dearth of known donors for salt tolerance traits in rice breeding programs other than Pokkali and Nona Bokra. These landraces are photoperiod sensitive and flower once a year, so that they are poorly compatible with breeding cultivars. Bangladeshi coastal landraces have been found to be biodiverse and salt tolerant accessions are still widely cultivated by farmers. These landraces may be potential donors for novel salt tolerance traits. One of these coastal varieties, Boilam, is both early-maturing as well as photoperiod insensitive and popularly grown as an Aus variety in Noakhali, in the southeast of Bangladesh. Boilam was therefore characterized and analyzed for QTLs linked to salt stress tolerance in a backcross population with a modern Aus variety BRRI dhan27. In order to identify and map salinity tolerance traits, a backcross population was generated between Boilam, and BRRI dhan27, a high yielding Aus variety. A total of 94 polymorphic SSR markers were used to construct a linkage map. 96 BC2F2 progeny were selected from both extremes of the population of 200 at the seedling stage with respect to their response to salinity stress at 12 dS/m. Composite interval mapping (CIM) using the linkage map revealed that the salinity tolerance loci were located on chromosomes 1, 4, 9 and 12. The identified QTLs were above the threshold LOD of 3.0 with significant R values ranging from 18.10% 22.40% as expression of phenotypic variance. Similar results were obtained using single marker regression (SMR), simple interval mapping (SIM) as well as composite interval mapping (ICIM) by QGene v-4.0. Both phenotypic and genotypic data revealed that Boilam can be used as a novel salt tolerance donor at seeding stage. Tolerant BC2F3 progenies having the background genotype of BRRI dhan27 will be developed further for release or use as parents with the help of the markers-linked to the salt tolerance QTLs for rice breeding programs aimed at increasing tolerance of rice to salt stress. -------------------------------------------------------------------------------------------------------------------------------------Date of Submission: 06-07-2017 Date of acceptance: 18-11-2017 --------------------------------------------------------------------------------------------------------------------------------------I. Background Soil salinity is a constraint to rice production in both inland irrigated areas as well as coastal regions. Despite its high sensitivity to salinity, considerable variation in tolerance was observed in rice (Akbar et. al.1972). Salinity affects rice growth in varying degrees at all stages starting from germination to maturation. The genetic complexity of salt tolerance and the strength of genotype and environment interactions make the trait very complex. Thus breeding for salt tolerance has been slow for all crop plants all over the world. Pokkali, a well known and popular Indian salt tolerant donor has been widely used for the development of salt tolerant rice varieties. However, success has been limited principally due to the difficulty of recovering elite genotypes with the salt tolerance trait (Gregorio et. al. 2002). Due to shrinking land resources, the production of rice is also being pushed to unfavorable environments (coastal regions, flood-prone, as well as low-lying areas). Due to the prevalent abiotic soil stresses, there is a strong need to combine high yield with salinity tolerance in rice. Therefore the most economic and sustained way to overcome problems of food scarcity will be to develop durable salt tolerant high-yielding rice varieties suitable for growth in coastal areas, particularly for the Southern region of Bangladesh. Due to the high sensitivity of modern rice to the effects of salinity, farmers in the Southern regions are forced to continue growing of traditional landraces which have low yields. These landraces may however be a good source as donor of salt tolerance traits. The traditional Bangladeshi rice landrace Boilam is early-maturing and photo-insensitive in contrast to Pokkali and shows both seedling and reproductive stage tolerance (A.K.Kaul. 1982). It is popularly grown in salt-affected areas in the South coast of Bangladesh and gives average yields of ~1.2 t/h. Boilam is characterized by its red pericarp color. Breeders in Bangladesh have earlier tried unsuccessfully to develop salt tolerant rice varieties by using Boilam as a donor. Molecular breeding therefore may prove to be an appropriate solution for producing salt tolerant high yielding rice. This strategy would involve QTL discovery in the donor, fine-mapping of the most promising loci providing tolerance, and use of Marker-assisted Backcrossing (MABC) for transfer to popular varieties (Thomson et. al. 2003). The study of natural variation in diverse donors enhances the functional analysis of plant genomes (Alonso-Blanco and Koornneef 2000; Yano 2001), which in turn increases the potential use of such variation in Salinity tolerance QTL mapping from Coastal Aus Landrace Boilam. DOI: 10.9790/264X-03040107 www.iosrjournals.org 2 | Page international breeding programs. QTL analyses of salt tolerance traits in rice, has been conducted by several other research groups, such as Zhang et al. 1995, Gong et al. 1999 and 2001, Prasad et al. 2000, Lang et al. 2001a and b, Koyama et al. 2001, Bonilla et al. 2002, Lin et al. 1998 and 2004, Takehisa et al. 2004, Ammar 2004, Ren et al. 2005, Ming et al. 2005, Lee et al. 2007, Zang et al. 2008, Sabouri and Sabouri 2008, out of which three reports used Pokkali as a donor, one, Nona Bokra and the rest were from uncharacterized ones. Nona Bokra is similar to Pokkali in that it is also a photosensitive, long duration variety. In addition to use of diffrent donors, multiple traits as well as marker-types were used for these analyses, making correlation difficult. Moreover, Boilam is a novel donor, due to its being an early-maturing and photo-insensitive cultivar. Therefore this study was undertaken to properly characterize its tolerance at seedling, vegetative and reproductive stages and to identify major quantitative trait loci (QTL) linked to the seedling salt tolerance trait. A backcross mapping population between Boilam and BRRI dhan27 was generated and the BC2F2 population analyzed by molecular marker technology for identification of new traits/genes that can be used for breeding purposes. II. Materials and Methods Plant material and DNA extraction: Screening and scoring of 200 BC2F2 progenies were accomplished in the IRRI Phytotron according to IRRI SES procedure (Glenn B. Gregorio, 1997). We took the advantage of selective genotyping as it can markedly decrease the number of individuals genotyped for a given power at the expense of an increase in the number of individuals phenotyped. 96 BC2F2 progenies which were selected from both extremes of the population (200) with respect to the response to salinity at 12 dS/m for seedling stage. Young leaves were collected in liquid nitrogen, crushed to powder and total DNA was isolated using the CTAB method (Doyle,1990) and quantified by Nano drop spectrometer. Construction of linkage map: From the preliminary screening of 550 SSR markers on two parental DNA Boilam and BRRI Dhan-27, 94 polymorphic SSR markers were selected to genotype the mapping population. By combining the SSR markers with selective genotyping, we constructed a linkage map and assigned each linkage group to a corresponding chromosome. Polymerase chain reaction (PCR) and genotyping: PCR mixture was prepared and amplified using the PCR settings as initial denaturing at 94°C for 5 min, followed by 35 to 40 cycles of denaturizing at 94°C for 1 min, annealing at 55°C for 1 min, extension at 72°C for 1 min, with an extension at 72°C for 7 min in a PCR thermal cycler. Polyacrylamide gel electrophoresis (PAGE) was used because it has much higher resolution and reveals unambiguous polymorphic bands among the amplified DNA than agarose gel electrophoresis. The PCR products are separated on non-denaturing polyacrylamide gels with a thickness of 1.5 mm (8% Polyacrylamide gel: 10 ml of 40% polyacrylamide (Invitrogen), 4 ml 25x TBE, 500 μL 10% APS, 42.5 μL TEMED, ddH20 up to 100 ml; pH 8.0; 1x TBE buffer; PH 8.0). Depending on the Molecular weight of the amplified products, 6 to 10% Polyacrylamide gel is used. The gel is stained by immersing it in electrophoresis buffer containing SYBRSafe stain for 10 minutes at room temperature and then photographed by AlphaImager HP (Alpha Innotech). Statistical analysis: QTL analysis was performed with QGene v-4.0 by applying a general interval mapping. The hypotheses was that a single locus or two linked loci have an effect on tolerance to salinity were evaluated. Permutation tests were performed 1000 times on the hypothesis that one locus on a chromosome has an effect on the SES (H1) versus the null hypothesis (H0) that the locus has no effect on the SES. The model with the highest LOD score was fitted to the QTL and when the models did not differ significantly the simpler model was chosen. LOD threshold was selected at 3.0. Single marker regression (SMR), Simple interval mapping (SIM) and Composite Interval Mapping (CIM) and Inclusive Composite Interval Mapping (ICIM) were done by QGene v-4.0. III. Results From the preliminary screening of 550 SSR markers on the two parental DNA of Boilam and BRRI dhan27, 94 polymorphic SSR markers were selected to genotype the mapping population. 96 BC2F2 progeny were selected from both extremes of the population of 200 with respect to their response at 12 dSm salinity stress at seedling stage. Of the total population 27% were found to be tolerant which SES score 3 to 4 and 29% were sensitive with SES scores of 8 to 9 (Fig.1). Such selective genotyping, where 25% of the total population is adequate for QTL determination has been reported previously (Darvasi and Soller, 1992; Ronin et al. 2003). Salini Collapse"
100229,1990,journal of the american statistical association,a monte carlo implementation of the em algorithm and the poor man's data augmentation algorithms,a monte carlo implementation of the em algorithm and the poor mans data augmentation algorithms,"Abstract The first part of this article presents the Monte Carlo implementation of the E step of the EM algorithm. Given the current guess to the maximizer of the posterior distribution, latent data patterns are generated from the conditional predictive distribution. The expected value of the augmented log-posterior is then updated as a mixture of augmented log-posteriors, mixed over the generated latent data patterns (multiple imputations). In the M step of the algorithm, this mixture is maximized to obtain the update to the maximizer of the observed posterior. The gradient and Hessian of the observed log posterior are also expressed as mixtures, mixed over the multiple imputations. The relation between the Monte Carlo EM (MCEM) algorithm and the data augmentation algorithm is noted. Two modifications to the MCEM algorithm (the poor man's data augmentation algorithms), which allow for the calculation of the entire posterior, are then presented. These approximations serve as diagnostics for the validity o... Collapse"
100230,1990,journal of the american statistical association,a frequency domain selection criterion for regression with autocorrelated errors,a frequency domain selection criterion for regression with autocorrelated errors,"Abstract We consider the regression model yt = η t + e t , t = 0, 1, …, n − 1, where y t are scalar observations, η t is the unknown regression function, and e t are unobservable errors generated by a zero-mean weakly stationary process, independent of η t and with completely unknown autocorrelation structure. We propose a data-driven method for selecting a parametric or nonparametric estimator of η t . The method is based on cross-validation in the frequency domain and requires no assumptions about the form of the estimator or the error correlations. It does, however, require the discrete Fourier transform (DFT) of the signal η t to be a smooth complex function of frequency, as is the case, for example, with transient signals or growth and decay curves. After giving some general motivations for the method, we focus on the special case of linear estimators of a nonparametric regression function, including both kernel and spline estimators. For these estimators, we develop efficient methods of evaluating t... Collapse"
100231,1990,journal of the american statistical association,calibrating prediction regions,calibrating prediction regions,"Abstract Suppose that the variable X to be predicted and the learning sample Yn that was observed have a joint distribution, which depends on an unknown parameter θ. The parameter θ can be finite- or infinite-dimensional. A prediction region Dn for X is a random set, depending on Yn , that contains X with prescribed probability α. This article studies methods for controlling simultaneously the conditional coverage proability of Dn , given Yn , and the overall (unconditional) coverage probability of Dn . The basic construction yields a prediction region Dn , which has the following properties in regular models: Both the conditional and overall coverage probabilities of Dn converge to α as the size n of the learning sample increases. The convergence of the former is in probability. Moreover, the asymptotic distribution of the conditional coverage probability about α is typically normal; and the overall coverage probability tends to α at rate n −1. Can one reduce the dispersion of the conditional coverage pr... Collapse"
100235,1990,journal of the american statistical association,kernel smoothing of data with correlated errors,kernel smoothing of data with correlated errors,"Cereal Chem. 69(3):254-257 A direct-drive, 2-g mixograph was used to examine the heritability offspring-parent heritability within and over crosses. These results demonof mixing characteristics in wheat flours. A good range of variation in strate that the 2-g mixograph meets the requirements for use in early mixograph parameters (each with acceptable errors of determination) was generation selection for wheat quality. The value of the instrument was found for flours derived from the seed of F2, single-plant selections and confirmed by simulated selection based on guideline values for two wheat in their F3 progeny. The results agreed well over years for the important grades and the response to selection as measured using realized heritability variables of time to maximum resistance (mixing time) and tolerance estimates. to mixing (resistance breakdown), as evidenced by the medium-to-high Cereal laboratories involved with wheat breeding programs continually develop and evaluate new procedures that can be applied to early generation selection for wheat quality. Such procedures must be simple, rapid, and reliable, and they must use only a small quantity of seed, have high correlations with end-use properties, and retain their predictive capacity independent of the location or year of growth. The advantages of such early generation testing procedures are that they result in more efficient use of limited cereal laboratory and breeding resources. Subsequent costly tests for field plot disease resistance, yield, milling, physical dough properties, and baking need only be conducted on lines that have an enhanced probability of having acceptable end-use quality. Early generation quality tests are many and varied in type, ranging from sedimentation tests of wheat meals and flour (e.g., Zeleny volume, sodium dodecyl sulfate volume, alkaline waterretention capacity) or yeasted wheat meal or flour dough balls (e.g., Pelshenke time) to varied flour-water dough mixing tests (e.g., mixograph [TMCO, Lincoln, NE] and farinograph [Brabender, Duisberg, Germany]), and microscale test baking procedures (MacRitchie and Gras 1973, Shogren and Finney 1984). The results of testing for kernel hardness, protein content, and rate of dough development have been shown to provide adequate prediction of test loaf quality (Fowler and de la Roche 1975a). Each of these parameters showed considerable, although population dependent, genetic variation (Fowler and de la Roche 1975b). Similarly, response to early generation selection has been reported for the use of measures of kernel hardness, protein content, and small-scale tests correlated with dough strength such as sodium dodecyl sulfate volume, percent residue protein, Zeleny volume, and Pelshenke time (Fischer et al 1989, O'Brien et al 1989). The use of a mixograph employing 10 g of flour (Finney and Shogren 1972) has enabled breeders to select directly for physical dough properties in early generations. The need to mill at least 15 g of seed to produce the flour for this purpose may have restricted its application in early generations in many breeding programs. A smaller recording mixer utilizing 5 g of flour has been reported recently (Finney 1989). A test that required even smaller sample sizes (2 g of flour obtained from 3-5 g of seed) would have many applications. It could be widely used in laboratories for basic research on wheat quality where quantity of sample is a major limitation. However, 'This is the written version of a paper presented at the 76th annual meeting of the AACC, held in Seattle, WA, October 13-16, 1991. 2 Grain Quality Research Laboratory, CSIRO Division of Plant Industry, P.O. Box 7, North Ryde NSW 2113, Australia. 3 The University of Sydney, Plant Breeding Institute, I. A. Watson Wheat Research Centre, Narrabri NSW 2390, Australia. © 1992 American Association of Cereal Chemists, Inc. the most widespread application would be for quality evaluation in the early generations of wheat breeding programs, allowing early generation testing for physical dough properties to be applied to any breeding program. Seed from single plants, progeny rows, or yield plots could be evaluated. The applicability of an early generation test relies not only on its correlation with standard measures of wheat quality, but also on the response measurable within segregating breeding populations in the generation following selection. This article reports the use of a direct-drive 2-g mixograph (Rath et al 1990) to determine the physical dough properties of flours from F2 singleplant selections. The accuracy and effectiveness of the procedure was evaluated on the resulting F3 progeny. MATERIALS AND METHODS Single plant-selections from the F2 generation of three crosses, Suneca/ Sunfield (SS), Vasco/ Hartog/ / Vasco (VH), and Sunbird/Dollarbird (SD), and their derived F3 lines were used in this study. In the F2 generation single plants were selected from space-planted rows on the basis of having acceptable straw strength, lodging resistance, height, flowering time, and maturity for the target region of the breeding program (northern NSW, Australia). Seed of the single plants was stored in a seed room for a minimum period of three weeks until it had equilibrated to approximately 12% moisture content. Ten grams of seed was preconditioned to 15% moisture content for 24 hr before being milled with a Quadrumat Junior flour mill (Brabender, Duisberg, Germany) fitted with a sieving screen covered with 9XX silk mesh. The milling procedure followed was similar to that described by Whan (1974). In the F3 generation the lines were sown in separate two-replicate experiments with randomized complete block designs, one for each cross. Each experiment included the parents of the cross and the varieties Sunco, Hartog, and Sunelg as checks. Plots were harvested with a plot combine, and a 10-g subsample of each replicate was milled to flour following the same procedures as for the F2 generation. Flour protein contents were determined by near-infrared reflectance spectrometry, using a calibration based on Kjeldahl analysis (protein = N X 5.7). All flours were equilibrated for 72 hr at 220 C with air at 60% relative humidity to obtain a moisture content of approximately 12.8% before mixograph testing (Bushuk and Winkler 1957). Water additions for mixing tests were calculated according to a formula relating water addition to flour moisture and protein content (AACC 1983). Mixing parameters were obtained with a prototype recording dough mixer (TMCO, Lincoln, NE) employing 2 g of flour per measurement (Rath et al 1990). Mixing parameters were obtained by automated interpretation of the recorded data using specially written software (Gras et al 1990). Mixing parameters were measured from the center line of the recorded data after 254 CEREAL CHEMISTRY smoothing, using a 25-sec moving mean. Parameters measured were time to maximum resistance and time to maximum bandwidth (both in minutes), maximum resistance (maximum height of the mixing curve), bandwidth at maximum resistance, breakdown in resistance, breakdown in bandwidth, and maximum bandwidth (reported in a 100-unit scale, where 0 and 100 correspond to the minimum and maximum deflection on a standard 35-g mixograph). Resistance and bandwidth breakdowns were defined as the decreases in resistance and bandwidth, respectively, measured 3 min after maximum resistance. The limited quantity of seed available from the F2 single-plant selections, combined with the predetermined quantity needed to plant the F3 generation, did not permit remilling of additional samples for repeats of mixographs that were discarded because of operator error. This resulted in a lower number of paired samples (F2 to F3) for the SD cross. Statistical analyses were performed by standard procedures using the MSUSTAT package of statistical analysis programs. RESULTS AND DISCUSSION In each generation, a wide range of values was found for most of the dough properties measured by the 2-g mixograph. The means and ranges for these parameters and the flour protein content in the F2 and F3 generations are shown in Table I. The skewnesses of the data were usually less than one with the exception of the data for the mix time of the F2 generation of the VH cross, where the skewness was 1.16. These results indicate that these crosses were a suitable sample to evaluate the effectiveness of the 2-g mixograph for early generation testing of doughmixing properties. The duplicate mixographs of the F2 generation flours indicate that in most cases the measurements of dough properties have acceptably low errors of determination (Table I). A wide range of values and low errors of determination are both prerequisites for the application of the 2-g mixograph to early generation selection. An analysis of variance of the duplicate mixographs of each field replicate for all lines of all crosses in the F3 generation indicated significant differences at the 0.001 level of probability between lines for the estimates of time to maximum resistance (mixing time), maximum resistance, resistance breakdown (tolerance to overmixing), and time to maximum bandwidth. Of these, time to maximum resistance and resistance breakdown are the most meaningful measures of dough quality (Hoseney and Finney 1974, Fowler and de la Roche 1975a). Significant block effects (field replication) were apparent for a number of variables, but these, like the few interactions between line and block, were not considered important because of the consistent differences between lines for mixograph properties in each cross. Offspring-parent heritability estimates (Frey and Horner 1957) for flour protein content and the mixograph parameters (Table II) indicated that medium-to-high values were obtained for a number of measures. For the population pooled over crosses, significant heritabilities were obtained for all parameters. However, some estimates, although stati Collapse"
100239,1990,journal of the american statistical association,a one-sided studentized range test for testing against a simple ordered alternative,a one sided studentized range test for testing against a simple ordered alternative,"Abstract Consider the usual balanced one-way fixed effects analysis of variance (ANOVA) model X ij = μ i + e ij (1 ≤ j ≤ n; 1 ≤ i ≤ k), where the e ij are independent N(0, [sgrave]2) random variables and μ i is the mean of the ith treatment (1 ≤ i ≤ k). The μ i and [sgrave] 2 are unknown parameters. Let X i be the sample mean of the ith treatment based on n observations (1 ≤ i ≤ k), and let S 2 be an unbiased estimate of [sgrave]2, which is distributed independently of the X i as a σ2 χ 2 v /v random variable. Usually, the ANOVA mean squared error with v = k(n − 1) df is used as the estimate S 2. A problem that has received considerable attention is that of testing the null hypothesis H 0 : μ 1, = … = μ k against the simple ordered alternative hypothesis H A : μ 1 ≤ … ≤ μ k with at least one strict inequality [see, e.g., Barlow, Bartholomew, Bremner, and Brunk (1972) and Robertson, Wright, and Dykstra (1988)]. Various test procedures have been proposed for this problem, such as the likelihood ratio test (... Collapse"
100240,1990,journal of the american statistical association,assessing the effects of multiple rows on the condition number of a matrix,assessing the effects of multiple rows on the condition number of a matrix,"Abstract Given a matrix X of n observations on k variables, it is known that the singular values of X can change substantially when few rows are omitted from X. Hadi (1988) shows that no general closed-form equation can relate the singular values of X to the singular values of X with one row deleted and gives closed-form approximations to the relationship between the singular values of the two matrices. In this article, we extend Hadi's results to the more general case of multiple-row deletion, carry out systematic numerical investigations to determine the goodness and the speed of the approximation, and give an example using real-life data to illustrate the usefulness of the results in diagnosing jointly influential observations in linear regression. The methods presented in this article deal with the computational as well as the data analytic aspects of problems arising in multivariate data analysis. These methods are applicable in situations where the eigenstructure of a matrix is of interest. Collapse"
100241,1990,journal of the american statistical association,confidence intervals for probabilities and tolerance regions based on a generalization of the mann-whitney statistic,confidence intervals for probabilities and tolerance regions based on a generalization of the mann whitney statistic,"Abstract Inference procedures are proposed for any specified function of two random variables f(X, Y), assuming that independent random samples from the X and Y populations are available. A generalization of the Mann-Whitney statistic is used to obtain point and interval estimates for the probability that f(X, Y) falls in a given interval. One proposed confidence interval is a modification and improvement to the approach of Halperin, Gilbert, and Lachin (1987) for estimating Pr(X < Y). A thorough review is made of the literature on nonparametric confidence bounds for Pr(X < Y), with emphasis on methods based on the central limit theorem. In addition to the improvement of Halperin et al.'s method, bounds analogous to the Clopper-Pearson confidence interval for a binomial parameter are proposed. Simulation results show the performance of the alternative procedures. Approximate nonparametric tolerance limits for f(X, Y) are also proposed. The article closes by constructing a two-sided tolerance interval for ... Collapse"
100244,1990,journal of the american statistical association,estimation for partially nonstationary multivariate autoregressive models,estimation for partially nonstationary multivariate autoregressive models,"Abstract For an m-variate (“partially”) nonstationary vector autoregressive process {Y t }, we consider the autoregressive model Φ(L)Y t = e t , where Φ(L) = I − Φ1 L − … − Φ p L p and det{Φ(z)} = 0 has d < m roots equal to unity and all other roots are outside the unit circle. It is also assumed that rank {Φ(1)} = r, r = m − d, so that each component of the first differences W t = Y t − Y t − 1 is stationary. The relation of the model to error correction models and co-integration (Engle and Granger 1987) is discussed. The process {Y t } has the error correction representation Φ*(L)(1 − L)Y t = −P 2(I r − Λ r )Q 2 1 Y t-1 + e t , where Q(I m − Φ(1))P = J = diag(I d , Λ r ) is in Jordan canonical form and Q' = [Q 1, Q 2]. It follows that the transformation Z t = QY t = [Zt 1t , Z2 2t ] t is such that the d × 1 process Z 1t is nonstationary with Z 1t − Z 1t-1 stationary while Z 2t is stationary. Asymptotic distribution theory for least squares parameter estimators of the model is first considered. A Gaussia... Collapse"
100247,1990,journal of the american statistical association,state-dependent utilities,state dependent utilities,"Several axiom systems for preference among acts lead to the existence of a unique probability and a state-independent utility such that acts are ranked according to their expected utilities. These axioms have been used as a foundation for Bayesian decision theory and the subjective probability calculus. In this paper, we note that the uniqueness of the probability is relative to the choice of what counts as a constant outcome. Although it is sometimes clear what should be considered constant, there are many cases in which there are several possible choices. Each choice can lead to a different “unique” probability and utility. By focusing attention on state-dependent utilities, we determine conditions under which a truly unique probability and utility can be determined from an agent’s expressed preferences among acts. Suppose that an agent’s preference can be represented in terms of a probability P and a utility U. That is, the agent prefers one act to another if and only if the expected utility of the one act is higher than that of the other. There are many other equivalent representations in terms of probabilities Q, which are mutually absolutely continuous with P, and state-dependent utilities V, which differ from U by possibly different positive affine transformations in each state of nature. An example is described in which two different but equivalent state-independent utility representations exist for the same preference structure. What differs between the two representations is which acts count as constants. The acts involve receiving different amounts of one or the other of two currencies and the states are different exchange rates between the currencies. It is easy to see how it would not be possible for constant amounts of both currencies to simultaneously have constant values across the different states. Savage (Foundations of statistics. John Wiley, New York, 1954, sec. 5.5) discovered a situation in which two seemingly equivalent preference structures are represented by different pairs of probability and utility. Savage attributed the phenomenon to the construction of a “small world”. We show that the small world problem is just another example of two different, but equivalent, representations treating different acts as constants. Finally, we prove a theorem (similar to one of Karni, Decision making under uncertainty. Harvard University Press, Cambridge, 1985) that shows how to elicit a unique state-dependent utility and does not assume that there are prizes with constant value. To do this, we define a new hypothetical kind of act in which both the prize to be awarded and the state of nature are determined by an auxiliary experiment. Collapse"
100254,1990,journal of the american statistical association,regression estimates in federal welfare quality control programs:  rejoinder,regression estimates in federal welfare quality control programs rejoinder,"Abstract States that administer federal family welfare programs review samples of the beneficiaries to guide corrective actions. Federal agencies review subsamples of the state samples to compute overpayment error rates using a regression estimator. These federal estimates make state-to-state comparisons of quality possible and are also used for fiscal sanctions. Such applications of the regression estimator have been widely challenged. We show that the regression estimates and estimates of their variances are closely unbiased, and we examine their statistical characteristics. We also comment on the recent reports of the National Research Council Panel on Quality Control of Family Assistance Programs. Collapse"
100256,1990,journal of the american statistical association,food stamp payment error rates:  can state-specific performance standards be developed?,food stamp payment error rates   can state specific performance standards be developed ,"Abstract Since 1981 the Food and Nutrition Service, U.S. Department of Agriculture has imposed financial penalties on 49 states with excessive overpayment errors in the Food Stamp program, many in more than one year, for a total of $423.5 million. Almost all have appealed the sanctions, and several have taken their appeal to the courts, questioning the fairness of the financial penalties. Although the quality control (QC) system holds all states to the same standard, payment error rates (defined as the proportion of total benefits paid erroneously) may reflect not only the administrative performance of an agency, but also the difficulty of the case load served and the characteristics of the social environment in which the program operates. To investigate the feasibility of adjusting states' overpayment error rates for differences on such external factors, we developed regression models for five components of the overpayment error rate. When fitted to the case-level data from the fiscal year (FY) 1984 QC s... Collapse"
100280,1990,journal of banking and finance,performance of currency portfolios chosen by a bayesian technique: 1967-1985,performance of currency portfolios chosen by a bayesian technique  1967 1985,"Abstract This paper is about normative currency portfolio rules. It assumes logarithmic investors who maximize the expected utility from lognormal currency returns with and without short sales restrictions. A number of implementable portfolio diversification policies are tried out on a large body of data covering nine major currencies and eighteen years of weekly observations. In doing so, we present ways of dealing with the ‘estimation risk’ in an international context. The necessary mean and covariance inputs are provided by a Bayesian prior on the means and by sample means and covariance matrices, which are estimated from weekly sample data. While some policies do produce abnormal returns (over and beyond proper reward for risk), none does so in a statistically significant way. This means that the evidence does not allow one to conclude that market participants are improperly diversified. As a byproduct of this investigation, techniques are found which would allow portfolio managers to earn a proper reward for risk by following a purely mechanical procedure; such techniques may be valuable in a multi-country world where the aggregate portfolio of currencies and securities is unknown and is not supposed to be efficient. Collapse"
100301,1990,journal of business and economic statistics,embodied technological change and tests of the internal-adjustment-cost hypothesis,embodied technological change and tests of the internal adjustment cost hypothesis,"Several recent studies analyzing labor-productivity changes have focused on the existence and measurement of the internal costs of adjustment attendant on investment in new-capital goods. These internal-adjustment costs are those borne directly by firms purchasing new-capital goods and include such costs as those associated with setting up the new capital, learning the operation of the new equipment, and assimilating the new factors into the production process. Such costs need not be explicit, as in the cost of training programs, but can take the form of a reduction in ordinary output, such as resources diverted from the production of regular output to accommodate the adjustment costs of new investment. (Internal-adjustment costs should be distinguished from the well-known external costs of adjustment, such as the increasing marginal resource costs of delivering the desired level of capital goods to industries.) Failure to consider these costs can have far-reaching effects in several areas. One such area is the analysis of the impact of new investment on output and productivity. If these internal-adjustment costs are significant and important, the Collapse"
100336,1990,journal of development economics,"determinants of the tariff structure of the israeli industrial sector, 1965-1977",determinants of the tariff structure of the israeli industrial sector  1965 1977,"Abstract This paper examines the determinants of the tariff structure of the Israeli manufacturing sector in the mid-1960s and late 1970s, a period marked by considerable trade liberalization. The empirical results point to the ‘made-to-measure’ nature of the tariff structure, i.e., relatively inefficient industries were granted higher rates of protection. The relative inefficiency measure which was derived from the CES production function kept a high explanatory power, along a wide range of substitution elasticities. The empirical results also show that in the early 1960s, tariffs favoured low-wage, labor-intensive industries; while in the late 1970s, industries with high levels of concentration lobbied successfully for higher levels of protection. Collapse"
100337,1990,journal of development economics,a sectoral accounting approach to national savings applied to korea,a sectoral accounting approach to national savings applied to korea,"Abstract This paper presents a sectoral accounting approach to the determination of the national savings ratio. Starting with the process of sectoral savings determination and showing the sectoral interactions, this approach offers an acceptable empirical framework for aggregate savings studies. The model, estimated with the Korean data, is used to run simulations to evaluate its predictive capacity, to decompose past savings performance by major factors and to assess the savings effect of policy changes and other exogenous shocks. The results indicate that income variables have been the most important determinants, although interest rate and tax policies have also played some role. Collapse"
100338,1990,journal of development economics,on measuring the relative size of the unregulated to the regulated money market over time,on measuring the relative size of the unregulated to the regulated money market over time,"Abstract This paper develops a method, based on the recent theories of index number, monetary aggregation and money supply, by which changes can be measured in the size of the Unregulated Money Market (UMM) relative to that of the regulated money market in developing countries. While the method requires some knowledge of the initial UMM assets and the growth rate of the monetary multiplier, it is virtually parameter-free in measuring the effects of asset substitution on the relative size of the UMM. For illustrative purpose it is applied to Korea, which appears to provide satisfactory results, indicating the usefulness of the method. Collapse"
100339,1990,journal of development economics,does regulation improve small farmers' access to brazilian rural credit?,does regulation improve small farmers access to brazilian rural credit,"Abstract Brazilian policymakers attempted to counter the inherent tendency for concentration in subsidized rural credit programs by requiring banks to lend a specified volume to small farmers. Inferences about the regulation's success are drawn from farm-level, panel data from Sao Paulo in the early 1980s. Membership in the target group should have a positive effect in an ‘access rule’, relating farmers' characteristics to their probability of receiving credit. When allowing for time-invariant individual effects, possibly correlated with target group indicators, tests indicate that the regulation had no effect. Subsidiary results cast doubt on the efficacy of a broader set of regulations. Collapse"
100340,1990,journal of development economics,is profit sharing a cure for unemployment in less developed countries?,is profit sharing a cure for unemployment in less developed countries,Abstract The effects of introducing profit sharing in modern sectors in LDCs are discussed within a dual model with endogenous wages and internal migration. Profit sharing does not have a direct job creation effect as long as firms set the pay parameters. Yet the new system may have long-run effects on employment allocation. Under some circumstances profit sharing may in fact destabilize short-run employment such that jobs last for a shorter duration. Modern sector wages are then raised to discipline workers and urban unemployment goes up. Arguments are also given for the hypothesis that profit sharing stabilizes short-run employment with the opposite results. Collapse
100341,1990,journal of development economics,complementarity between public and private investment in india,complementarity between public and private investment in india,"Abstract In this paper, we examine the question of complementarity between public and private investment in India under different modes of allocation and financing of public investment. We use an 18 sector computable general equilibrium modey where money plays a non-neutral role. We find that public investment crowds out private investment; but in terms of its effect on total investment and growth and distribution of income, the economy is better off with increased public investment. That raises the question: Is crowding out all that undesirable? Collapse"
100342,1990,journal of development economics,shifts over time and regional variation in women's labor force participation rates in a developing economy:  the case of greece,shifts over time and regional variation in womens labor force participation rates in a developing economy the case of greece,"Abstract In this study an attempt is made (a) to explain the decline and the inter-area variation in women's activity rates in Greece during the process of its economic development, (b) to ascertain if there were any shifts in the relevant relationships over time and (c) to estimate women's hidden unemployment. The decline in activity rates was explained with the help of the hypothesis of a U-shaped impact of economic development on them. Among the most important findings were the considerable discouraging effect on labor force participation of unemployment and the negative impact of education. Women's hidden unemployment was found to be two to three times larger than reported unemployment. Collapse"
100343,1990,journal of development economics,"the ramaswami proposition and the choice among labor, capital and technology flows",the ramaswami proposition and the choice among labor capital and technology flows,"Abstract This paper examines the choice between labor and capital flows in the context of a one-good, two-country model where the two countries differ in their levels of disembodied technology. In contrast to the Ramaswami proposition, it is shown that a country may prefer an optimal outflow of its abundant factor to an optimal inflow of its scarce factor. It is also shown that for sufficiently small differences in technology, the labor-rich, technology inferior country will benefit more from an optimal outflow of labor than from a gift of superior technology. Collapse"
100344,1990,journal of development economics,concentration and instability revisited,concentration and instability revisited,"From the temperature dependence of the impedance dispersion in dipalmitoylphosphatidylcholine liposomes, we reported that halothane decreased the apparent activation energy of the lateral surface conductance of phospholipid vesicles (Yoshida et al. Biochim. Biophys. Acta 1990, 1028, 95; Yoshino et al. Biochim. Biophys. Acta 1992, 1107, 55). However, the presence of proton flow within the electrical double layer at the lipid−water interface has been a matter of controversy. The difference in interpretation on two-dimensional proton flow is focused on imperfection in the experimental procedure, in particular, change of the meniscus of water surface at the electrode and the trough caused by monolayer formation, which changes the effective surface area of the electrode, pH instability due to local CO2 concentrations, polarization by the accumulation of ions on the electrode surface, and temperature instability. We designed a novel horizontal-plate electrode, totally immersed in water, to eliminate formation o... Collapse"
100345,1990,journal of development economics,concentration and instability:  again,concentration and instability again,"From the temperature dependence of the impedance dispersion in dipalmitoylphosphatidylcholine liposomes, we reported that halothane decreased the apparent activation energy of the lateral surface conductance of phospholipid vesicles (Yoshida et al. Biochim. Biophys. Acta 1990, 1028, 95; Yoshino et al. Biochim. Biophys. Acta 1992, 1107, 55). However, the presence of proton flow within the electrical double layer at the lipid−water interface has been a matter of controversy. The difference in interpretation on two-dimensional proton flow is focused on imperfection in the experimental procedure, in particular, change of the meniscus of water surface at the electrode and the trough caused by monolayer formation, which changes the effective surface area of the electrode, pH instability due to local CO2 concentrations, polarization by the accumulation of ions on the electrode surface, and temperature instability. We designed a novel horizontal-plate electrode, totally immersed in water, to eliminate formation o... Collapse"
100346,1990,journal of development economics,corruption and allocation efficiency,corruption and allocation efficiency,"Corruption and targeting failures in the delivery of public services in developing countries has frequently been argued to result from absence of controls on the behavior of central bureaucrats delegated authority over service delivery. This has motivated recent initiatives towards decentralization of service procurement and delivery to elected local governments expected to be more accountable to user interests. However if local democracy is prone to capture by local elites, decentralization can also be subject to diversion and targeting failures. This paper presents a simple analytical framework to evaluate the resulting trade-off, and predict the effects of decentralization on volume and allocation of service delivery under different financing mechanisms. Consistent with existing cross-country empirical evidence, we find that greater fiscal autonomy of local governments expands the volume of service delivery, but this tends to be accompanied by service overprovision to local elites at the expense of non-elites. Restrictions on the ability of local governments to raise local taxes can accordingly be justified on efficiency and equity grounds. User fee mechanisms ensure that decentralization welfare dominates centralization, irrespective of the degree of local capture. Collapse"
100366,1990,journal of econometrics,are consumption-based intertemporal capital asset pricing models structural?,are consumption based intertemporal capital asset pricing models structural ,"Abstract Hansen and Singleton (1982) and Dunn and Singleton (1986) have found supporting evidence for the overidentifying restrictions of two empirical consumption-based asset pricing models, when estimated with a particular set of single asset returns. In this paper, we submit these models to further scrutiny by testing whether they exhibit (structural) stability. A series of tests, recently developed by Ghysels and Hall (1990), are applied and a test for structuralinvariance is introduced based on the likelihood ratio type test procedure of Eichenbaum, Hansen, and Singleton (1988). There are a number of reasons why structural stability tests are particularly appropriate for diagnostic testing of Euler equation models, namely: (1) the Lucas econometric policy evaluation critique; (2) Euler equations are only a partial description of the data-generating process and parameter stability is one of the few assumptions imposed in their estimation via the generalized method of moments; and finally (3) it is demonstrated that Hansen's overidentifying restrictions test has no power against a class of local alternatives characterized by a parameter drift. Collapse"
100367,1990,journal of econometrics,using conditional moments of asset payoffs to infer the volatility of intertemporal marginal rates of substitution,using conditional moments of asset payoffs to infer the volatility of intertemporal marginal rates of substitution,"Previously Hansen and Jagannathan (1990a) derived and computed mean-standard deviation frontiers for intertemporal marginal rates of substitution (IMRS) implied by asset market data. These frontiers give the lower bounds on the standard deviations as a function of the mean. In this paper we develop a strategy for utilizing conditioning information efficiently, and hence improve on the standard deviation bounds computed by Hansen and Jagannathan. We implement this strategy empirically by using the seminonparametric (SNP) methodology suggested by Gallant and Tauchen (1989) to estimate the conditional distribution of a vector of monthly asset payoffs. We use the fitted conditional distributions to calculate both conditional and unconditional standard deviation bounds for the IMRS. The unconditional bounds are as sharp as possible subject to robustness considerations. We also use the fitted distributions to compute the moments of various candidate marginal rates of substitution suggested by economic theory, and in particular the time-nonseparable preferences of Dunn and Singleton (1986) and Eichenbaum and Hansen (1990). For these preferences, our findings suggest that habit persistence will put the moments of the IMRS inside the frontier at reasonable values of the curvature parameter. At the same time we uncover evidence that the implied IMRS fails to satisfy all of the restrictions inherent in the Euler equation. The findings help explain why Euler equation estimation methods typically find evidence in favor of local durability instead of habit persistence for monthly data. Collapse"
100387,1990,journal of economics and business,is normative portfolio theory dead?,is normative portfolio theory dead,"Quite a heated debate has been raging in Finance Theory since the early 1990?s regarding the relevance of Modern Portfolio Theory. Yet both adversaries are overlooking something very fundamental that could in fact bring them much closer. My working paper on the Market Indifference Curve provides this missing link: it allows for the integration of what investors refer to as ?sentiment? in the efficient pricing of risk by rational investors. Specifically, it demonstrates that (even) in the context of rational investors, the fair price of total risk at any time t reflects the aggregate ? not absolute or unanimous! ? risk aversion among investors at that time. If only anecdotic, intuitively such interpretation corresponds closely to what we observe when monitoring spread risk: the continuous consensual pricing of the risk premium by investors. When next, as Behavioral Finance proscribes, ?non-rational? behavior such as excessive optimism and excessive pessimism enters the equation, periods of substantial consensual mispricing of risk (herd behavior) may occur indeed, yet still based on the same premise: the efficient market reflects the consensus price of risk. In sum, the rational investor is alive and kicking and in principle at any time t the market efficiently provides the fair consensual price of risk. However, irrespective of whether investors are rational or not, this fair price of risk ? a time sensitive consensual phenomenon by default ? should be distinguished from the (incorrect) notion of an unanimous or absolute price of risk. Collapse"
100393,1990,journal of economics and business,implementing the growth-optimal policy for choosing portfolios,implementing the growth optimal policy for choosing portfolios,"Abstract In this paper we compare practical procedures for implementing the well-known “growth-optimal” strategy for managing portfolios. The policy involves choosing in each investment period the portfolio that maximizes the expectation of the logarithm of the portfolio's return. We consider two vesions of the decision rule—the expected-log function itself and two-monent approximation of it—and three procedures for determining ex ante expectations of returns. In addition to a nonparametric approach, we examine two parametric models, one that takes returns to be jointly lognormal and a second that imposes a single-factor structure on the logs of returns. The relative performance of the six procedures is compared using data for indexes of securities and for the individual stocks in the Dow-Jones Industrial Average. Our main conclusion is that relatively simple methods for carrying out the growth-optimal program work about as well as methods that are computationally difficult. While naively analyzing historical returns cannot be relied upon to provide above-average performance, there are indications that performance can be improved by giving higher weight to the most recent returns. Collapse"
100410,1990,journal of economic behavior and organization,the microanalytic approach for modeling national economies,the microanalytic approach for modeling national economies,No Result.
100428,1990,journal of economic dynamics and control,the effects of taxes and dividend policy on capital accumulation and macroeconomic behavior,the effects of taxes and dividend policy on capital accumulation and macroeconomic behavior,"Abstract This paper analyzes the macrodynamic effects of changes in various tax rates in an intertemporal optimizing framework. Two aspects emphasized include the role of dividend policy and the behavior of the stock market. Both permanent and temporary tax changes are considered, with the transitional adjustment paths being characterized in detail. The contrast between the short-run and long-run effects is highlighted. In particular, an increase in any of the tax rates will cause short-run employment to fall, and with the capital stock fixed instantaneously, the capital-labor ratio immediately rises. Over time, as the capital stock declines, the capital-labor ratio falls. Collapse"
100429,1990,journal of economic dynamics and control,sources of output fluctuations in the united states during the inter-war and post-war years,sources of output fluctuations in the united states during the inter war and post war years,"Abstract This paper attempts to decompose the sources of fluctuations in industry output into: (1) an aggregate (national) shock, (2) industry group specific shocks, and (3) idiosyncratic factors. The empirical methodology is the Engle-Watson dynamic MIMIC model. The work is motivated as an attempt to compare empirically aggregate, ‘single factor’ theories of the cycle with real business cycle theories emphasizing technology shocks. Disaggregate data from the inter-war and post-war periods are examined. Conclusions on the relative importance of the various shocks seem to be dependent on the period being considered. The ‘single index’ theory of the cycle seems more appropriate for the inter-war period. Collapse"
100430,1990,journal of economic dynamics and control,when does coordination pay?,when does coordination pay,"This study examines the stock transactions of top managers of bidder firms for their personal accounts as signals about their motivations regarding corporate takeovers. Overall, the data indicate that, prior to takeover announcements, top managers increase their net purchases rather than sales. Bidder managers purchase more shares when the stock price reaction to the takeover announcement is large and positive than when it is large and negative. Bidder managers are also more optimistic in cash offers than in equity offers. Overall, the evidence does not appear to support the hypothesis that bidder managers knowingly pay too much for target firms. Copyright 1990 by the University of Chicago. Collapse"
100431,1990,journal of economic dynamics and control,on the term structure of interest rates,on the term structure of interest rates,"AbstractAimsThe aim of the study was to evaluate and compare continuous subcutaneous insulin infusion (CSII) use in pediatric and adult age groups. Methods Data were collected with a questionnaire sent by e-mail to CSII-experienced Diabetes Centers. The questionnaire assessed: (1) number of CSII-treated patients; (2) patient demographic data and characteristics; (3) structure and organization of Diabetes Centers providing CSII therapy; (4) pump characteristics (conventional pump, sensor-augmented pump); and (5) CSII dropouts.ResultsA total of 217 out of 1093 Italian centers participated: 51 pediatric (23.5 %) and 166 (76.5 %) adult centers (AP). Compared to a survey performed in 2005, there was a significant increase in the number of pediatric units when compared to adult units (112 vs 37 %, respectively, p < 0.05). Pediatric age is characterized by a greater concern for quality of life and injections, and a higher dropout rate (10.6 vs 8.9 %) mainly related to pump wearability and site reactions. A complete diabetes-care team is associated with a superior use of technology (fewer dropouts, increased CGM and advanced bolus use) which is, however, still used in a small percentage of patients.ConclusionsIn Italy, the number of CSII-treated pediatric patients (PP) is growing more significantly when compared to adults. Only 60 % of all patients are using advanced functions and 20 % are using CGMs continuously. This confirms the great interest in diabetes technology that is growing in pediatric diabetologists. However, much improvement is warranted in the organization and specialized training of pediatric, adult and transitional facilities. Collapse"
100432,1990,journal of economic dynamics and control,output-inflation cycles in an economy with staggered wage setting,output inflation cycles in an economy with staggered wage setting,"Abstract The question whether increased price flexibility is stabilizing in macromodels of the business cycle has been recently questioned by De Long and Summers (1986). The present paper is an attempt to reconsider the question by applying some methods from the theory of nonlinear dynamics, especially bifurcation theory, to the analysis of a dynamic model with sluggish price adjustment. It is argued that, when inflation plays a destabilizing role through the Mundell effect, a stable closed orbit representing the perfect-foresight dynamics of the output-inflation pair may appear in the vicinity of a stationary state. Moreover, bifurcation theory is applied to find analytical results about the relation between the amplitude of the cycles and the ‘degree’ of flexibility of prices. This relation is shown to be positive in a variety of cases. Collapse"
100434,1990,journal of economic dynamics and control,how to do comparative dynamics on the back of an envelope in optimal control theory,how to do comparative dynamics on the back of an envelope in optimal control theory,"Abstract The dynamic primal-dual methodology of Caputo (1988) is extended and applied to a general nonautonomous optimal control problem with a fixed and finite time horizon, a fixed vector of initial states, a free vector of terminal states, a time-independent vector of parameters, and a vector of nondifferential constraints on the state and control variables. A symmetric negative semidefinite matrix subject to constraint is shown to characterize the qualitative properties of the model as implied by the dynamic maximization assertion. Collapse"
100435,1990,journal of economic dynamics and control,delaying or deterring entry:  a game-theoretic analysis,delaying or deterring entry   a game theoretic analysis,"Abstract Few models in the entry literature consider interaction among potential entrants or how the incumbent can exploit this interaction. I present a predation model in which incomplete information about the incumbent gives each entrant an incentive to enter after others in order to observe what happens to them upon entry. The incumbent can exploit this incentive to delay entry. Thus the model leads to endogenous delayed entry, rather than entry deterrence only. The interaction of the entrants leads to surprising ambiguities. For example, lowering the incumbent's payoff to predation for all types can increase the probability of predation, slowing entry. Collapse"
100436,1990,journal of economic dynamics and control,optimal dynamic durability,optimal dynamic durability,"Abstract Consider a firm that adjusts its production and the choice of durability for its products instantaneously. We show that when the marginal cost with the respect to durability is nonincreasing, (a) the optimal durability for both the competitive firm and the monopolist decreases over time and (b) the monopolist will produce a good with lower durability than the competitive firm. We thus lend support for empirical findings and causal observations that found the phenomenon of declining durability over time. Collapse"
100437,1990,journal of economic dynamics and control,migration under uncertainty about quality of locations,migration under uncertainty about quality of locations,"""This paper develops a search-theoretic framework for analyzing migration decisions of workers who can only observe the quality of a location by migrating to the location after accepting a job there. After learning the location quality, the worker can stay at the current job, search locally at the same location, or search for a job at another location (leading to repeat migration). This paper develops the properties of optimal search, migration, and repeat migration decisions, and finds how revealed location quality influences the tradeoffs between local search and repeat migration. The effects of search costs and better job opportunities on the reservation levels of wages and revealed quality levels are also determined."" Collapse"
100438,1990,journal of economic dynamics and control,the solution of the infinite horizon tracking problem for discrete time systems possessing an exogenous component,the solution of the infinite horizon tracking problem for discrete time systems possessing an exogenous component,"Abstract In this paper we derive an algorithm that yields, for a discrete-time system, a control minimizing a quadratic cost functional. The system considered is linear and possesses an exogenous component. The cost functional is a quadratic tracking equation over an infinite time horizon with positive semi-definite weighting matrices such that a weighted sum of these matrices is positive definite. The infinite planning horizon Minimum Variance cost criterion and the Linear Quadratic regulator are special cases. For stabilizable systems we give a characterization of the asymptotically admissible reference trajectories. Collapse"
100439,1990,journal of economic dynamics and control,periodic linear-quadratic methods for modeling seasonality,periodic linear quadratic methods for modeling seasonality,Optimal linear regulator methods are used to represent a class of models of endogenous equilibrium seasonality that has so far received little attention. Seasonal structure is built into these models in either of two equivalent ways: periodically varying the coefficient matrices of a formerly nonseasonal problem or embedding this periodic-coefficient problem in a higher-dimensional sparse system whose time-invariant matrices have a special pattern of zero blocks. The former structure is compact and convenient computationally; the latter can be used to apply familiar convergence results from the theory of time-invariant optimal regulator problems. The new class of seasonality models provides an equilibrium interpretation for empirical work involving periodically stationary time series. Collapse
100458,1990,journal of economic history,the financial crisis of a.d. 33:  a keynesian depression?,the financial crisis of ad 33 a keynesian depression,No Result.
100459,1990,journal of economic history,scattering and contracts in medieval agriculture: challenges ahead,scattering and contracts in medieval agriculture challenges ahead,"Among the issues involved in understanding the organization of medieval agriculture, two have particularly interested economic historians: the manorial contractual arrangements between the lord of the manor and the peasants, and the scattering of peasant land holdings on open fields. While our understanding of each issue has improved, the symmetry between them needs further attention. Examining the issues separately is only a necessary first step; it is also important to consider their interdependence. I argue below that the explanations offered by Stefano Fenoaltea, Donald McCloskey, and Douglass North and Robert Thomas all need modification to address more adequately the facts of the overall organization of medieval agriculture.' Note, however, that my goal here is simply to alert future researchers to the symmetry between open fields and the manorial organization. Pointing out the incompleteness of earlier explanations offers a way to reopen the debate. Collapse"
100460,1990,journal of economic history,bank entry and the low issue of national bank notes:  a re-examination,bank entry and the low issue of national bank notes   a re examination,"Economic historians have long been puzzled by the low issue of national bank notes between 1882 and 1891. Circulation fell over 50 percent from $360 million in 1882 to $171 million in 1891 (see Table 1). Yet over that same period the number of national banks more than doubled. In 1890, when circulation had almost reached its lowest point, a confused Comptroller of the Currency wrote that new banks ""are now being organized with greater rapidity than in any time for the past twenty-five years.""1 Since bank note issue was the major privilege associated with a national bank charter, the anomaly of record numbers entering the system and ignoring note issue has puzzled historians. There are two long-standing theories explaining low note issue. The comptroller's theory links bank note profitability to the price of U.S. government bonds used as collateral to back note issue. Bond prices began rising in the mid-1880s, making note issue less profitable. Banks sold their bonds at a premium and circulation contracted. Note issue rebounded when the bond premiums disappeared after the treasury ran deficits for four consecutive years following the Panic of 1893. This theory was unchallenged until John James focused on the local lending rate as the key variable in determining profitability.3 James argued that a national bank faced two options after accepting a deposit: loan the deposit directly or purchase U.S. bonds as collateral and issue notes. The wider the spread between the loan rate and the bond rate, the more likely a bank would loan the deposit directly rather than tie up funds in low earning bonds. This opportunity-cost approach predicts that note issue should have been lower in regions with higher local lending rates (the South, Midwest, and Pacific). As regional interest rate differentials narrowed, around the turn of the century, note issue became more profitable in the South and West and circulation increased. Local lending rates and bond prices were important variables in determining profitability, but they ignored bank entry and have not adequately explained changes in circulation. I argue that changes in the regulatory environment help greatly to explain the movements in circulation and increased bank entry. The reduction in circulation can be traced to an 1882 change in the law that allowed banks to divest their bond holdings. The law also reduced barriers to entry and made national banking attractive to potential entrants. Circulation rebounded in 1900 when another change in the law made note issue more profitable and induced new banks to enter the system.4 Collapse"
100461,1990,journal of economic history,exchange rates and economic recovery in the 1930s:  an extension to latin america,exchange rates and economic recovery in the 1930s an extension to latin america,"In a study of ten European countries during the 1930s Barry Eichengreen and Jeffrey Sachs show that devaluation benefited the initiating countries and that there can be no presumption that depreciation was beggar-thy-neighbor.' I apply their methodology to ten Latin American countries. Examining the behavior of the Latin American economies is important in several respects: these countries were hit first by and reacted earlier to the effects of the Depression; they were part of the ""periphery"" and their policies did not directly affect the behavior of the ""center""; and their experience during the 1930s led them to select a subsequent inward-oriented path of development, based on import substitution.2 This model of development built upon a particular set of social and economic structures that were seen as shaping the world economic order, notably a center and a periphery with different structures of production. Unlike those of the periphery, the industrialized economies of the center are self-sustained; specifically, the periphery is constrained by an export sector confined to a very small range of goods and offering only limited backward and forward linkages with the rest of the economy.3 This note shows that Latin America reacted in the same way to depreciation as did Europe, and so offers evidence against the argument that Latin American economies were structurally constrained during this period. Collapse"
100481,1990,journal of economic literature,analysis and vision in the history of modern economic thought,analysis and vision in the history of modern economic thought,"dominate modern economic history, taking that phrase to refer to the 50-year period from 1939 to 1989. One is the increasing strain on, and eventual structural failure of, centralized planning in virtually all of the self-styled socialist world. The other, less dramatic, but of no less historical significance, is the continued success of capitalism in its major strongholds. In both cases, I use as the crucial but not sole indicator of success or failure the political fortunes of the two social orders. There have been economic successes for socialism-above all, the initial industrialization of the USSR and the early modernization of China; there have been economic failures of capitalisminstability, uneven growth, unsatisfactory income distributions, dangerous international imbalances. From the perspective of the present, however, the half century is remarkable for the political verdict that has finally been passed on the two systems. With few exceptions, socialism has experienced a public delegitimization without precedent in modern, perhaps in all, history; whereas despite its failures, capitalism has enjoyed an uncontestable, and probably rising degree of internal political support. In this paper I shall be concerned only indirectly with these historical developments, for my purpose is neither to describe nor to explain the contrasting fates of the two great social orders. Rather, I wish to review and interpret the manner in which modern developments have been perceived by economists. Thus, as my title indicates, this is an essay in the history of economic thought, not in economic history. But it would be disingenuous not to admit to a more pointed purpose of my investigation. It is to inquire into the successes and failures of economic thought in anticipating the march of actual events. It will come as no surprise that failures have considerably outweighed successes in this endeavor, even excluding the momentous, and utterly unforeseen happenings at the ""conclusion"" of the period in 1989. A few observers have offered prognoses of history's long line that were subsequently vindi- Collapse"
100482,1990,journal of economic literature,what is new-keynesian economics?,what is new keynesian economics ,"This research was supported by the National Science Foundation. I am grateful to George Williams for help with the data and to Steven Allen, Laurence Ball, Olivier J. Blanchard, Timothy Bresnahan, Charles Calomiris, Dennis Carlton, Robert Chirinko, Russell Cooper, Stanley Fischer, Herschel Grossman, R. Glenn Hubbard, David Laidler, John Leahy, Assar Lindbeck, N. Gregory Mankiw, David Romer, Julio Rotemberg, Dennis Snower, John Taylor, Andrew Weiss, and two anonymous referees for comments on one or more earlier drafts. I am also indebted to Michael Parkin and Edmund S. Phelps for helping to establish the etymology of the phrase ""new-Keynesian."" Collapse"
100487,1990,journal of economic perspectives,government failures in development,government failures in development,"This review of the development of national-level evaluation of Government policies and programs for eight European and North American countries illustrates two important points; one deliberate, the other, perhaps, accidental, but frankly admitted by the editor. the first is that the status of development of evaluation at the national level is poor and its future uncertain. Driven more by political demands than by an objective review of management needs and facing a climate of suspicion by departmental bureaucrats, advance in the use of this important tool is a matter of one and a half steps forward and one back. The second, which has bedeviled evaluation literature for years, is the enormous range of definitions of evaluation and the widely differing uses of terms such as program and policy evaluations. The editor admits at the outset of his opening chapter that &dquo;phrases like ’program evaluation,’ ’policy analysis,’ ’policy evaluation,’ ’policy studies,’ ’effectiveness audits,’ and ’policy forecasting’ were all used, and often interchangeably.&dquo; He is rightly troubled by the failure of the individual country reviews to resolve these confusions. Far from contributing to such resolution, the reviews cause the reader’s head to spin as these terms and others proliferate page upon page and breed further even more confusing offspring such as &dquo;component evaluation,&dquo; &dquo;evaluability assessment,&dquo; &dquo;policy program evaluation,&dquo; &dquo;evaluation research,&dquo; &dquo;sectoral social-science research,&dquo; &dquo;retrospective policy evaluation,&dquo; and many more. Until evaluation is defined to cover a limited part of the spectrum of analysis of implementation and impact of activities, this confusion will persist. And more than semantics is involved. If evaluation is perceived to include monitoring of progress, supervision of management performance, surveillance by central authorities over the actions of line departments, and auditing of the physical and financial records, it is inevitable that the political and governmental demands of evaluation will be in conflict with the evaluators’ perception of what evaluation should encompass. This point is well made in the concluding overview chapter by Derlien. Essentially the Collapse"
100488,1990,journal of economic perspectives,market failure and government failure,market failure and government failure,"F or several decades a debate has been raging in development economics on the relative virtues of the free market as opposed to state intervention. With the help of analytical models of a market economy, the interventionists demonstrate what they consider serious instances of ""market failure""-that is, the inability of a market economy to reach certain desirable outcomes in resource use. The protagonists of free markets, on the other hand, compile impressive lists of ill-conceived and counterproductive policy measures implemented by the governments of different countries at various times, leading to wasteful use of resources in their economies. This debate inevitably remains inlconclusive. The analytical results on "" market failure"" do not disappear in the face of the evidence that most governments (or for that matter most economies of less developed countries, with or without state intervention) have performed rather badly. When there appears to be scope for imnprovement over the market outcome, the search for the appropriate corrective measures goes on. Some protagonists of ""government failure"" question the significance of such market failures; others voice skepticism about the ability of governments to take any action in the economy which is not counterproductive; but none of them seems to be able to explain why a less developed country like India failed to grow during the first half of this century under a non-interventionist colonial adnministration. Thus, while the debate goes on, neither side succeeds in convincing the other. Collapse"
100489,1990,journal of economic perspectives,industrial policy in an export-propelled economy:  lessons from south korea's experience,industrial policy in an export propelled economy   lessons from south korea s experience,"Kr orea provides an illuminating case of state intervention to promote economic development. Like many other third world governments, Korea's government has selectively intervened to affect the allocation of resources among industrial activities. It has also used similar policies: taxes and subsidies, credit rationing, various kinds of licensing, and the creation of public enterprises, for example. But these policies have been applied in the context of a radically different development strategy, one of export-led industrialization. Moreover, Korea's economy has experienced exceptionally rapid development with relatively equitable distribution of the gains. This paper argues that the government's selective industrial policies have contributed importantly to Korea's rapid achievement of international competitiveness in a number of industries.' Though accepted by many knowledgeable observers, the conclusion is controversial. It is inherently so owing to insufficient historical information and lack of agreement about the required counterfactual. Reasons for believing that the benefits of selective intervention must have outweighed the costs also are considered. The discussion flags the policy implications of a Schumpeterian approach that views industrialization as a cascade of interlinked technological changes. The implications are no less controversial than is the interpretation of Korean experience. Collapse"
100490,1990,journal of economic perspectives,the latin american state,the latin american state,"Klaus J. Meyer-Arendt Department of Geology and Geography Mississippi State University Mississippi State, MS 39762 Research on Latin American and Caribbean tourism geography during the 1980s has increased greatly over previous decades. Tourism research is an interdisciplinary field of study, and research opportunities for geographers abound. Of the various academic and applied fields that have contributed to the great increase in tourism research, geographers have contributed a greater than proportional share. Of eleven major categories of research identified among the 1980s publications, which include geographic as well as related non-geographic sources, the most popular ones for geographers are tourism development, descriptive studies and historical studies. Tourism impact studies are also quite popular, not only in terms of general impacts but also specifically environmental impacts, economic impacts and socio-cultural impacts. A high proportion of the publications are on specific impacts, economic impacts and socio-cultural impacts. A high proportion of the publications on specific impacts were supplied by scholars in respective related fields. Also, a total of fourteen theses and dissertations were written on Latin American and Caribbean tourism topics, almost half in Canadian departments of geography. In terms of geographic distribution of the research foci of tourism studies in the 1980s, a map of Latin America exhibits dense clusters in Mexico and at several Caribbean island destinations and vast empty spaces across the South American continent. This pattern may reflect several important points: 1) few researchers are providing a large number of the publications; 2) tourism research is concentrated where English is the native language; 3 the distance-decay principle may keep most North American scholars closer to home; and 4) few Latin American/Caribbean geographers have examined their local tourism industries. In spite of tremendous increases having been made in the number of geographic and related non-geographic publications on Latin American and Caribbean tourism, the increases have been location-specific. The spectrum of research, ranging from explanatory to descriptive to predictive, is encouraging but the geographic applications of that research need to be greatly expanded throughout the Latin American realm. Perhaps these goals will yet be realized before this decade ends. INTRODUCTION In spite of world overpopulation, food shortages, war, and economic recessions, international tourism has become one of the prime revenue generators in the world today. The size of the tourism industry, whether measured by tourist arrivals, revenues derived from tourism, or employment in the tourism sector, steadily continues to expand. Tourism bas already become the primary ""export"" of many lesser-developed countries including several in Latin America and the Caribbean. In 1986, the World Tourism Organi:zation (WTO) estimated receipts from international tourism at 115 billion United States dollars (Pearce 1989). Latin America's share has been estimated at over 10 percent of the world total (Bar-On 1989). As tourism increases year by year, both in the world as well as in Latin America, there is an increasing need to fill research gaps. Knowledge is needed about: 1) the tourist destinations and their historical evolution as such; 2) the movement of tourists to and from and also between destinations; 3) the impacts of tourism, not just in general terms but specifically in terms of economic impacts, socio-cultural impacts, and environmental impacts; and 4) proper guidelines for tourist infrastructure development. Because of the nature of the tourism industry, replete with geographic themes such as population flows/spatial connectivity, perceptions of places, landscape change, environmental impacts, preservation/conservation/management and planning, geographers are well-positioned to help fill the research gaps in tourism studies. Ten years ago, Mings (1981) observed that in spite of rapid growth in the world tourism industry throughout the 1970s, a corollary expansion in tourism research was not evident. For the 1980s, the rate of correlation between tourism and tourism studies has improved, and much literature has become incorporated into the interdisciplinary body of tourism research. Geography has been well represented among the contributing Benchmark 1990, Conference of Latin Americanist Geographers, Vol. 17/18, p. 199 o Copyright 1992, Conference of Latin Americanist Geographers 199 academic disciplines such as anthropology, economics, history, sociology and political science as well as supporting applied fields such as business, hotel/motel management and leisure studies. In spite of tremendous strides having been made in filling research gaps during the past decade, particularly within the realm of geographic research, a review of available publications reveals a continued deficit of knowledge about the patterns, dynamics and impacts of tourism. PURPOSE This paper represents an effort to summariz.e the contributions made to research in tourism geography in Latin America and the Caribbean during the decade of the 1980s. Previous benchmark papers presented at the 1970 and 1980 Conference of Latin Americanist Geographers meetings summariz.ed tourism research from all academic disciplines as well as from government and consulting sources. In view of both the increase in the volume of geographic literature on tourism and also the specific preoccupation with geography, and perhaps also ancillary subfields from related disciplines, this benchmark paper intends to summariz.e geographic publications and a limited amount of related non-geographic literature. This latter category was subjectively evaluated with regard to its extent of overlap with the field of geography and interest of geographers. MEIBODOLOGY As a starting point in the compilation of bibliographic references, monthly issues of Current Geographical Publications were scanned under the ""Tourism,"" ""Recreation"" and ""Latin America"" headings, as were several chapters in the recent landmark volume on Geography in America (Gaile and Wilmott 1989). The Annals of Tourism Research, the pre-eminent quarterly devoted to social science aspects of tourism, was perused to cull articles and also to inspect the contents of the other major tourism journals which were, until last year, published at the end of the journal. The Proceedings and Yearbooks of the Conference of Latin Americanist Geographers were also examined, as was the American Geographical Society's Focus magazine. This is America's equivalent of the Geographical Magazine, which publishes a high proportion of tourism-related articles. Even the popular National Geographic Magazine was examined to see if tourism studies were highlighted. Also, to determine the extent of Latin American tourism research conducted at the Master's and Ph.D. levels, the Guide to Departments of Geography in the United States and Canada, Dissertation Abstracts International, and a summary article on Ph.D.-level tourism studies (Jafari and Aaser 1988) were examined. Excluded from the survey were: 1) papers presented at professio~ meetings, many of which were not very professional and therefore unpublishable; 2) clear non-geography references, which ranged from descriptive pieces in popular publications to highly-statistical input-output economic analyses; and 3) various tourist industry reports provided by individual countries, international organizations and private consulting firms. Mings (1981) made reference to numerous country-wide tourism baseline studies, market analyses, economic analyses and planning documents, but the extent of geographers' inputs was not made explicit. United States tourism consulting firms such as Davidson-Peterson Associates, Inc. of Maine have conducted numerous tourism studies for various Latin American and Caribbean nations, but most of these remain proprietary (T. L. Davidson, personal communication 1990). Tourism reports by inter-regional and international organizations such as the Organization of American States (OAS) and various branches of the United Nations, including the United Nations Environmental Programme, were omitted because of limited circulation and because these often comprise baseline data compilations rather than geographic analyses. The International Geographical Union (IGU) has several working groups investigating world tourism and its impacts and several projects are underway for the 1992 meetings. Past IGU publications did not appear in the Current Geographical Publications, however, and time constraints precluded an extensive international bibliographic search. THEMES IN LATIN AMERICAN TOURISM RESEARCH The literature search yielded a total of 95 references. Of this total, 50 citations were publications by geographers, 17 were theses and dissertations in geography, and 28 were publications by non-geographers and from nongeography journals but on topics related to research interests of tourism geographers. Although comparison with the earlier benchmark papers is difficult because of different sets of data analyz.ed, the 50 publications by geographers on Latin America tourism in the 1980s (not including theses and dissertations) greatly exceed the 8 Collapse"
100491,1990,journal of economic perspectives,innovation and cooperation:  implications for competition and antitrust,innovation and cooperation implications for competition and antitrust,"N k robel Laureate Robert Solow and his colleagues on MIT's Industrial Productivity Commission recently noted (Dertouzos, Lester, and Solow, 1989, p. 7): ""Undeveloped cooperative relationships between individuals and between organizations stand out in our industry studies as obstacles to technological innovation and the improvement of industrial performance"" and later (p. 105) that ""interfirm cooperation in the U.S. has often, though not always, been inhibited by government antitrust regulation."" These striking conclusions warrant further exploration. Unfortunately, industrial organization textbooks still discuss horizontal cooperation and competition almost exclusively in terms of standard cartel theory. (On the other hand, vertical cooperation/contracting is viewed differently, and some textbooks provide treatments of supplier-buyer relationships in which cooperation is viewed as enhancing efficiency.) Both in the textbooks and in policy discussion among economists, cooperation among competitors is highly suspect, being perhaps the last bastion of what was once referred to as the ""inhospitality tradition"" in antitrust. As a result, very little literature addresses how cooperation among competitors can promote competition, notwithstanding that cooperation among competitors may sometimes be essential if innovating firms are to compete in today's increasingly global markets (Imai and Baba, 1989). Such cooperation is already important in Japan and in Europe. 1 Collapse"
100492,1990,journal of economic perspectives,antitrust law and innovation cooperation,antitrust law and innovation cooperation,"The issue posed by the Jorde-Teece paper and by several bills proposed in Congress is whether the antitrust laws should be modified to promote collaboration in innovation between business firms, especially in high technology industries. Jorde and Teece and other proponents of such legislation argue that in the arena of world competition, the United States cannot afford to have antitrust laws that are more restrictive of innovation collaboration than the laws of Europe and Japan. They argue that since there is very little risk of cartelization in rapidly evolving high technology industries, the benefits from permissive antitrust can be achieved at virtually no cost. My own analysis will conclude that innovation collaboration, particularly when it encompasses production and marketing, can create anticompetitive risks, and should be subject to the antitrust laws. It appears unlikely that actual antitrust enforcement inhibits technological collaboration in any direct way because government enforcement is extremely permissive and no successful private cases have been brought in recent years. To the extent that misguided perceptions of antitrust risk may have discouraged some types of innovation collaboration, a few narrowly targeted reforms are sufficient to correct the problem. Collapse"
100493,1990,journal of economic perspectives,on the antitrust treatment of production joint ventures,on the antitrust treatment of production joint ventures,No Result.
100494,1990,journal of economic perspectives,the affinity between ownership forms and coordination mechanisms:  the common experience of reform in socialist countries,the affinity between ownership forms and coordination mechanisms the common experience of reform in socialist countries,"The title of the session in which this paper was given, ‘The Theory of Markets in a Planned Economy’, suggests two alternative approaches. One is that of a normative theory — that is, the elaboration of a theoretical blueprint for an economy, relying upon both the plan and the market. Whatever might be the significance of such an approach, this paper takes as its basic themes the other approach, namely positive analysis. A reform process is under way in several socialist countries. The course of thought applied in the paper is based on the hypothesis that in all reform countries it is possible to identify certain common tendencies. Of course, each reforming country constitutes a unique case, and one must look hard to find suitable ways of discerning common patterns of reform in countries as diverse as Yugoslavia, Hungary, China, Poland and the USSR. One must accomplish another equally difficult task as well, that is the task of properly evaluating the cases of failed reform such as the one which was attempted in 1968 in Czechoslovakia. Nevertheless, there exist many common lessons, and in this paper we will attempt to delineate some of them. Collapse"
100496,1990,journal of economic perspectives,on the economics of crime and confiscation,on the economics of crime and confiscation,No Result.
100497,1990,journal of economic perspectives,foreign exchange,foreign exchange,"most (all?) behavior can be explained by assuming that agents have stable, well-defined preferences and make rational choices consistent with those preferences in markets that (eventually) clear. An empirical result qualifies as an anomaly if it is difficult to ""rationalize,"" or if implausible assumptions are necessary to explain it within the paradigm. This column will present a series of such anomalies. Readers are invited to suggest topics for future columns by sending a note with some reference to (or better yet copies of) the relevant research. Comments on anomalies printed here are also welcome. The address Collapse"
100499,1990,journal of economic perspectives,tax credits for low-income workers with children,tax credits for low income workers with children,"Understanding the causes of the rise in disability rolls lies at the heart of policies concerned with the interaction of working life, family well-being, and a country’s social safety net. To explore the long-run drivers of disability insurance (DI) receipt, we use administrative tax data that allows us to link young adults (ages 24-34) to their parents. Our findings are threefold. First, DI receipt is strongly linked to the income of the recipient’s parents, with rates for young adults from the poorest families roughly six times higher than those from the richest families. Second, children from low income families display sharply varying probabilities of receiving DI depending on the place where they grew up, while those from rich families show no similar differences. Suggestive evidence indicates that roughly 50% of these place-based differences are causal. Third, places where poor children grow up to have the highest rates of DI receipt tend to be “good” areas based on many standard characteristics, including lower inequality, lower segregation, higher school quality, and higher social capital. 1. Background and Literature A striking pattern over the past few decades is the large and steady rise in participation rates in various sickness and disability related programs. Of particular interest is the rise in disability insurance (DI) receipt. This is in part because DI is the largest social insurance program in most industrialized countries, but also because it is usually an absorbing state: few individuals who go onto DI re-enter the work force at a later date. For example, over the past 50 years DI rolls have steadily risen from less than 1% to 6% 1 This research was supported by the U.S. Social Security Administration through grant #1 DRC1200000204 to the National Bureau of Economic Research as part of the SSA Disability Research Consortium. The findings and conclusions expressed are solely those of the author(s) and do not represent the views of SSA, the U.S. Department of Treasury, any other agency of the Federal Government, or the NBER. of the adult population in the U.S. (Autor and Duggan, 2006, Burkhauser and Daly, 2012). Prominent researchers have argued that such rises in disability insurance rolls are fiscally unsustainable (Autor and Duggan, 2006), especially as current DI recipients are younger and have longer life expectancies on average compared to previous cohorts of recipients. Understanding the causes of the rise in disability rolls lies at the heart of policies concerned with the interaction of working life, family well-being, and a country’s social safety net. To date, research has largely focused on describing non-medical factors correlated with the probability of claiming disability benefits, such as economic conditions, local allowance rates, and age. For instance, DI applications and awards spike during recessions and fall off during boom years (Black, Daniel and Sanders 2002, Autor and Duggan 2003), a pattern that held strongly as DI applications rose during the Great Recession (Mueller, Rothstein and von Wachter 2015). Less educated workers and older workers are also more likely to claim disability benefits (SSA 2014). There is also considerable variation in disability receipt across areas related to compositional differences in the population with respect to age, education, and industrial structure (Ruffing 2015). While this research has been important in describing certain correlates of DI receipt, it has been limited in its ability to look at long-term factors that shift an individual’s chances of DI receipt. Another limitation is that only a few existing studies try to distinguish between selection and causation in the factors that predict DI receipt. Sorting out these scenarios is central to understand how economic conditions or government policies may affect disability rolls. In this paper, we try to address these limitations, making two key contributions. First, we use U.S. administrative tax data to link young adults to their parents, and carefully describe the long-term predictors of DI receipt. In particular, the size and detailed nature of the data we are using allows us to describe the relationship between parents’ income and the probability that the child participates in DI as young 2 This trend is not specific to the U.S., as documented by OECD (2010). In the U.K., for example, DI rolls have steadily risen from 1% to 7% over the past 50 years. 3 A notable exception is Dahl et al. (2014). They take advantage of random assignment of judges to DI applicant to show that DI receipt in one generation is causing DI participation in the next generation. adult. Using our rich data, we characterize how these intergenerational relationships vary across areas within the U.S. The geographic variation in the association between parents’ income and children’s DI participation could be driven by two very different sources. One possibility is that neighborhoods have causal effects on the chances an individual claims DI: that is, moving a given child to a different neighborhood would change her likelihood of participating in DI. Another possibility is that the observed geographic variation is due to systematic differences in the types of people living in each area, such as differences in demographic makeup or wealth. The second contribution of our paper is to test these explanations and identify the causal effects of neighborhoods by studying families who move across counties and exploiting differences in their children’s ages when they move. This strategy has previously been used by Chetty and Hendren (2015) to study intergenerational income mobility. 2. Data Our dataset is the universe of IRS administrative tax data from 1996-2014. Our sample of potential DI claimants includes those born in the 1980-1990 cohorts. We measure DI receipt for young adults (ages 24-34) through the receipt of Form 1099-SSA, which the SSA files with the IRS for all DI payments. (Our data do not include SSI payments.) We cannot distinguish disabled workers from other claiming benefits (spouses, adult children, or dependents), but for individuals receiving SSDI payments at ages 24-34, just 2% of program recipients are spouses and dependents would be ineligible. Adult children are a greater concern, but our approach to study hazard rates (rather than the stock of DI recipients) minimizes this concern, since most adult children begin to receive benefits before age 24. We then link young adults to their parents by finding the household that claims each child as a dependent for tax purposes. This procedure is especially effective for lowincome children, whose parents often receive large tax credits as a result of filing; 4 Raj Chetty and John N. Friedman accessed these data under contract TIRNO-16-E-00013 with Statistics of Income (SOI) Division of IRS. altogether, previous work in these data has linked 95% of all children to a household in this way (Chetty et al. 2014). We measure household income for the parents using adjusted gross income (AGI) from income tax returns, imputing this income from various information returns (including W-2s, 1099-SSA, and 1099-UI) for non-filers, using data from 1996-2000 (which is the earliest that we can observe parental income). We then rank parents’ income against other households with children in the same cohort; this within-cohort ranking helps adjust for differences in the age of income measurement or in the calendar years at which income is measured. While these households may not include a child’s biological parents, they do represent circumstances in which the child grew up (to simplify language we refer to such households as “parents”). We drop young adults whom we cannot link to their parents in this way. Including all 11 cohorts, this leaves us with a sample of 38.4 million young adults and 222.4 million individual-year observations. Table I presents summary statistics for the key variables in our analysis. In Panel A, we present data at age 24, the only year when we have data for all 11 of our cohorts. The average DI rate in the full sample is 0.66%. Panel B presents the same statistics at age 34 (for cohort 1980 only). At that age, 2.0% of individuals receive SSDI payments. It is also worth noting that 2.5% of individuals at age 34 have received SSDI income at some point since age 24; thus, 20% of individuals ever receiving income from the program have left. This reflects (as least in part) a somewhat larger recovery rate for young adults; the comparable rate for disabled beneficiaries on average across the entire program is substantially lower. It is also possible that changing relationship to a beneficiary (e.g., divorced spouse) accounts for some of this, but the preponderance of disabled workers among beneficiaries at these ages implies that this should be a relatively small share of those leaving the program. We can also calculate, for each individual in each year, whether they are covered by the SSDI program. SSA rules mandate that individuals work a minimum number of quarters of coverage (QCs) before applying to DI, where a worker earns one QC for each $1,260 (in 2016) of covered earnings up to a maximum of four QCs per year. (Despite the label “quarters,” it does not actually matter when in the year workers earn this income; for example, a worker may earn all four credits in January even if she does not work in any other month.) For each worker in each year, we calculate the number of QCs earned by dividing the sum of Social Security Wages (W-2, Box 3) and Net SelfEmployment Income (Schedule SE, Box 4 (Short Schedule) or Box 6 (Long Schedule)) by the annual QC amount. We then compare an individual’s accrued QCs to the minimum number required for eligibility. This minimum varies by age; individuals must have accumulated a minimum of 2*(Age – 21) QCs since the time they were 21 years old. For instance, a 27-year-old must have earned at least 12 QCs after turn Collapse"
100557,1990,journal of finance,information content of insider trading around corporate announcements: the case of capital expenditures,information content of insider trading around corporate announcements the case of capital expenditures,"There is gathering evidence of insider trading around corporate announcements of dividends, capital expenditures, equity issues and repurchases, and other capital structure changes. Although signaling models have been used to explain the price reaction of these announcements, a usual assumption made in these models is that insiders cannot trade to gain from such announcements. An innovative feature of this paper is to model trading by corporate insiders (subject to disclosure regulation) as one of the signals. Detailed testable predictions are described for the interaction of corporate announcements and concurrent insider trading. In particular, such interaction is shown to depend crucially on whether the firm is a growth firm, a mature firm, or a declining firm. Empirical proxies for firm technology are developed based on measures of growth and Tobin's q ratio. In the underlying ""efficient"" signaling equilibrium, investment announcements and net insider trading convey private information of insiders to the market at least cost. The paper also addresses issues of deriving intertemporal announcement effects from the equilibrium (cross-sectional) pricing functional. Other announcement effects relate the intensity of the market response to insider trading, variance of firm cash flows, risk aversion of the insiders, and characteristics of firm technology (growth, mature, or declining). Collapse"
100560,1990,journal of finance,equilibrium exchange rate hedging,equilibrium exchange rate hedging,"In a one-period model where each investor consumes a single good, and where borrowing and lending are private and real, there is a universal constant that tells how much each investor hedges his foreign investments. The constant depends only on average risk tolerance across investors. The same constant applies to every real foreign investment held by every investor. Foreign investors are those with different consumption goods, not necessarily those who live in different countries. In equilibrium, the price of the world market portfolio will adjust so that the constant will be related to an average of world market risk premia, an average of world market volatilities, and an average of exchange rate volatilities, where we take the averages over all investors. The constant will not be related to exchange rate means or covariances. In the limiting case when exchange risk approaches zero, the constant will be equal to one minus the ratio of the variance of the world market return to its mean. Jensen's inequality, or ""Siegel's paradox,"" makes investors want significant amounts of exchange rate risk in their portfolios. It also makes investors prefer a world with more exchange rate risk to a similar world with less exchange rate risk. Collapse"
100561,1990,journal of finance,default risk in futures markets: the customer-broker relationship,default risk in futures markets  the customer broker relationship,"The traditional view of the futures clearinghouse as an insurer that eliminates the need for customers to evaluate default risk is inaccurate. A clearinghouse member default in 1985 confirms that the clearinghouse only guarantees payment from member to member, not from customer to customer or member to customer. Thus, non-defaulting customers are subject to losses as a result of the action of individuals with whom thay have no contractual obligations. This study models the behavior of customers choosing a futures commission merchant (FCM) given the current legal position of the clearinghouse. In a single-period model with symmetric information, customers can eliminate their exposure to defaults of other customers or of their FCM only by choosing to trade through ""boutique"" (undiversified) FCMs. In practice, monitoring and rebalancing costs may impede the attainment of zero default risk. However, FCM diversification remains an important factor in customer choice of an FCM. When setting capital requirements, clearinghouses and government regulators need to consider the implications of diversification for both customer and market protection. FUTURES MARKETS PROVIDE A well-recognized service to the economy by allowing for broader access to the means for reducing risk. Futures markets are distinguished from forward markets largely by the role played by the clearinghouse and its members. The clearinghouse has been viewed by Telser and Higinbotham (1977), Black (1976), Edwards (1983), and others as providing an important ""insurance"" function. Each trader need not worry about the risk of default by the other side of the contract because the clearinghouse presumably plays the short for every long and long for every short. Traders need only be concerned about the ability of the clearinghouse to live up to the terms on the futures contract. One feature believed to enhance confidence in the clearinghouse is the requirement that margin be posted at the clearinghouse by each side of the contract (see Telser (1981), Garbade and Silber (1983), and Brennan (1986)). Given daily resettlement on futures contracts, if one side of the contract defaults, the clearinghouse can close the position and use the margin to satisfy the other side of the contract. In practice, the clearinghouse requires only that the broker, or futures commission merchant (FCM), post the margin and, within some constraints, leaves the FCM to set requirements with its own customers. Collapse"
100582,1990,journal of finance,forward and futures prices:  evidence from the foreign exchange markets,forward and futures prices evidence from the foreign exchange markets,"CORNELL AND REINGANUM (1981), hereafter CR, report that price differentials for future contracts and forward contracts are statistically insignificant in foreign exchange markets. Based on this finding, CR conclude that marking-to-market is insignificant in the formulation of currency futures prices. This note identifies two potential concerns with the CR tests. One problem relates to the timing of delivery dates for ""matched"" contracts. A second problem relates to the time period for the CR study. We show that correcting for these problems does not affect the overall conclusions of the CR study; marking-to-market does not appear to have a significant effect on currency futures prices. Collapse"
100583,1990,journal of finance,changes in the cost of intermediation:  the case of savings and loans,changes in the cost of intermediation the case of savings and loans,"The minimum cost output configuration for a firm may change as the result of a variety of factors, including changes in market structure. In this paper we test this structural change hypothesis with savings and loan data. We find support for the hypothesis that separable, constant returns to scale production functions characterize the average savings and loan in our sample in 1983. This is in contrast to the cost complementarities found in 1978. We argue that this result may be the result of regulatory changes that allowed savings and loans to alter their production mix to fully capture the benefits of joint production. IT SEEMS REASONABLE TO assume that the cost minimizing configuration of a multiproduct firm is potentially subject to nonstationarity during periods of structural change in input and/or output markets. The late 1970s and early 1980s represent such a period of turbulence for many markets in the financial services industry. Volatile interest rates, combined with technological and regulatory changes, may have caused firms in this industry to alter their output mixes in an effort to capture cost complementarities from joint production. The purpose of this paper is to test this hypothesis of structural change in the savings and loan industry using both a translog and minflex Laurent translog (MLT) specification of the production function. We have chosen to investigate savings and loans (S&Ls) during the late 1970s and early 1980s for two reasons. First, S&Ls provide an ideal vehicle for testing our hypothesis because they were restricted almost exclusively to mortgages and securities as outputs before the passage of the Depository Institutions Deregulation and Monetary Control Act of 1980 and the Garn-St. Germain Act of 1982. Moreover, there were also market changes with respect to both the pricing and mix of inputs (i.e., deposits). Our second rationale involves the fact that there is a large and growing body of literature in the area of what is now called the Collapse"
100584,1990,journal of financial and quantitative analysis,estimation of stock price variances and serial covariances from discrete observations,estimation of stock price variances and serial covariances from discrete observations,"Stock price discreteness adds noise to price series. The noise increases return variances and adds negative serial correlation to return series. Standard variance and serial covariance estimators therefore overestimate the variance and serial covariance of the underlying stock values. Discreteness-induced variance and serial covariance depend on underlying volatility and on the size of the bid/ask spread. Simple formulas for approximating the effects of discreteness on variance and serial correlation are derived and presented. The approximations, which are accurate in daily data, can be used to adjust the standard variance and serial covariance estimators. Collapse"
100585,1990,journal of financial and quantitative analysis,monetary regimes and the relation between stock returns and inflationary expectations,monetary regimes and the relation between stock returns and inflationary expectations,"This paper analyzes the impact of changes in monetary policy regimes on the relation between stock returns and changes in expected inflation. Post-war evidence from four countries reveals a direct link between these relations and the central banks' operating targets (i.e., money supply or interest rates). Specifically, the post-war negative relations between stock returns and changes in expected inflation are significantly stronger during interest rate regimes. Collapse"
100586,1990,journal of financial and quantitative analysis,time-varying return and risk in the corporate bond market,time varying return and risk in the corporate bond market,"This paper examines the pricing of exchange-traded long-term corporate bond portfolios. Observable instruments measuring the term structure of interest rates, levels of bond and stock prices, and a January dummy are found to predict excess returns on corporate bonds. An intertemporal asset pricing model with changing expectations and unobservable factors is then estimated for the predictable excess returns using Hansen's Generalized Method of Moments. The results show that a multibeta linear time-varying model of con? ditional expected returns with constant betas can successfully value corporate bonds. Spe? cifically, the tests indicate the presence of two time-varying hedge portfolios. The data, however, support a single latent variable specification when all January observations are excluded. This result suggests the existence of a strong January seasonal in one of the latent variables. Collapse"
100587,1990,journal of financial and quantitative analysis,securityholder taxes and corporate restructurings,securityholder taxes and corporate restructurings,"Previous studies have found that positive abnormal stock returns are associated with corporate spin-offs and divestitures. Using a simplified model of the process of investor tax trading, we show that an improvement in the value of the tax-timing option component of securities prices is a likely contributing factor to those abnormal returns. The analysis indicates that the same phenomenon also may be part of the explanation for the generally higher returns observed for spin-offs than for divestitures, both when leverage is and is not present in the restructuring transactions. Collapse"
100588,1990,journal of financial and quantitative analysis,the heterogeneous investment horizon and the capital asset pricing model:  theory and implications,the heterogeneous investment horizon and the capital asset pricing model theory and implications,"This paper generalizes the risk-return relationship implied by the traditional capital asset pricing model with finite investment horizons. It examines the effect of heterogeneous investment horizons on the functional form of capital asset pricing and proposes a translog model for estimating the risk-return relationship. In addition, this paper contends that some empirical findings that are inconsistent with the traditional CAPM have resulted from misspecification of the CAPM by ignoring the discrepancy between the observed data periods and the true investment horizons. Finally, the paper shows that under various conditions, the translog model is a suitable function for estimating the relationship between risk and expected returns. Collapse"
100589,1990,journal of financial and quantitative analysis,the relation between risk and optimal debt maturity and the value of leverage,the relation between risk and optimal debt maturity and the value of leverage,"This paper considers the capital structure and debt maturity choice for a value-maximizing corporation. In the model, interest expense is tax deductible, bankruptcy is costly, and debt is fairly priced at issue. In contrast to the results of Kane, Marcus, and McDonald (1985), optimal debt maturity does not always approach zero in the absence of transaction costs, and is increasing in the volatility of the assets of the firm. The model predicts a positive association between the value of leverage and total risk in some circumstances. Collapse"
100590,1990,journal of financial and quantitative analysis,"stock returns, money, and fiscal deficits",stock returns money and fiscal deficits,"Using the FPE/multivariate Granger-causality modeling technique, this paper tests whether changes in Canadian stock returns are caused by a number of economic variables, including base money and fiscal deficits. The empirical results from monthly data show that lagged changes in fiscal deficits, in particular, Granger-cause stock returns. If expected returns to equity are not time-varying, such a finding appears inconsistent with market efficiency. Collapse"
100592,1990,journal of financial and quantitative analysis,a nonparametric distribution-free test for serial independence in stock returns:  a correction,a nonparametric distribution free test for serial independence in stock returns   a correction,"A fundamental statistical test of serial independence developed by Ashley and Patterson (1986) to examine a possible form of serial dependence in daily stock returns is shown to be improperly constructed. As a consequence, the significance probabilities that they ob? tain are overstated. This paper presents a corrected version of their test. The test statistic obtained after correction is shown to possess the same limiting distribution as the Kolmogorov-Smirnov test statistic. Applying the corrected test procedure to data identical to that used by Ashley and Patterson, we find that their original null hypothesis can no longer be rejected at conventional significance levels. Collapse"
100622,1990,journal of human resources,assessing empirical approaches for analyzing taxes and labor supply,assessing empirical approaches for analyzing taxes and labor supply,"Recent surveys on the labor-supply responses of men document a divergence in the estimates of substitution and income effects obtained using various estimation approaches. Generally, studies accounting for nonlinear tax schedules in a static setting via a piecewise-linear approach produce estimates that typically imply higher substitution and lower income responses than are suggested by empirical work applying other approaches. This paper demonstrates that maximum likelihood estimation of a consumer-choice problem with nonlinear budget sets implicitly relies on the satisfaction of inequality constraints that translate into behaviorally meaningful restrictions. These constraints arise not as a consequence of economic theory, but instead as a requirement to create a properly defined statistical model. In the analysis of piecewise-linear budget sets, the implicit constraints required by maximum likelihood in estimation amount to imposition of Slutsky conditions at all wage-income combinations associated with kink points. In the analysis of differentiable budget sets, the tacit constraints invoked by maximum likelihood also involve inequality restrictions on Slutsky terms. The empirical work presented in this study supports the contention that these implicit constraints play a major role in explaining the discrepancies in estimates found in the literature on men's labor supply. Collapse"
100624,1990,journal of human resources,"labor supply, income taxes, and hours restrictions in the netherlands",labor supply income taxes and hours restrictions in the netherlands,"In this paper, two models of individual labor supply are discussed. The first one is the by now classical Hausman-type model with convex piecewise linear budget constraints, in which both random preferences and optimization errors are incorporated by means of normally distributed random variables. Estimated coefficients are plausible but the model has the shortcoming that unemployment for males is not captured and that the simulated hours distribution misses the spikes in the sample distribution of working hours. Therefore, an alternative model is introduced which explicitly takes into account demand side restrictions on working hours. The difference with the standard model is the replacement of the optimization error by the assumption that each individual can choose from a finite set of wage hours packages and either picks the job offer yielding highest utility or decides not to work. It turns out that this model captures the sample distribution of working hours very well, for males as well as females. Wage and income elasticities according to the two models are similar and in line with other recent findings in The Netherlands. Dead weight loss calculations for the second model which explicitly take the hours restrictions into account, imply that the dead weight loss is much smaller than as calculated with the standard model.(This abstract was borrowed from another version of this item.) Collapse"
100671,1990,journal of the japanese and international economies,do the japanese elderly reduce their total wealth?  a new look with different data,do the japanese elderly reduce their total wealth a new look with different data,ERR
100672,1990,"journal of law, economics, and organization",the politics of government decision making:  regulatory institutions,the politics of government decision making regulatory institutions,"Supported by the Ford Foundation, the Pew Charitable Trust, the Guggenheim Foundation, the Center for Energy Policy Research at MIT, the National Science Foundation and the French Ministere de l'Education Nationale. Collapse"
100674,1990,"journal of law, economics, and organization",diseconomies of scale in employment contracts,diseconomies of scale in employment contracts,No Result.
100675,1990,"journal of law, economics, and organization","optimal deterrence, uninformed individuals, and acquiring information about whether acts are subject to sanctions",optimal deterrence uninformed individuals and acquiring information about whether acts are subject to sanctions,ERR
100676,1990,"journal of law, economics, and organization","investment ""myopia"" and the internal organization of capital allocation decisions",investment myopia and the internal organization of capital allocation decisions,"The principal objective of this paper is to show that a firm's internal organization of the capital investment decision as well as its capital allocation choices are significantly influenced by the necessity for the firm to raise capital from uninformed investors who reflect their strategic disadvantage in pricing the capital they provide. I focus on moral hazard (a brief discussion in the Appendix shows that the results are unchanged by private information). There is symmetric information prior to contracting, but those who buy claims against the firm's assets cannot directly and costlessly control the firm's actions that affect the productivity of these assets. This moral hazard affects asset valuation (see also Ramakrishnan and Thakor). The fact that informational problems are intricately linked to how firms are internally organized is now widely accepted (see, for example, Holmstrom, 1988, and Williamson, 1967, 1970). Moreover, lolmnstrom (1988) and Sah and Stiglitz also make the point that real investment activity is influenced by how investment decisions are internally organized within firms. To this literature the intended contribution of this paper is the observation that information-related frictions can lead value maximizing firms to not adhere Collapse"
100677,1990,"journal of law, economics, and organization",the comparative advantage of long-term contracts and firms,the comparative advantage of long term contracts and firms,"This paper analyzes the choice between long-term contracts and firms. The basic setup is a two-stage version of a vertical integration problem. In stage one, the upstream and downstream managers, respectively, expend costreducing and revenue-enhancing effort. In the second stage, managers observe private information about upstream costs and downstream revenues, and a quantity decision is made. This paper focuses on the ex ante choice between long-term contracts and firms, before specific investment. The central contribution is to provide a simple, testable theory of the implicit efficiency tradeoffs between long-term contracts and firms. ""Longterm contracts"" are defined as arrangements where parties prespecify a nonlinear (transfer) pricing schedule before investment in specific assets. One firm then sets output, and each manager is compensated out of the residual earnings of the portion of production they manage. Under a ""firm"" one of these stages is owned by the other, and the manager of that stage is then Collapse"
100678,1990,"journal of law, economics, and organization","time, space, and shopping:  the regulation of shopping hours",time space and shopping the regulation of shopping hours,"Few would dispute that shopping is costly and that time is among the more significant costs. Time enters the cost of shopping in a number of different ways, perhaps most directly by diverting from other activities the time needed to plan purchases, search for selection, travel to store locations, and queue for service. Because the time spent shopping is often independent of the quantity of goods purchased, households economize by choosing to shop discretely through time. An optimal shopping frequency is chosen by trading off the falling (per unit time) cost of postponing a shopping trip with the rising cost of holding higher levels of inventories. While time costs may be independent of the quantities purchased, the time spent shopping is not independent of the store's location. Reducing the distance between a household and its shopping area reduces directly the time spent in each trip and lowers the cost of the optimal plan. Indirectly, the time spent shopping can be reduced by combining shopping with other activities (e.g., dropping the laundry off on the way to work or buying milk on the way home from a party). The household's ability to take advantage of the particularities of time and place is a function of the length of time that Collapse"
100679,1990,"journal of law, economics, and organization",optimal prosecution of defendants whose guilt is uncertain,optimal prosecution of defendants whose guilt is uncertain,"When a crime has been committed and a suspect (or suspects) apprehended, it is the purpose of the criminal justice system to dispose of the case in the socially optimal manner. Of course, how one should define optimal in this context is subject to debate; in this paper I consider three goals: (1) minimization of the social cost of legal errors; (2) punishment of the truly guilty; and (3) efficient use of the resources needed to operate the system. Considering the first of these, legal errors necessarily result from imperfect information on the part of courts and are generally of two types: incorrect conviction of innocent defendants (type I errors), and incorrect acquittal of guilty defendants (type II errors). Both are assumed to impose some cost on society. On the other hand, society presumably gains from correctly punishing the truly guilty. One reason, of course, is deterrence, although I abstract from this motive here. Additionally, however, I assume that society benefits by matching the punishment of criminals to the crime they have committed (one might call this ""retribution"").2 By the appropriate choice of the variables of the legal process-namely, prosecutorial effort in collecting evi- Collapse"
100680,1990,"journal of law, economics, and organization",rings and promises,rings and promises,"The ALS-U upgrade promises to deliver diffraction limited performance throughout the soft x-ray range by lowering the horizontal emittance to about 50 pm resulting in 2-3 orders of brightness increase for soft x-rays compared to the current ALS. The design utilizes a multi bend achromat lattice with on-axis swap-out injection and an accumulator ring. One central design goal is to install and commission ALS-U within a short dark period. This paper summarizes the status of the conceptual design of the accelerator, as well as some results of the R&D program that has been ongoing for the last 3 years. Collapse"
100681,1990,"journal of law, economics, and organization",the framing hypothesis:  is it supported by credit card issuer opposition to a surcharge on a cash price?,the framing hypothesis is it supported by credit card issuer opposition to a surcharge on a cash price,"In the spring of 1984 the American Express Company mailed to 8 of its 13 million card holders an elaborate appeal urging them to send a card to their Congressman urging an extension of the then existing federal ban on credit card surcharges-additional charges imposed by the seller on purchases made with the use of a credit card. Some three million pieces of mail opposing surcharges were subsequently received on Capitol Hill.1 Federal law has, however, permitted cash discounts-a reduction from the price allowed by the seller on purchases made by cash rather than made with the use of a credit card. To that American Express made no objection. Since a discount for cash is the equivalent of a surcharge for the use of a credit card, the American Express objection to the use of one but not the other seems puzzling. Why would American Express be more concerned about a surcharge for credit card use than a discount for cash? One scholar who had offered an answer to that question a few years earlier is Richard Thaler (39). He says: Collapse"
100682,1990,"journal of law, economics, and organization",an analysis of the stock price effect of the 1986 ohio takeover legislation,an analysis of the stock price effect of the 1986 ohio takeover legislation,"In November of 1986, takeover-related legislation was enacted by the state of Ohio (Amend. Subst. H.B. No. 902) in response to the attempted takeover of Goodyear Tire by James Goldsmith. The quick passage of this legislation provides an opportunity to examine whether stock prices of Ohio firms reacted to its enactment. Two related studies have been published regarding this price reaction, the first by The Securities and Exchange Commission's Office of the Chief Economist (Ryngaert, Netter, Belk, and Poulsen) and the second by Ryngaert and Netter.' In this paper, we present our own analysis of the stock price effect, which differs in several ways from that used in these studies and results in a different conclusion. Specifically, we conclude that there were no significant effects from the 1986 legislation, with respect to both average stock prices and shareholder wealth, while the earlier studies report a significant negative effect on share prices. This variance in conclu- Collapse"
100683,1990,"journal of law, economics, and organization",shareholder wealth effects of the 1986 ohio antitakeover law revisited:  its real effects,shareholder wealth effects of the 1986 ohio antitakeover law revisited its real effects,"In an earlier published paper (Ryngaert and Netter), we argue that the November 1986 Ohio antitakeover legislation provided a unique opportunity to test competing hypotheses about the shareholder wealth effects of antitakeover legislation. 1 The Ohio law provided an excellent setting for this test since the bill's introduction was a surprise to the market and its passage was accomplished in a short period of time.2 We found that the Ohio firms that would be expected to be most affected by the legislation suffered significant stock price declines in the period when news about the legislation became public. Our paper has been criticized by Margotta, McWilliams, and McWilliams (hereafter MMM), who conclude that their analysis of the stock price reaction to the 1986 Ohio legislation finds no significant shareholder wealth effects. In this paper, we argue that our original results concerning the shareholder wealth effects of the Ohio legislation are correct and that the MMM results, if anything, support our original conclusions. Collapse"
100684,1990,"journal of law, economics, and organization",a rational choice theory of supreme court statutory decisions with applications to the  state farm and  grove city cases,a rational choice theory of supreme court statutory decisions with applications to the state farm and grove city cases,ERR
100686,1990,"journal of law, economics, and organization",auctions and contract enforcement,auctions and contract enforcement,"Pt I: Statutes: now includes the relevant sections of the Marine Insurance (Gambling Policies) Act 1909 the Auctions (Bidding Agreements) Act 1927 the Accommodation Agencies Act 1953 the Corporate Bodies' Contracts Act 1960 the Carriage of Goods by Road Act 1965 the Arbitration (International Investment Disputes) Act 1966 the Administration of Justice Act 1970 the Prices Act 1974 the Prices Act 1975 the Industry Act 1975 the Insurance Brokers (Registration) Act 1977 the Banking Act 1979 the Business Names Act 1985 the Weights and Measures Act 1985 the Food Safety Act 1990 the Arbitration Act 1996 Late Payment of Commercial Debts (Interest) Act 1998 the Competition Act 1998 and the Data Protection Act 1998, Pts I, II, Schs 1-4.In addition, the Contracts (Rights of Third Parties) Bill will be included when it receives Royal Assent.Pt II: Statutory Instruments: This Part has also been expanded and now contains a wider range of instruments, including the Price Marking Order 1991, SI 1991/138, the Timeshare (Cancellation Notices) Order 1992, SI 1992/1942 the Timeshare (Repayment of Credit on Cancellation) Order 1992, SI 1992/1943 the Property Misdescriptions (Specified Matters) Order 1992, SI 1992/2834 the Price Indications (Resale of Tickets) Regulations 1994, SI 1994/3248 the Trading Schemes Regulations 1997, SI 1997/30 the Trading Schemes (Exclusion) Regulations 1997, SI 1997/31 the European Community Competition Law (Articles 88 and 89) Enforcement Regulations 1996, SI 1996/2199 the Foreign Package Holidays (Tour Operators and Travel Agents) Order 1998, SI 1998/1945 the Telecommunications (Data Protection and Privacy) (Direct Marketing) Regulations 1998, SI 1998/3170.Pt III: European and International Materials. Collapse"
100687,1990,"journal of law, economics, and organization",the english rule for allocating legal costs: evidence confronts theory,the english rule for allocating legal costs evidence confronts theory,ERR
100688,1990,"journal of law, economics, and organization",legal restrictions on private contracts can enhance efficiency,legal restrictions on private contracts can enhance efficiency,"Legal Restrictions on Private Contracts Can Enhance Efficiency Author(s): Philippe Aghion and Benjamin Hermalin Source: Journal of Law, Economics, & Organization, Vol. 6, No. 2 (Autumn, 1990), pp. 381- Published by: Oxford University Press Stable URL: http://www.jstor.org/stable/764783 Accessed: 22-06-2016 01:00 UTC Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at http://about.jstor.org/terms JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Oxford University Press is collaborating with JSTOR to digitize, preserve and extend access to Journal of Law, Economics, & Organization This content downloaded from 128.32.162.35 on Wed, 22 Jun 2016 01:00:07 UTC All use subject to http://about.jstor.org/terms Collapse"
100689,1990,"journal of law, economics, and organization",enforcement costs and the optimal progressivity of income taxes,enforcement costs and the optimal progressivity of income taxes,"Mirrlees, Sheshinski, Atkinson, Feldstein, Stern, Hellwig, and others have analyzed the balance between redistribution and production of income in setting marginal tax rates. These studies indicated that the trade-off would resolve to optimal marginal tax rates substantially below 100 percent. Recently, attention has turned to the effect of marginal tax rates on evasion (Townsend; Sandmo; Graetz and Wilde; Mookherjee and Png, 1989a; Sanchez and Sobel). In this line of work, a revenue collector can enforce compliance with tax law by expending resources to audit taxpayers' reports. The more progressive the income tax, however, the more tempted will a taxpayer be to evade, and hence the more auditing will be required to Collapse"
100690,1990,"journal of law, economics, and organization",costly litigation and legal error under negligence,costly litigation and legal error under negligence,"In this article, private enforcement under negligence when there is legal error and litigation is costly is examined. Ordover (1978) demonstrated that in a negligence regime in which there is no legal error and litigation is costly, equilibrium requires the presence of actors who refuse to obey the due-care standard. Accordingly, in such a negligence regime, an undercompliance equilibrium must result. Since the existence of litigation costs implies that the socially optimal level of care is greater than that required by the traditional Hand formula, which defines negligence as a failure to take care where the cost of taking care is less than the expected loss if the accident occurs, 1 it is a short step from Ordover's undercompliance result to the conclusion that injurers,2 under negligence, exercise less than the socially optimal level of precaution (see Hylton). It is demonstrated that, because of legal error, an undercompliance equilibrium need not result under negligence. In a negligence regime in which courts err and litigation is costly, perfect and overcompliance equilibria are Collapse"
100691,1990,"journal of law, economics, and organization",switching costs and bidding parity in government procurement of computer systems,switching costs and bidding parity in government procurement of computer systems,"The issue in dispute . . . began with HGOC's [House Government Operations Committee] resistance to allowing consideration of full costs for converting computer programs fi'om the form used by incumbent computers to the form needed for equipment of prospective new vendors. HGOC correctly believes that considering full conversion costs tends to restrict competition to vendors of equipment compatible with incumbent machines. P. R. Werling, Ph.D. dissertation Collapse"
100721,1990,journal of mathematical economics,on the upper and lower semicontinuity of the aumann integral,on the upper and lower semicontinuity of the aumann integral,"Abstract Let ( T ,τ,μ) be a finite measure space, X be a Banach space, P be a metric space and let L 1 (μ, X ) denote the space of equivalence classes of X -valued Bochner integrable functions on ( T ,τ,μ). We show that if φ: T × P →2 X is a set-valued function such that for each fixed p ϵ P , φ(·, p ) has a measurable graph and for each fixed t ϵ T , φ( t ,·) is either upper or lower semicontinuous then the Aumann integral of φ, i.e.,∫ T φ ( t , p )d μ ( t )= {∫ T x ( t )d μ ( t ): xϵS φ ( p )}, where S φ ( p )= { yϵL 1 ( μ , X ): y ( t ) ϵφ ( t , p ) μ −a.e.}, is either upper or lower semicontinuous in the variable p as well. Our results generalize those of Aumann (1965, 1976) who has considered the above problem for X = R n , and they have useful applications in general equilibrium and game theory. Collapse"
100722,1990,journal of mathematical economics,on non-atomic weighted majority games,on non atomic weighted majority games,"Abstract We present some characterizations for the class of non-atomic weighted majority games which are defined on a measurable space ( I , C ). The characterizations are done within the class of all monotonic simple games which are upper semicontinuous on C and continuous at I with respect to the N A -topology on C . We also use the results on simple games to obtain a characterization for the games of the form ƒ ∘ μ where μ is a non-atomic probability measure and ƒ is a nondecreasing upper semicontinuous function on [0,1]. Collapse"
100747,1990,"journal of money, credit, and banking",the international debt crisis and bank loan-loss-reserve decisions:  the signaling content of partially anticipated events,the international debt crisis and bank loan loss reserve decisions   the signaling content of partially anticipated events,"THE HUGE INCREASES IN LOAN-LOSS PROVISIONS by major bank holding companies (BHCs) in 1987 were a signal of impending asset writedowns of loans to less-developed countries (LDCs). 1 Since loan-loss-reserve decisions, like asset write-downs, are simply bookkeeping adjustments, significant market reactions to such announcements should not be expected. Strong and Meyer (1987), however, found a positive and statistically significant cumulative average residual after the announcement of asset write-downs.2 Thakor (1987) argues that write-downs have a price effect not because they mean much by themselves but because they signal events to come. Specifically, he sees write-downs as a signal of ""economic value-enhancing corporate restructuring to come in the future"" (p. 662). We use event-study methods to examine security returns for the twenty-five largest U.S. bank holding companies surrounding Citicorp's $3 billion loan-loss- Collapse"
100759,1990,journal of policy analysis and management,the policy research markets,the policy research markets,"Keynote Addresses. Toward the Most Efficient Operation of the Korean Stock Market (B.-W. Koh). Volatility, Episodic Volatility, and Coordinated Circuit-Breakers: The Sequel (M.H. Miller). Saving in Developing Countries - Growth, Income, and Other Factors. (F. Modigliani). Developing Asian Capital Markets (K. Tarumizu). PACAP Distinguished Speakers Series. Malaysian Financial Markets (L. See-Yan). Tokyo Equity Market: Its Development and Policies (M. Sato). The Philippine Financial System: Path Towards Liberalization and Internationalization (G.C. Singson). Competitive Research Award Papers. Convertible Notes Issues by Japanese Firms in Switzerland: Pricing and its Effects (W. Wasserfallen, R.A. Hunkeler). The Volatility of Japanese Interest Rates: A Comparison of Alternative Term Structure Models (K.C. Chan, G.A. Karolyi, F.A. Longstaff, A.B. Sanders). Issues in Asian Corporation Finance. Determinants of Corporate Capital Structure: Australian Evidence (C. Chiarella, T.M. Pham, A.B. Sim, M.M.L. Tan). Financial Management Practices Among South Korean Firms (J.S. Ang, M. Jung). Optimal Corporate Investment in Imperfect Capital Markets: Evidence from Korea (M. Dailami). Empirical Studies on Asian Capital Markets. International Transmission of Stock Market Movements and Korea and Taiwan Fund Prices (Y. Jeng, C.-W. Kim, W.M.H. Wan-Sulaiman). Non-Performance Risk in Markets for Asian Commodities: Evidence and Implications (W. Bailey, E. Ng). The Impact of Dividend and Bonus Issue Announcements on the Hong Kong Exchange: An Empirical Investigation (E.Y. Guo, A.J. Keown). Korean Stock Market Studies. Is the APT True in Korea? (K. Cheong). Market Model Nonstationarity in the Korean Stock Market (T. Bos, T.A. Fetherston). Size, Price-Earnings Ratio, and Seasonal Anomalies in the Korean Stock Market (Y.G. Kim, K.H. Chung, C.S. Pyun). The Price Mechanism of IPO Market in Korea: with Emphasis on the Recent Liberalization Reform (U. Lim). Stock Market Volatility. On the Determinants of Stock Market Volatility: An Empirical Analysis of the Taiwan Stock Market (T. Ma). The Crash of 1987: An Empirical Examination of Liquidity, Volatility, and Volume Across International Stock Markets (G.N. Naidu, M.S. Rozeff). ADRs. Seasonal Patterns in ADR Returns: Some Implications (J. Park). The Bid-Ask Spreads of American Depositary Receipts (J.S. Howe, J.-C. Lin). Derivative Securities. Empirical Tests on the Pricing of the Nikkei Stock Index Options (M. Toshino). Selection of Underlying Index for Stock Index Futures in Korea (C.-P. Kook, Y.-J. Kwon, W.H. Lee, H.S. Choe). Speculative, Hedging, and Arbitrage Efficiency of the Nikkei Index Futures (K.-G. Lim). Recent Trends in Global Financial Markets. Bad Projects, Good Projects and IPO's (S.A. Ravid, M. Spiegel). The Global Market for Underwriting Services (R. Nachtmann, F. Phillips-Patrick). Some Structural Changes and Performances of Finance and Securities Companies in Thailand during 1981-1990 (P. Trairatvorakul, P. Collapse"
100763,1990,journal of policy analysis and management,"innovation in public sector human services programs:  the implications of innovation by ""groping along.""",innovation in public sector human services programs the implications of innovation by groping along,"This paper examines innovation in seventeen human services programs cited by the 1986 Ford Foundation Awards Program for Innovations in State and Local Government. The sample is particularly useful for distinguishing between two models for successful innovation: a policy planning model and Behn's model of ""groping along."" The cases suggest that the ""groping along"" model best fits the way that innovation came about in these programs. Innovative ideas typically developed through practice; programs began operating very quickly; and programs, once operating, were repeatedly modified in response to operational experience. Translating the groping-along model into specific prescriptions for managers requires us to reconsider the role of analysis. The case examples suggest that analysis may be most valuable in helping managers learn from experience. Collapse"
100776,1990,journal of policy analysis and management,introduction to symposium on managing local development,introduction to symposium on managing local development,"INTRODUCIION Economic development has become one of the major public policies emphasized by state, county, and city governments for the last two decades (e.g., Eisinger, 1988; Levy, 1990; Luke et al., 1988; McGowan and Ottensmeyer, 1993). The importance of economic development for these governments has to do with changes in both the domestic and international economic and political environments. Domestically, for example, the turbulence of the national economy in the 1970s, the recessions of the early 1980s, the rise of federal budget deficits, and the cutback of federal aid to the states have forced state and local governments to broaden their efforts in economic development. Internationally, the globalization of the world economy, especially the increasing role of foreign trade and investment in the United States, has forced many state (and some large city) governments to become actively involved in international economic activities (e.g., Kline, 1983; Liou, 1993). To promote economic development, public managers and policymakers have developed and implemented various policies and programs to attract and retain businesses (i.e., the third wave of state and local economic development policy). The policies and programs emphasized range from reducing the cost of doing business (e.g., production costs, direct loans, industrial parks, job training, tax abatements) to increasing services for business operation (e.g., RD Eisinger, 1988; Luke et al., 1988). The effectiveness of these policies and programs has been empirically tested and generally supported in the literature of state and local economic development (Bartik, 1991). State and local government officials have recently emphasized the use of strategic planning as one of the key managerial techniques not only to coordinate economic development activities but also to accommodate economic changes (e.g., Allen and Plosila, 1988; Mier, Moe, and Sherr, 1986; Reed and Blair, 1993; Reed, Reed, and Luke, 1987). The linkage between strategic planning and economic development provides an unique opportunity for researchers of public management and policy to examine concepts and issues related to strategic planning and economic development and to make a contribution to the literature of both disciplines. For strategic planning researchers, the linkage between the two will help them understand the application of strategic planning in the public management and policy arenas. For economic development researchers, the linkage will give them more knowledge about critical issues related to economic development, planning, and management functions. The purpose of this symposium is to examine the relationship between strategic planning and economic development. This introduction article will first offer explanations about some critical concepts that are related to strategic planning and economic development. Next, this article will summarize major issues and findings of the symposium articles. CRITICAL CONCEPTS Economic development has been defined or referred to as different goals or objectives for various governments under different conditions. At the national level, for example, the objective of economic development policy for developed countries is to increase their economic growth, usually measured by such output or income as gross national product (GNP) or gross domestic product (GDP). This objective is different from that of the developing countries. For those countries, the objective of their economic development policy refers not only to increase in the growth rate of GNP or GDP but also to changes in their economic, social, and political structures (Goode, 1984). Locally-based economic development has been explained as ""a process by which local government and/or community-based groups manage their existing resources and enter into new partnership arrangements with the private sector, or with each other, to create new jobs and stimulate economic activities in a well-defined economic zone"" (Blakely, 1989:58). … Collapse"
100779,1990,journal of policy analysis and management,improving economic development strategies: symposium comment,improving economic development strategies symposium comment,"There is an old bromide, often associated with former Speaker of the House Tip O'Neill, that says “all politics is local.” The papers under review here implicitly raise the question of whether all (or at least a good portion) of economic development is local, and what sort of strategies state and local governments should pursue in fostering healthy economic development. Collapse"
100786,1990,journal of policy modeling,presidential elections and the federal reserve's interest rate reaction function,presidential elections and the federal reserves interest rate reaction function,Abstract We examine the political business cycle from the perspective of an interest rate reaction function and then contrast the Federal Reserve's willingness to endure unemployment for the sake of lower inflation with that of the electorate. Collapse
100787,1990,journal of policy modeling,testing for political business cycles,testing for political business cycles,"Abstract This paper presents a new test of political business cycle and partisan theories of politico-economic interaction. It builds on the hypothesis that model dynamics and not just the intercept of a time-series model vary over electoral periods and party regimes. Using long-run data for the United States, we find evidence that Presidential elections and the political party of incumbent Presidents influence the behavior of economic targets and instruments. However, these outcomes do not arise independently of recent economic performance. Therefore, our results support the satisficing model of Frey and Schneider and reject the traditional version of the theory. Collapse"
100788,1990,journal of policy modeling,alternative financing of social insurance systems,alternative financing of social insurance systems,"Abstract Social insurance systems in many countries are partly financed with contributions paid by employers in proportion with salaries of employees. In this article the macroeconomic effects of alternative methods of financing the system are estimated for the Netherlands with a multisectoral macroeconomic model. Three alternatives are investigated: a new levy on net value-added, an increase in the VAT rate and an increase in personal income tax rates. Collapse"
100790,1990,journal of policy modeling,the world trade model:  revised estimates,the world trade model revised estimates,"This article develops a rational-choice model of the effects of social change on religious organizations, which we use to analyze the Mormon Church's response to change in women's roles. Content analysis and time-series techniques are used to estimate the Church's response to social change and the effect that this response has had upon member commitment and rates of conversionfrom 1950 to 1986. Wefind that despite initial resistance in the late 1960s and 1970s the Church has been moving to accommodate change in women's roles. Accommodation appears to increase participation among younger and less experienced members but decrease participation among older and more experienced members, suggesting that a successful church must strike a balance between accommodation and resistance to social change. How do churches respond to changes that threaten the social underp gs of their beliefs? Such changes are pervasive in the modem world: Catholicism confronts new contraceptive technologies and acceptability of divorce; Evangelical denominations must come to terms with evolutionary science; Mormonism must deal with changing family structures. As these examples suggest, the problems created by change are not easily resolved. Intransigence puts a church increasingly at odds with the prevailing culture and risks alienating both current members and potential converts; accommodation undermines its claim to transcendent truth and divine authority. In this article, we develop a rational-choice model of church behavior that identifies the trade-offs imposed by change and demonstrates its tendency to polarize church membership. We then test this model by measuring the Mormon Church's response to change in the roles of American women. Using time-series techniques, we estimate the Mormon Church's response to social change and, in turn, the effect this response has had upon member commitment and rates of growth from 1950 to 1986. Our results suggest that a successful church must strike a balance between accommodation and resistance to social change, We speculate that the ability to Portions of this research were supported by theArthur ViningDavis Junior Faculty Fellowship awarded by Santa Clara University. We are grateful to John Glidewell, Martin Marty, Harry Roberts,ArmandMauss,RogerFinke, and two anonymous refereesfor valuable comments and suggestions. An early version of this paper was presented at the 1987Annual Meetings of the American Sociological Association. Direct correspondence to Laurence R. lannaccone, Department of Economics, Santa Clara University, Santa Clara, CA 95093 X) The University of North Carolina Press Social Forces, June 1990, 68(4):1231-1250 This content downloaded from 157.55.39.177 on Wed, 16 Nov 2016 04:30:58 UTC All use subject to http://about.jstor.org/terms 1232 \ Social Forces 68:4, June 1990 strike such a balance has contributed not only to the growth of Mormonism but also to that of conservative Protestant denominations. The Inevitable Dilemma When social or technological change reduces the perceived value of an organization's outputs, continued success requires a response. But whereas a business firm's optimal response usually entails embracing the new environment, a church's ""optimal"" response is less clear-cut. Churches confront an ""inevitable dilemma"" (Yinger 1946). They must respond to the wishes of their members, lest they lose support (Brannon 1971), but they must also make distinctive demands, lest they lose credibility. Churches exist in part because they offer an alternative to societal norms (Hunter 1983; Lee 1960; Neal 1971). Hence, a church that loses its distinctiveness also loses its appeal. A Soviet Molokan sect studied by Lane (1975) illustrates this principle. Committed to a communitarian lifestyle, this sect approved of communist government before the Russian Revolution and was in tum tolerated after the Communists took power. Yet, as the sect gained government acceptance, it lost both members and commitment. In contrast, a radical Molokan splinter group that opposed the Communist govenmment grew. The development of American Protestant denominations likewise illustrates the need for continuing distinctiveness. For more than a generation American Protestantism has seen its culturally accommodated liberal wing decline as its sectarian, conservative wing has grown (Finke & Stark 1989). Noting the near-perfect correlation between cultural accommodation and membership decline, Roof and McKinney (1987:21) recently observed that ""almost all of the churches that retained distance from the culture by encouraging distinctive life-styles and beliefs grew; those most immersed in the culture and only vaguely identifiable in terms of their own features suffered declines"" (c.f., Hoge & Roozen 1979; Kelley 1972).1 The preceding examples show that churches often pay a price for accommodation. But it is also true that churches can be hurt by intransigence. When the Pope officially and decisively reiterated the Catholic Church's traditional ban on contraception in the Humanae Vitae encyclical, he shocked and disappointed the great majority of American Catholics and set into motion a backlash that is said to have dramatically reduced American Catholic church attendance, contributions, and confidence in papal authority (Greeley 1985; Greeley, McCready & McCourt 1976; Greeley & McManus 1987; Hout & Greeley 1987). The simultaneous losses of Catholicism on the one hand and liberal Protestantism on the other highlight the dilemma posed by social change. A Cost/ Benefit Model of Church Response to Social Change The problems identffied above can be analyzed with a cost-benefit model of religious organizations.2 In this model, the rewards a member obtains for adhering to her church's demands (and the sanctions she suffers for rejecting them) are summarized by a mathematical relation that economists call a ""production function."" One such function, R, is depicted by the solid line in Figure 1. A person's conduct is represented as a point on the horizontal axis. The height of the hill lying atop this axis indexes the rewards associated with various manners of conduct. Conduct that the church advocates as ideal and rewards most strongly may be called the church's This content downloaded from 157.55.39.177 on Wed, 16 Nov 2016 04:30:58 UTC All use subject to http://about.jstor.org/terms Mormon Church and Women's Roles / 1233 FIGURE 1: Society and Church, Similar Normsa Collapse"
100791,1990,journal of policy modeling,the structural impact of the small refiner bias:  an econometric analysis,the structural impact of the small refiner bias an econometric analysis,"Abstract The Small Refiner Bias (SRB) provided subsidies to smaller refiners under the federal oil entitlements program from 1973 to 1981. The program led to a proliferation of smaller refineries between 1973 and 1981. When the program ended in 1981, 110 of 282 refineries closed, but capacity fell by only 11 percent. A logit model of the industry shake-out is presented. Refinery capacity, growth between 1973 and 1981, and whether the refinery started production after 1973 were significantly correlated with the probability of closing. The author concludes that the SRB program provided strong incentives for inefficient production. Collapse"
100793,1990,journal of policy modeling,the long-run behavior of velocity:  the institutional approach revisited,the long run behavior of velocity   the institutional approach revisited,"In this paper we provide evidence using annual data for the period 1880 to 1986 that institutional variables are significant determinants of velocity in the United States, United Kingdom, Canada, Sweden and Norway. This evidence supplements our earlier findings (Bordo and Jonung, Cambridge University Press, 1987) for annual data ending in the early 1970's. We present eVidence that several proxies for institutional change in the financial sector are significant determinants of the long-run velocity function; that for the majority of countries the long-run velocity function incorporating institutional determinants has not undergone significant change over the last 10 to 15 years; and that out of sample forecasts over the last 10 to 15 years based on our institutional hypothesis are superior to those based on a benchmark long-run velocity function for a number of countries. these results suggests that failure to account for institutional change in the financial sector such as may be captured by our proxy variables may well be one factor behind the recently documented instability and decline in predictive power of short-run velocity models incorporating dynamic adjustment and higher frequency data. Collapse"
100795,1990,journal of policy modeling,developments in monetary aggregation theory,developments in monetary aggregation theory,We survey the recent advances in monetary aggregation theory as well as the earlier literature on the subject. We discuss the limitations in the current state of our knowledge about monetary aggregation and speculate about productive areas for future research. Collapse
100797,1990,journal of policy modeling,empirical predictions of the new monetary economics: perspectives on velocity,empirical predictions of the new monetary economics perspectives on velocity,"Recent work by Black (1970), Fama (1980), Hall (1983), Wallace (1983), and others has stimulated interest in what Hall (1982a) labels the ""new monetary economics"" (henceforth, NME).I Most generally, the NME emphasizes the distinction between the provision of monetary services and the provision of money. More specifically , money's two functions of medium of account and medium of exchange need not be united in a single asset called money . In contrast to current financial institutions , transactions could be conducted by using assets that are traditionally considered nonmonetary , such as equity shares , interest-bearing assets, or bookkeeping entries through a central accounting system. The unit of account could be a commodity basket or even be purely abstract; in either case it could exist separately from the media of exchange . For this reason, NME systems are often described as "" advanced barter. "" Furthermore, NME banks would not differ in kind from other financial intermediaries (Cowen and Kroszner , 1989b, forthcoming). Depositors would hold checkable accounts based upon return-bearing assets; some extension of today's "" Cash Management Account"" (Merrill Lynch) might become the standard form for transactions deposits . The authors associated with the NME, however , do not adopt any monolithic or even necessarily consistent position on many important issues. Some formulations are strictly cashless, with no physical media of exchange, whereas others contain currency that might be provided Collapse"
100799,1990,journal of policy modeling,"money demand, expectations, and the forward-looking model",money demand  expectations  and the forward looking model,"An error correction model is derived from a stochastic dynamic programming problem incorporating rational expectations. A parametric restriction is derived that allows a test for the theoretical proposition that the optimal strategy behind the error correction form entails the failure to asymptotically close the gap between the choice variable and the growing target. This is accomplished by nesting a partial adjustment model with forward-looking expectations within the error correction paradigm. The counterintuitive behavior embodied in the error correction model is not supported by the data in the context of a cross-country comparison of cash balances relationships. Copyright 1990 by John Wiley & Sons, Ltd. Collapse"
100802,1990,journal of policy modeling,buffer stock models of the demand for money and the conduct of monetary policy,buffer stock models of the demand for money and the conduct of monetary policy,"Popular wisdom informs us that one of the prime victims of the American financial deregulation of the last decade has been the demand for money function. While there is some evidence that the demise of standard monetary relations has been oversold (see Rasche 1987, 1988, and Darby, Mascaro, and Marlow, 1989), it is with some trepidation that we agreed to reexamine functional forms which we helped develop in 1980. Collapse"
100804,1990,journal of policy modeling,"buffer stocks, credit, and aggregation effects in the demand for broad money:  theory and an application to the u.k.  personal sector",buffer stocks credit and aggregation effects in the demand for broad money theory and an application to the uk personal sector,"This paper considers the implications of the buffer stock notion for modeling broad money. Assuming target-threshold account monitoring behavior, we examine the issue of aggregate money exogeneity and the role of credit, and other supply side variables. We show that aggregate money can appear exogenous and exhibit slow adjustment to supply shocks in spite of being endogenously and freely generated by the nonmonetary private sector. The mechanism is illustrated by a simulation. One consequence of our analysis is that the short-run money demand function cannot be viewed as a representative agent model, and is conventionally misspecified. We consider instead the system approach based on modeling the supply side, or money counterparts. We illustrate our method by an application to U.K. personal sector broad money holdings, using the Granger-Engle cointegration methodology to estimate the longrun demand schedule. The role of the demand schedule in adjustment to disequilibrium is tested in the context of the National Institute of Economic and Social Research U.K. quarterly forecasting model. Collapse"
100807,1990,journal of policy modeling,the demand for money and the monetary policy process in canada,the demand for money and the monetary policy process in canada,ERR
100809,1990,journal of policy modeling,modeling money demand in large industrial countries:  buffer stock and error correction approaches,modeling money demand in large industrial countries buffer stock and error correction approaches,"The empirical performance of two recent approaches to modeling money demand is compared by estimating functions for narrow and broad aggregates in five large industrial countries. Error correction modeling outperforms the Carr-Darby buffer stock model (which in turn outperforms the conventional partial adjustment model) within the 1974–1985 sample period. Post-sample, however, the comparisons are mixed. The specification of money demand appears to vary substantially from case to case. Collapse"
100828,1990,journal of political economy,shark repellents and managerial myopia:  an empirical test,shark repellents and managerial myopia an empirical test,"Since the proxy fights of the 1950s, commentators have debated the welfare implications of corporate takeovers. Although observers such as Manne (1965), Jensen and Meckling (1976), Fama (1980), and Jensen and Ruback (1983) argue that the market for corporate control promotes efficiency and enhances wealth, some critics, such as managers of firms subject to hostile takeover attempts, contend that takeovers destroy firm value. The critics frequently assert that takeover pressure forces managers to sacrifice profitable, but slowyielding, long-term investments in favor of less productive short-term investments that offer immediate returns. While the evidence supporting takeover-induced shortsightedness is largely anecdotal, a recent paper by Stein (1988) develops a formal Collapse"
100829,1990,journal of political economy,the problem of development:  introduction,the problem of development introduction,""" This article uses an ecological approach to analyze factors in the effectiveness of work teams--small groups of interdependent individuals who share responsibility for outcomes for their organizations. Applications include advice and involvement, as in quality control circles and committees; production and service, as in assembly groups and sales teams; projects and development, as in engineering and research groups; and action and negotiation, as in sports teams and combat units. An analytic framework depicts team effectiveness as interdependent with organizational context, boundaries, and team development. Key context factors include (a) organizational culture, (b) technology and task design, (c) mission clarity, (d) autonomy, (e) rewards, ( f ) performance feedback, (g) training/consultation, and (h) physical environment. Team boundaries may mediate the impact of organizational context on team development. Current research leaves unanswered questions but suggests that effectiveness depends on organizational context and boundaries as much as on internal processes. Issues are raised for research and practice. The terms work team and work group appear often in today's discussions of organizations. Some experts claim that to be effective modern firms need to use small teams for an increasing variety of jobs. For instance, in an article subtitled ""The Team as Hero,"" Reich (1987) wrote, If we are to compete in today's world, we must begin to celebrate collective entrepreneurship, endeavors in which the whole of the effort is greater than the sum of individual contributions. We need to honor our teams more, our aggressive leaders and maverick geniuses less. (p. 78) Work teams occupy a pivotal role in what has been described as a management transformation (Walton, 1985), paradigm shift (Ketehum, 1984), and corporate renaissance (Kanter, 1983). In this management revolution, Peters (1988) advised that organizations use ""multi-function teams for all development activities"" (p. 210) and ""organize every function into tento thirty-person, largely self-managing teams"" (p. 296). Tornatzky (1986) pointed to new technologies that allow small work groups to take responsibility for whole products. Hackman (1986) predicted that, ""organizations in the future will rely heavily on member self-management"" (p. 90). Building blocks of such organizations are self-regulating work teams. But University of Tennessee University of Wisconsin--Eau Claire University o f Tennessee far from being revolutionary, work groups are traditional; ""the problem before us is not to invent more tools, but to use the ones we have"" (Kanter, 1983, p. 64). In this article, we explore applications of work teams and propose an analytic framework for team effectiveness. Work teams are defined as interdependent collections of individuals who share responsibility for specific outcomes for their organizations. In what follows, we first identify applications of work teams and then offer a framework for analyzing team effectiveness. Its facets make up topics of subsequent sections: organizational context, boundaries, and team development. We close with issues for research and practice. A p p l i c a t i o n s o f W o r k T e a m s Two watershed events called attention to the benefits of applying work teams beyond sports and mih'tary settings: the Hawthorne studies (Homans, 1950) and European experiments with autonomous work groups (Kelly, 1982). Enthusiasm has alternated with disenchantment (Bramel & Friend, 1987), but the 1980s have brought a resurgence of interest. Unfortunately, we have little evidence on how widely work teams are used or whether their use is expanding. Pasmore, Francis, Haldeman, and Shani (1982) reported that introduction of autonomous work groups was the most common intervention in 134 experiments in manufacturing firms. Production teams number among four broad categories of work team applications: (a) advice and involvement, (b) production and service, (c) projects and development, and (d) action and negotiation. Advice and Involvement Decision-making committees traditional in management now are expanding to first-line employees. Quality control (QC) circles and employee involvement groups have been common in the 1980s, often as vehicles for employee participation ( Cole, 1982 ). Perhaps several hundred thousand U.S. employees belong to QC circles (Ledford, Lawler, & Mohrman, 1988), usually first-line manufacturing employees who meet to identify opportunities for improvement. Some make and carry out proposals, but most have restricted scopes of activity and little working time, perhaps a few hours each month (Thompson, 1982). Employee involvement groups operate similarly, exploring ways to improve customer service (Peterfreund, 1982). 120 February 1990 • American Psychologist Copyright 1990 by the American Psyc2aological A~mciafion, Inc. 0003-066X/90/$00.75 Vol. 45, No. 2, 120-133 QC circles and employee involvement groups at times may have been implemented poorly (Shea, 1986), but they have been used extensively in some companies Collapse"
100857,1990,journal of public economics,"housing, taxes, and capital allocation",housing taxes and capital allocation,"In this paper we explore the efficiency gains from the Tax Reform Act of 1986 and prospective tax reforms, separating out the intersectoral and intertemporal efficiency consequences. To assess these effects, we employ a general equilibrium model that considers the effects of taxes on the allocation of capital across industries, assets, sectors, and time. We find that the 1986 tax reform yielded only a small improvement in the intersectoral allocation of capital because the beneficial effects from its more uniform treatment of capital within the business sector are largely offset by adverse effects stemming from increased tax disparities between the business and housing sectors. The intertemporal efficiency effects of the reform, in contrast, are significant and negative. Hence the overall efficiency impact of the reform is negative as well. Our results indicate that the economic margins offering the greatest scope for efficiency gains are different from those that received the most attention under the 1986 tax reform. While much of the 1986 reform concentrated on reducing tax disparities within the business sector, much larger efficiency gains would result from reducing tax disparities between the business and housing sectors and from general reductions in effective marginal tax rates on capital. Collapse"
100872,1990,journal of risk and insurance,classifying financial distress in the life insurance industry,classifying financial distress in the life insurance industry,"First Executive Corporation’s bankruptcy in 1991 made the insurance policyholders, investors and regulators to concern about the solvency of life insurer in US market. As First Executive Corporation had more than 15 billion dollars in assets, its failure provoked increasing debate over the prediction system of financial distress in insurance industry, the Insurance Regulatory Information System (IRIS). From 1970s, National Association of Insurance Commissioners (NAIC) has developed IRIS which serves as a baseline solvency screening system. IRIS helps regulators prioritize insurers for detailed financial analysis and allocate their resources according to need by producing 12 financial ratios for life insurers and 11 for property-casualty insurers (Atchinson 1996). When an insurer has four or more of those ratios beyond the specified range, the state regulators and NAIC will classify it as priority firm and pay immediate attention. Two years after failure of First Executive, NAIC adopted the Financial Analysis and Solvency Tracking System (FAST) as an expansion of IRIS. FAST assigns different points for different ranges of 29 financial ratio results. The higher the total score is, the higher attention the state regulators and NAIC pay for. Essentially, IRIS is the implementation of univariate approach of discriminant analysis first proposed by Beaver (1966), and FAST is the implementation of multivariate analysis first proposed by Altman (1968). From 1980s, the logistic analysis has replaced multivariate analysis as a most used statistical method for insolvency prediction purposes, since multivariate analysis suffers from its normality assumption. Except the prediction systems used by NAIC, financial academy and practitioner proposed a number of other methods to predict business insolvency, ranging from nonfinancial industries to financial industries. In the insurance industry, most previous studies have been made on the property & casualty insurers, but very limited of them have been applied to life & health sector. More important, most approaches applied to insurance companies are statistical methods such as discriminant or logit analysis. Both of them suffer from their statistical assumptions about the observed information which is a set of financial ratios in most cases. Recently, the integration of artificial intelligence with the financial economics proposed neural network as a promising alternative for classifying the insolvency financial institutions (Brockett 1994; Tam 1990; Salchengerger 1992; Wilson 1994). In this paper, we used a neural network method in life insurer insolvency prediction. Following (Brockett 1994), we constructed a feed-forward backpropagation three-layer artificial neural network, but we applied this method to life insurers listed in NAIC InfoPro data tape. We also compared the predicating ability of stepwise multivariate linear discriminant analysis, the stepwise multivariate logit analysis, and neural network. Using matched data of solvent and insolvent life insurers in the time period from 1993 to 2001, we showed that a neural network model with feed forward, back propagation algorithm can provide satisfactory insolvency predictive ability compared to the traditional methods. Although the neural network model is promising in the prediction of business insolvency, a lot of jobs are still needed to do in the future. At the end of the paper, we further investigate the convergence condition and the proper repeating time. Collapse"
100928,1990,journal of urban economics,a theory and empirical test of land option pricing,a theory and empirical test of land option pricing,"Abstract Land option contracts are agreements whereby a landowner agrees to sell property at a stipulated exercise price to a potential buyer (developer) within a specified length of time. The option contract gives the developer time to coordinate the expertise of many professionals and obtain zoning changes and financing for the property, if necessary. In most cases the developer must either purchase the option to buy the proposed site or purchase the land outright before knowing whether the development is feasible. The landowner's objective is to choose an exercise price (and hence an option premium) which will ensure that the developer will exercise the land option. In a bid-price model where developers are risk-neutral and landowners are risk-averse, the prediction is that land options are written with a zero or close to zero time premium. This paper is a test of this hypothesis and the results confirm the theoretical model. Collapse"
100930,1990,journal of urban economics,commuting between local authorities in england and wales: econometric evidence from the 1981 census,commuting between local authorities in england and wales econometric evidence from the 1981 census,ERR
100931,1990,journal of urban economics,market segmentation and valuing amenities with hedonic models: the case of hazardous waste sites,market segmentation and valuing amenities with hedonic models the case of hazardous waste sites,ERR
100935,1990,journal of urban economics,taste heterogeneity and urban spatial structure:  the logit model and monocentric theory reconciled,taste heterogeneity and urban spatial structure the logit model and monocentric theory reconciled,"Abstract A model of urban spatial structure for a linear and closed monocentric city is formulated by allowing the presence of randomly distributed idiosyncratic tastes for location in an otherwise uniform household population. Land use equilibrium conditions are shown to have a stochastic representation which depends on the distribution of the idiosyncratic tastes over the population of households. A special case in which the multinomial logit model gives the representative household's locational choice probability is derived and examined for a logarithmic utility function. It is shown that the stochastic equilibrium can also be obtained as the solution of a welfare optimum problem, either by maximizing total social welfare (primal) or by minimizing total opportunity cost (dual). Either formulation gives the equilibrium allocations and associated decentralizing land prices. Some comparative statics analysis is undertaken to show that while most results are like those of the Alonso-Mills-Muth model, increasing taste heterogeneity decentralizes the linear urban form, “flattening” the rent and population density gradients. The Alonso-Mills-Muth solution is obtained as an asymptotically limiting case which provides an upper bound on the “steepness” of the rent and population density gradients and a lower bound on the length of the linear city and the welfare level of the representative consumer. Collapse"
100938,1990,journal of urban economics,"an analysis of inefficiency due to inadequate mortgage financing:  the case of seoul, korea",an analysis of inefficiency due to inadequate mortgage financing the case of seoul korea,ERR
100939,1990,journal of urban economics,knowledge and production in the cbd,knowledge and production in the cbd,No Result.
101041,1990,monthly labor review,recent gains in women's earnings:  better pay or longer hours?,recent gains in womens earnings better pay or longer hours,"828 The Journal of Human Resources women in hiring and promotion practices, then to (2) differences in noncognitive skills, namely assertive negotiating, and finally, and least importantly, to (3) the possession of education and skills needed for high paying jobs (Hill and Silva 2005, p. 3). After decades of publications investigating male-female earnings differences, economists have formed a consensus that human capital variables—like education, work experience and skills—explain more and discrimination explains less of the gender income gap than the public thinks. Despite the public’s common sense understanding that career success is influenced by noncognitive skills, such as confidence, motivation, and assertiveness, and by work/life preferences, economists cannot offer a consensus judgment regarding the wage gap effect of either. The human capital model of Becker (1964) predicted earnings differences to arise from differences in the broad array of individual abilities and in educational investments. Due to the ease of using cognitive test scores and the difficulty of empirically operationalizing personality traits and noncognitive characteristics, to date empirical analyses have used cognitive test scores to proxy for “individual ability.” Social scientists, able to typically account for only half of the gender pay gap with human capital models based on nationally representative data sets, have long hypothesized that gender heterogeneity may characterize noncognitive skills and a variety of work/life preferences, both of which cause wage differences. Now a burgeoning literature, especially by those conducting lab and field experiments, reports gender heterogeneity of preferences and noncognitive skills (Croson and Gneezy 2009; Booth 2009). Economists and others, though, are just beginning to test the labor market outcomes of such gendered work-life choices and personality traits. Using an especially rich national data set, the twin goals of this paper are (1) to identify noncognitive and preference sources of otherwise unobserved gender heterogeneity and then (2) to estimate whether such heterogeneity accounts for more of the male-female earnings gaps than can be explained by an extensive set of human capital variables. We view our analysis, then, as part of a broad agenda to enrich 2. Although the unexplained component of the gender wage gap is often attributed to discrimination, it also may result from a misspecification of the relationships or from unobserved gender heterogeneity (Polachek and Kim 1994; Altonji and Blank 1999). Regarding discriminatory behavior, see, for example, Neumark et al. (1996) and Goldin and Rouse (2000). Although we do not test for discrimination, Montgomery and Powell (2003), using the first three waves of our data set, found that obtaining an MBA sharply diminishes the gender wage gap, comparing wages of MBAs and non-MBAs. 3. Regarding the challenges of systematically analyzing the labor market outcomes of noncognitive skills, see Borghans et al. (2008) and ter Weel (2008). 4. Psychologists prefer the term character or personality traits (see Thiel and Thomsen 2009). 5. For example, Blau and Kahn (1997), using the Panel Study of Income Dynamics (PSID) for full-time workers with incomes and labor market experience, found an unadjusted male-female wage ratio of 72.4 percent in 1988. Controlling for human capital variables, occupation, industry, and unionism explains half of the gap. Polachek and Kim (1994), also using the PSID, estimate that half of the male-female earnings differences results from unobserved gender heterogeneity. 6. See, for example, Bowles et al.’s (2001) review of the early explanations of wage differences due to personality and the 2008 Journal of Human Resources symposium issue entitled “The Noncognitive Determinants of Labor Market Outcomes and Behavioral Outcomes.” In response to criticisms of narrowly measuring ability, as of July 2009. the GRE includes a formal measure that attempts to capture noncognitive skills (the “Personal Potential Index”). Grove, Hussey, and Jetter 829 the human capital model as envisioned by Becker (1964) by more fully understanding the variation of individual abilities, especially of noncognitive skills and of work/ life preferences, and how such heterogeneity influences labor market outcomes. Economists have taken three approaches to better understand the gender pay gap. First, the growing lab and field experiment findings about gender differences in, for example, confidence, career-orientation, and assertiveness, are consistent with gender earnings gaps, with the under-representation of women in the upper tier of leadership in professions and corporations, and with the anecdotal evidence of professional women “opting out” of careers; to date, though, little empirical analysis has investigated those potential relationships (Thiel and Thomsen 2009). The notable exceptions focus on the personality traits of the Big Five (see Braakmann 2009; Mueller and Plug 2006) and measures of locus of control and self-esteem (Fortin 2008; Urzua 2008). We test the role of various confidence measures and 15 noncognitive skills (deemed especially important for business professionals) in explaining the MBA male-female pay gap. Secondly, scholars have focused upon gender differences in labor market tastes such as the priority of family, career, wealth, and job characteristics. According to Long (1995) and Fortin (2008), the priority of work and money contributes to the pay gap. Chevalier (2007) finds that women with a preference for childbearing earn less even before they have children due to their choice of college major and because they engage less intensively in job searching (also see Goldin and Polachek 1987). Our data contain a variety of individuals’ priorities regarding family and career, as well as the reported importance of nonpecuniary job attributes, recorded about eight years prior to the earnings data we assess. Finally, because nation-wide data sets, like the Panel Study of Income Dynamics (PSID), lack information regarding, for example, college quality, college major, and detailed work histories, researchers have sought smaller, specialized, and homogeneous data sets with greater educational and labor market detail; examples from individual institutions of higher education include studies based on surveys of undergraduates from Harvard College (Goldin and Katz 2008), lawyers from the University of Michigan (Wood, Corcoran, and Courant 1993), and MBAs from the University of Chicago (Bertrand et al. 2009) and the London School of Business (Graddy and Pistaferri 2000). Children, according to Bertrand et al. (2009), mainly contribute to female MBAs’ reduced earnings via fewer hours worked and increased career interruptions. Furthermore, from the Harvard and Beyond data set, female MBAs have greater difficulty balancing careers and children than do medical doctors, lawyers, or Ph.D.s (Goldin and Katz 2008; Herr and Wolfram 2009). In addition to 7. A recent survey by Catalyst, for example, found that “26 percent of women at the cusp of the most senior level of management don’t want the promotion” (Belkin 2003). For anecdotal evidence of high powered professional women “opting out” of careers, see Belkin’s (2003) widely read article in The New York Times Magazine. In contrast, Stone (2007) argues that mostly professional women want to but cannot manage to raise children and function in demanding careers (see also Leonhardt 2010). However, Antecol (2010) find that professional women largely return to work within two years of childbirth. 8. A recent New York Times article entitled “A Labor Market Punishing to Mothers” (Leonhardt 2010), which cites Bertrand et al. (2009), makes a similar argument about professional women generally, noting that the three recent female Supreme Court nominees do not have children. 830 The Journal of Human Resources children, though, Bertrand et al. (2009) also attribute the gender wage gap to differences in MBA training and hours worked. Because these data sets come from individual elite institutions, it is not clear how their results generalize either to typical MBAs or to other average highly educated professionals. The existence of a unique and especially rich data set, the GMAT Registrant Survey, allows us to estimate the role of preferences and noncognitive skills in explaining the gender earnings gap. A stratified random sample of all registrants for the Graduate Management Admission Test (GMAT), the GMAT Registrant Survey, contains longitudinal data in four waves from 1990 to 1998. After registering to take the GMAT but prior to enrolling in an MBA program (Wave I), respondents provided information regarding career and family priorities, 15 noncognitive skills, expected future managerial responsibility, individuals’ job preferences regarding the importance of nonmonetary job characteristics, and information used to create five confidence measures. The data set also provides detailed information about both undergraduate and MBA educational experiences, work histories, earnings, family background, marriage, children, and more. Drawn from a national sample of aspiring MBAs, this data set includes the wide range of MBA program qualities and types available in the United States (Arcidiacono et al. 2007), rather than merely graduates of the most elite programs (for example, Bertrand et al. 2009; Graddy and Pistaferri 2000). Among our sample of MBAs, females employed full-time earn 15.5 percent less per year than do males, a smaller gap than is found in economy-wide data sets (for instance, Blau and Kahn 1997). When we add basic human capital variables (for example, family background, work experience, ability measures, undergraduate and MBA educational experiences), the unexplained gap falls to 9.5 percent, and then further to 6.5 percent with the addition of hours worked and current employment characteristic Collapse"
101042,1990,monthly labor review,raising the minimum wage: effects on family poverty,raising the minimum wage effects on family poverty,"Inequality in the US is steadily growing as a result of the increasing gap in wages due, among other factors, to the erosion in the real value of minimum wages. Although minimum wage policies intend to tackle this problem, an intense partisan debate among politicians has led to the current stalemate of minimum wages. Despite the emerging consensus in academia that the increase of minimum wages has a very limited effect on unemployment, previous research has yielded opposite findings. This is the first study that relies on time series state level data of a long period of time, from 1977 to 2012, to analyze the effect of minimum wages on employment. Using a fixed effects estimator with a sample of 1,576 observations, the findings of this study are in line with the emerging consensus, namely that there is no evidence that the minimum wage has an effect on employment. The results show that the employment elasticity of minimum wage is insignificant in the whole period. However, when analyzing the effect within different subperiods (pre-1990, 1990-1999, and post-1999), the results show that the effect is changing substantially through time. The elasticity was negative before 1990, insignificant in the 1990s, and positive in the 2000s. It seems that the increasing power of firms is turning an efficient lowincome labor market before 1990 into a monopsony low-income labor market in the 2000s. The effect is more significant for youth, particularly for women. It suggests that current minimum wage policies in the US can worsen the lingering high youth unemployment rates. Acknowledgments I would like to acknowledge the Department of Labor, the Census Bureau, and the Department of Commerce for providing the data used in this evaluation and professor Jane Palmer from American University for her helpful suggestions. THE EFFECT OF MINIMUM WAGE POLICIES 2 Inequality and poverty rates in the US are steadily growing (USCB, 2013) as a consequence of the increasing gap in wages and the decline in real value of earnings of lowincome workers. The stalemate of the real value of minimum wages is considered one of the main factors that is driving low-income workers into poverty (Addison & Blackburn, 1999; Daly & Valletta, 2006; Lee, 1999; Leigh, 2007). While minimum wage policies intend to tackle this problem, there is an intense partisan debate about the benefits and risks of imposing a minimum wage. Republicans and the industry oppose any increase in the minimum wage alleging that it will entail employers hiring fewer workers, thus potentially leading to higher inequalities (Crittenden & Nelson, 2014). President Obama has signed an executive order raising the minimum wage for federal contractors (Parsons, 2014) because “no one who works full time should ever have to raise a family in poverty” (Obama, 2014, p.arr. 50). Polls show broad support for raising the minimum wage (Crittenden & Nelson, 2014). Almost three-fourths (73%) of the public favors raising the federal minimum wage to overcome increasing inequalities (Pew Research, 2014). Minimum Wage Policies Minimum wage policies started in 1938 when the federal government established a minimum wage under the Fair Labor Standards Act. Currently, five states do not impose a minimum wage, five states impose a minimum wage under the federal level and forty two states impose a minimum wage equal or above the federal level (DOL, 2014a). A Social Problem of Poverty among Low-Income Workers The relevance of the social problem behind minimum wage policies can be easily seen by comparing the poverty level for a family of four (HHS, 2014) to the yearly income of a full-time worker earning the average of states minimum-wages (DOL, 2014a). A full-time worker earning THE EFFECT OF MINIMUM WAGE POLICIES 3 the minimum wage is far below the poverty level and the situation has only slightly improved in the last 30 years. The problem is exacerbated when considering that the level of poverty in real terms has remained the same, although the real GDP per capita has increased 52% in the same period (DOC, 2014; DOL, 2014b). Current minimum wages condemn low-income workers to poverty as can be seen in figure 1. Do Minimum Wage Policies Reduce Inequality? Inequality has grown steadily in the US in the last years due, among other factors, to the increase in wage gaps. Minimum wage policies are intended to take low-income workers out of poverty and to tackle increasing inequality. The evolution of the Gini index compared to the federal minimum wage is shown in figure 2. From 1969 to 1989 most of the rise in family income inequality was due to the growing dispersion of earnings (Daly & Valletta, 2006). The Figure 1: Full-time worker earnings and poverty level (in 2013 real terms) Source: Compiled by the author based on (DOC, 2014; DOL, 2014a, 2014b; HHS, 2014) THE EFFECT OF MINIMUM WAGE POLICIES 4 observed growth in inequality in the 1980s among low-income workers (Lee, 1999) and women (DiNardo, Fortin, & Lemieux, 1995) was explained by the erosion in the minimum wage which decreased by 27% in real terms between 1979 and 1988 (DiNardo et al., 1995). In the 1990s inequality (Daly & Valletta, 2006) and poverty rates (Addison & Blackburn, 1999) grew at lower rates because of the 16% increase in real terms in the federal minimum wage between 1989 and 1998. However, it is usually stated that minimum-wage policies contribute to destroy lowincome employment, thus potentially increasing both inequality and poverty. Moreover, the effect of minimum wages on poverty household may be overstated because most of these workers, namely young students, are evenly distributed along the entire spectrum of family incomes (Horrigan & Mincy, 1993, p. 251). According to Leigh (2007) the effect of the minimum wage on inequality is unclear because it depends on wage and labor demand elasticities of the industries. Figure 2: Evolution of the household GINI index in the US compared to the federal minimum wage Source: Compiled by the author based on (CBO, 2011) THE EFFECT OF MINIMUM WAGE POLICIES 5 Literature Review Theoretical Framework The economic orthodoxy suggests that in a perfect labor market, an increase in the minimum wage imposes a floor to wages that exceeds the competitive wage, thus decreasing employment (C. C. Brown et al., 1982, p. 488; Cahuc & Zylberberg, 2004, p. 718) as shown in figure 3. This classical view is supported by the industry to defend its arguments against minimum wage policies. However, several factors such as imperfect information, commuting costs, and inertia can generate monopsony powers to firms. The monopsony model shown in figure 4 suggests that an increase in the minimum wage could increase employment because the imposed minimum wage makes employers price-takers and more workers are willing to work at that wage (C. C. Brown et al., 1982, p. 489; Cahuc & Zylberberg, 2004, p. 721). Figure 3: Simple Supply-Demand Model of the effect of minimum wage on employment Source: Compiled by the author based on (C. C. Brown et al., 1982; Cahuc & Zylberberg, 2004) THE EFFECT OF MINIMUM WAGE POLICIES 6 Research before 1990 Supports the Simple Supply-Demand Model Research in the 1970’s and 1980’s, mainly based on time-series analyses, supported the traditional view by finding statistically significant negative employment effects resulting from minimum wage increases (Fox, 2006, p. 3). Several authors (Bernstein & Schmitt, 1998; Card & Krueger, 1995) criticized previous findings by pinpointing important flaws and publication bias. They suggested that these flaws were not analyzed because the results were so thoroughly aligned with the prevailing theory that they were not called into question (Fox, 2006). Neumark and Wascher (1998) evaluated Card and Krueger’s suggestions to conclude that the results of previous studies were not biased. Further Research Challenges the Traditional View Further research has found opposite results. Some studies used cross-sectional regional level data (Lee, 1999) and time series state-level data (Addison & Blackburn, 1999) to find a positive effect of minimum wages on reducing inequality and poverty. Other studies found a Figure 4: Monopsony Model of the effect of minimum wage on employment Source: Compiled by the author based on (C. C. Brown et al., 1982; Cahuc & Zylberberg, 2004) THE EFFECT OF MINIMUM WAGE POLICIES 7 positive effect of minimum wage on employment analyzing data of firms and workers using difference-in-difference estimators (Card, 1992; Card & Krueger, 1993) or OLS and 2SLS regressions (Katz & Krueger, 1992). These findings were highly criticized by Cahuc and Zylberberg (2004) and Neumark and Wascher (1995), but they established a new long-term trend. Today, there is an emerging consensus that the increase of minimum wages has very limited effect on unemployment (EPI, 2006, p. 1). Nobel Laureate Joseph Stieglitz of Columbia University has not found evidence of the effect of minimum wage on unemployment rate and that the minimum wage increase “was totally swamped by other factors going on in the economy” (Chipman, 2006, p.arr. 25). A statement signed by over 650 economists, including five Nobel laureates in economics and six past presidents of the American Economic Association, stated that modest increases in state and federal minimum wages can “significantly improve the lives of low-income workers and their families, without the adverse effects that critics have claimed” (EPI, 2006, p. 1). The dynamic and inefficient nature of low-wage labor markets seems to challenge the traditional view (Bernstein & Schmitt, 1998). Low-wage labor markets seem to generate monopsony powers to firms, thus making them inefficient at the current minimum wage (Ashenfelter, Farber, & Ransom, 2010; Dube, Lester, & Reich, 2011; Manning, 2003). Addison and Blackburn (1999) found that the effect was very different in the 1980’s compared to the 1990’s, suggesting that there might be dissipation thr Collapse"
101043,1990,monthly labor review,productivity in the rubber and plastics hose and belting industry,productivity in the rubber and plastics hose and belting industry,No Result.
101045,1990,monthly labor review,productivity in industry and government in 1988,productivity in industry and government in 1988,"1 New Challenges to the Automobile Production Systems in Europe.- 1.1 Introduction.- 1.2 The European Automobile Industry in Global Context.- 1.2.1 The global significance of the European automobile system.- 1.2.2 Europe's position in the global strategies of the major automobile producers.- 1.3 The Automobile Production Systems Approach.- 2 National and International Regulatory Frameworks: The Politics of European Automobile Production and Trade.- 2.1 Introduction.- 2.2 National Regulatory Environments in Europe.- 2.3 International Regulation: the European Community's Policies Towards Auto Production and Trade.- 2.4 Concluding Comments.- 3 ""Europeanisation"" in the Automotive Components Sector and Its Implications for State and Locality.- 3.1 Introduction.- 3.2 The Changing Geography of Automotive Components Production in the United Kingdom.- 3.3 ""Europeanisation"" and Corporate Strategies in the Automotive Component Sector.- 3.3.1 Bosch.- 3.3.2 Valeo.- 3.3.3 Four major UK companies: Lucas, GKN, T & N, BBA.- 3.3.4 Four major US companies: TRW, Allied-Signal, ITT and Tenneco.- 3.3.5 Japanese component companies and Europe.- 3.4 Concluding Comments.- 4 The Japanese, the European Market and the Automobile Industry in the United Kingdom.- 4.1 Introduction.- 4.2 Competition Between Automobile Companies in the United Kingdom.- 4.3 Cooperation Between Automobile Companies in the United Kingdom.- 4.4 Cooperation Between Component Suppliers and Automobile Producing Companies in the United Kingdom.- 4.5 Competition and Cooperation Between Component Companies in the United Kingdom and European Community.- 4.6 Capital: Labour Relations.- 4.7 State Regulation of Japanese Competition: European Community Trade Policies, National Interests and Corporate Interests.- 4.8 The Local and Regional Development Implications of Japanese Inward Investment: Just-in-Time and In One Place ?.- 4.9 Concluding Comments.- 5 The German Automobile Production System Going European.- 5.1 Introduction.- 5.2 Market Structures During the 1980s.- 5.3 Competitiveness via the Technological Competences of Automobile Producers.- 5.3.1 The emergent spatial pattern.- 5.3.2 Choices in product and process.- 5.3.3 Two joint strategies: Europeanisation and flexibilisation.- 5.3.4 The 1990s.- 5.4 Organisational and Spatial Restructuring of the Components Sector.- 5.5 Consequences for Labour.- 5.6 The Changing Geography of the German Automobile Production System During the 1990s.- 6 The Italian Automobile Industry and the Case of Fiat: One Country, One Company, One Market ?.- 6.1 Introduction.- 6.2 The Italian Automobile Industry: Some Structural Features.- 6.3 The Relationship Between the Automobile Industry and Government Economic Policies.- 6.3.1 Protectionist policies.- 6.3.2 Bail-out policies.- 6.3.3 Policies of territorial re-equilibrium.- 6.3.4 Other forms of state intervention.- 6.4 Spatial Strategies and Reorganisation Strategies.- 6.4.1 The approach: the automobile industry as a complex industrial system.- 6.4.2 From expansion to crisis.- 6.4.3 The first turning point.- 6.4.4 The 1980s: the great rationalisation.- 6.5 Continuity and Discontinuity in the Geography of the Italian Automobile System.- 6.6 Concluding Comments.- 7 Competitive Strategies in the World Market: The Case of Renault and the Emergence of a European Group ?.- 7.1 Introduction.- 7.2 Why Did European Automobile Companies Have to Make Strategic Changes ?.- 7.3 A Basic Answer: Improving Efficiency.- 7.3.1 Project structures.- 7.3.2 Development of quality.- 7.3.3 Automation, just-in-time manufacturing and supply.- 7.3.4 A new supply policy.- 7.3.5 Restructuring the European plants.- 7.3.6 National and international agreements.- 7.4 A Missed Opportunity: Renault's Failure to Grow Outside Europe.- 7.4.1 The missed deal in the USA.- 7.4.2 The weak positions in other countries.- 7.5 The Result: The Weaknesses of Renault in the 1990s.- 7.6 A European Answer: Rise and Fall of the Renault-Volvo Merger.- 7.6.1 1990-1993: the deepening of the alliance.- 7.6.2 September 1993: the move towards a merger.- 7.6.3 December 1993: the merger failure.- 7.7 Privatisation and After?.- 7.7.1 Towards a Renault-Fiat deal ?.- 7.7.2 Towards new international alliances within Europe.- 8 The Restructuring of the Swedish Automobile Production System.- 8.1 Introduction.- 8.2 The Swedish Automobile Production System - Some General Characteristics.- 8.3 Volvo and Saab: Corporate Structures and Strategies.- 8.3.1 Diversification.- 8.3.2 Internationalisation.- 8.3.3 Production organisation and location.- 8.4 1970 to 1987: From Crisis to Success.- 8.5 1988 to 1992: Renewed Crisis and Intensive Restructuring.- 8.5.1 Saab Automobile.- 8.5.2 Volvo.- 8.6 Post 1992: Future Prospects for Swedish Car Production.- 8.6.1 Lost national identities ?.- 8.6.2 Prospects for component suppliers.- 8.7 Concluding Comments.- 9 Multi-purpose Vehicles, a New Opportunity for the Periphery ? Lessons from the Ford\VW Project (Portugal).- 9.1 Introduction.- 9.2 Prospects for the Growth of Multi-purpose Vehicles: The Visible Hand of the Single Market.- 9.3 The Automobile Industry in Portugal.- 9.3.1 The formation of the industry.- 9.3.2 The current situation.- 9.4 The Ford Wolkswagen (AutoEuropa) Project.- 9.4.1 Background.- 9.4.2 A brief description of the project.- 9.4.3 Investment and financing.- 9.4.4 Production and markets.- 9.4.5 Employment.- 9.4.6 The process of setting up supplier-networks.- 9.4.7 The geography of direct suppliers: possible outlines of an emerging archipelago.- 9.5 Concluding Comments.- 10 Interdependent and Uneven Development in the Spatial Reorganisation of the Automobile Production Systems in Europe.- 10.1 Introduction.- 10.2 Challenges to the Core from Western and Southern Europe.- 10.3 Central and Eastern Europe - the New 'Frontier' of European Automobile Production.- 10.4 Resistance to the Erosion of the European Automobile Core.- 10.5 Towards a New Map of Automobile Production in Europe ?.- Author Index.- Location Index. Collapse"
101046,1990,monthly labor review,"population changes, the baby boom, and the unemployment rate",population changes the baby boom and the unemployment rate,"The aging of the U.S. baby-boom generation and its impact on the decline of the unemployment rate during the 1980s are discussed. The author considers ""the effect that population changes might have on other key indicators of labor market activity such as the labor force participation rate and the employment-population ratio."" Included is a decennial analysis of unemployment and labor force participation by sex and age for the period 1959-1989. (EXCERPT) Collapse"
101049,1990,monthly labor review,employee absences in 1989:  a new look at data from the cps,employee absences in 1989 a new look at data from the cps,ERR
101050,1990,monthly labor review,would a higher minimum wage help poor families headed by women?,would a higher minimum wage help poor families headed by women,"Social Security's special minimum primary insurance amount (PIA) provision was enacted in 1972 to increase the adequacy of benefits for regular long-term, low-earning covered workers and their dependents or survivors. At the time, Social Security also had a regular minimum benefit provision for persons with low lifetime average earnings and their families. Concerns were rising that the low lifetime average earnings of many regular minimum beneficiaries resulted from sporadic attachment to the covered workforce rather than from low wages. The special minimum benefit was seen as a way to reward regular, low-earning workers without providing the windfalls that would have resulted from raising the regular minimum benefit to a much higher level. The regular minimum benefit was subsequently eliminated for workers reaching age 62, becoming disabled, or dying after 1981. Under current law, the special minimum benefit will phase out over time, although it is not clear from the legislative history that this was Congress's explicit intent. The phaseout results from two factors: (1) special minimum benefits are paid only if they are higher than benefits payable under the regular PIA formula, and (2) the value of the regular PIA formula, which is indexed to wages before benefit eligibility, has increased faster than that of the special minimum PIA, which is indexed to inflation. Under the Social Security Trustees' 2000 intermediate assumptions, the special minimum benefit will cease to be payable to retired workers attaining eligibility in 2013 and later. Their benefits will always be larger under the regular benefit formula. As policymakers consider Social Security solvency initiatives--particularly proposals that would reduce benefits or introduce investment risk--interest may increase in restoring some type of special minimum benefit as a targeted protection for long-term low earners. Two of the three reform proposals offered by the President's Commission to Strengthen Social Security would modify and strengthen the current-law special minimum benefit. Interest in the special minimum benefit may also increase because of labor force participation and marital trends that suggest that enhancing workers' benefits may be a more effective means of reducing older women's poverty rates than enhancing spousal or widow's benefits. By understanding the Social Security program's experience with the special minimum benefit, policymakers will be able to better anticipate the effectiveness of other initiatives to enhance benefits for long-term low earners. This article presents the most recent and comprehensive information available about the special minimum benefit in order to help policymakers make informed decisions about the provision's future. Highlights of the current special minimum benefit include the following: Very few persons receive the special minimum benefit. As of December 2001, about 134,000 workers and their dependents and survivors were entitled to a benefit based on the special minimum. Of those, only about 79,000 received a higher total benefit because of the special minimum; the other 55,000 were dually entitled. (In effect, when persons are eligible for more than one type of benefit--that is, they are dually eligible--the highest benefit payable determines total benefits. If the special minimum benefit is not the highest benefit payable, it does not increase total benefits paid.) As of February 2000, retired workers who were special minimum beneficiaries with unreduced benefits and were not dually entitled were receiving, on average, a monthly benefit of $510 per month. That amount is approximately $2,000 less than the annual poverty threshold for an aged individual. Special minimum benefits provide small increases in total benefits. For special minimum beneficiaries who were not dually entitled as of December 2001, the average special minimum monthly PIA was just $39 higher than the regular PIA. Most special minimum beneficiaries are female retired workers. About 90 percent of special minimum beneficiaries are retired workers, and 77 percent of those retired workers are women. The special minimum benefit has never provided poverty-level benefits. Maximum payable special minimum benefits (unreduced for early retirement) equal 85 percent of the poverty level for aged persons, down from 96 percent at the provision's inception. Major public policy considerations raised by this analysis include the following: Social Security benefits alone do not protect all long-term low earners from poverty. Low earners with 30 years of earnings equal to the annual full-time minimum wage who retired in selected years from 1982 to 2000 received benefits that were 3.9 percent to 20.1 percent below the poverty threshold, depending on the year they retired. For 40-year earners, the range was 3.9 percent to 15.3 percent below poverty. Furthermore, in 1993, 29.2 percent of retired-worker beneficiaries who were poor had 30 or more years of coverage. The size of the universe of persistently low earners with significant attachment to the covered workforce is unknown. Available research that examines two 28-month periods suggests that only 4 percent to 6 percent of full-time, full-period earners had below-minimum wages for more than 12 consecutive months. Targeting enhanced benefits only toward long-term, regular workers who are low earners is difficult under the current Social Security program. All else being equal, if total wage-indexed lifetime covered earnings are the same for both a full-career low earner and for a high earner who has worked only occasionally, then their Social Security benefits will be identical. Social Security has no information on number of hours worked, hourly wages, or other information that could distinguish between two such persons. Collapse"
101052,1990,monthly labor review,life insurance benefits for retired workers,life insurance benefits for retired workers,"The concern about the aged after retirement has been raised in Korea and it becomes very important problem to be solved now. The reasons why we have to be are as follows : The increasing in the life expectancy since the mid-70's has resulted in the aged phenomena of the population of the old : Though many retirees still have abilities to work, they have to leave the jobs by the regulation of age limit : So, many employees want to have opportunities to work continuously by re-employment or searching other jobs; Since pension system in Korea hasn't been settled down, yet, most retirees has .suffered from poverty; Because that the ageds social roles has been deprived the retirees by rapid social change through industrialization, they have been isolated from Korean society. Though recently government has asked companies to extend retirement age and to employ the aged supporting financial system, most companies are not providing such opportunities as well as for the retirement. If these opportunities are supported for pre-retirees, they'll develop themselves. secure their family lives, and contribute to development of community as adult learners. Recognizing the necessity of education for adaptation on social life after retirement. the objective of this study is to suggest the theoretical background for making the adult educational program. As basic materials to do it, social policy about the retirement, individual problems faced in this term and its influences on each are studied. First, though there are social policies about the retirement like health. residence, income and other social services in Korea, more positive policies like re-employment system which can extend retirement age are required. Second, retirees faced individual affairs at the family life cycle like children's marriage, separating from them, parents' passing away and realizing of getting old. Third, they faced followings through the retirement; discontinuity of work, no more career, decreasing of income, the responsibility of the retirement grants, adaptation on the new social roles and so on. Though they have opportunities to work continuously, their status, contents of job and wage are different from before. Otherwise, retirement can be defined in various ways. Some theorists regard any person who performs no gainful employment during a given year as retired. Others apply this definition only to those who are currently receiving retirement pension benefits. And still others consider anyone who is not employed full time, year around, as retired. In Korea, the considerable focus on the retirement is divided into such as the retirees' committing suicide, unsatisfying act and/or regulation for retirees. the extension of the average length of life. On an aspect of demographic consideration, the proportion of older people who retired has increased dramatically since the turn of 1960s : 961, 319 in 1966 to 1,749,549 ill 1985 over 65 of age. Pertaining to the focus, this research aimed to check the late adulthood's living surrounds, to study the institutional system on the mandatory retirement, and to settle down the criteria for formal retirement preparation program. For purposes of the present discussion, a retired person is one who is not employed at a full-time paying job and who receives at least some income from a pension due to prior employment. The retirement is an earned reward, one that results from having previously been a member of the labor forces. Also, retirement means a total separation from the world of work. Lots of retirees anticipate financial problems during retirement, even though these also expect retirement to reduce their income up to 50% from pre-retirement. Few employees take concrete action to ensure that their retirement income will be adequate. However, there is no indication that this lack of formal pre-retirement planning has any substantial impact on postr-etirement well-being. Since voluntary retirement affect only a very small number of employees, the decision to retire must often be due to other reasons. These include the employee's job level and job satisfaction, age discrimination and employer pressures to retire, actual and self-perceived health, the expectation of a more enjoyable life, and the influence of one's family, friends. and coworkers. Phases of retirement have been suggested that retirees tend to proceed through a series of distinct psychological and emotional stages. Retirement may well begin with a euphoric and busy honeymoon phase, during which the individual eagerly tries to do many of the things that were ruled out by full-time work. This intense activity may be follwed a period of letdown or disenchantment, especially if the individual's prior expectation of retirement were unrealistically positive. And other phase, the retirement produces a substantial drop income, so much so that the retiree's standard of living and satisfaction with life are adversely affected. And another phases are those retirees' facing on poor health, nothing to do, and the biggest loneliness. Here again, the voluntary retiree is an optimistic one: the minority find retirement to be a busy and satisfying period. In particular, those individuals who developed satisfying hobbies and recreational activities prior to retirement are most likely to enjoy their years away from full-time work. However, those who had few avocations during their working years may well find that retirement is disenchanting and depressing. After retirement the important factors to make life are time, money and relation with others. First, retirees should spend their time effectively whatever they would do. If they fail to do it, they can get bad mental conditions like tension, despair, irratation and so on. Second, only with dependence on pension, insurance and savings, they can't cope with inflation. Third, if they fail to relate with others continuously, they'll feel seriously isolated. In conclusion, this study couldn't find out to be any kinds of the mandatory retirement policy for them, to subsidize those retirees beyond the retirement pension, to solve both psychological and emotional problems of their having faced, and to be carried out the retirement preparation program. Therefore, this project suggest to keep implementing of their role playing in social activities; to be instructed as techniques on the interest raising between save and withdraw of pension; to be daily exercised for the health betterment and maintaining; to inquire into working alone, working with others, and working half and half; to prepare, and then to participate the retirement preparation program; and to be specialized for post-retirement well-being. Retirees should have wisdom to turn this critical moment into opprotunity to deveolop themselves. Social policy also offer such an opprotunity, too. However, the retiree's can be helped to make their renew life desirable through the programs viz, how the eged cope with his/her life So, educational programs for the retirees to adapt on their new life should be developed considering such suggestions as ; Financial reward should be offered through taking educational programs: These programs should satisfy the needs to belong to somewhere; Participation in any institutions should offer chances to share retirees' experiences with the young workers and to enhance their abilities to communicate with many others; Educational researcher should find out the common interest which the retirees want to learn regardless of their various characteristics; Many informations about how the retirees cope with the life after retirement should be presented before retirement. These informations should be open to anyone; Educational programs should be very practical which can be adated on daily life; and Educational programs should solve the aged' affectional problem like loneliness, despairs and so on. Collapse"
101054,1990,monthly labor review,profiles in safety and health:  roofing and sheet metal work,profiles in safety and health roofing and sheet metal work,ERR
101071,1990,national tax journal,taxes and spending under gramm-rudman-hollings,taxes and spending under gramm rudman hollings,"Discusses the three key elements of Gramm-Rudman-Hollings (GRH) that were expected to affect budgeting. Concludes that while many people regard the GRH law as a failure, it is not clear that any other process could have achieved it's modicum of success. Collapse"
101073,1990,national tax journal,"precautionary saving, wealth accumulation, and the saving downturn of the 1980s",precautionary saving wealth accumulation and the saving downturn of the 1980s,Argues that the dismal performance of the personal saving rate during the 1980's does not imply that U.S. households during this period were spendthrifts. Tests for the impact of precautionary saving using an Euler-equation approach based on a utility-maximizing model of life cycle consumption. Collapse
101074,1990,national tax journal,iras and national savings,iras and national savings,"A growing body of evidence suggests that young people have a low level of financial understanding and that they are making poor financial choices. It is a development reflected in part in savings rates. Personal saving is calculated as disposable income less personal spending. The U.S. personal savings rate has fallen since the early 1990s. From 1990 to 1995, the average annual rate of personal savings was 8.72 percent. From 1997 to 2001, the average annual rate was 3.32 percent. This percentage of savings suggests that American households are financially overextended. The problem is also reflected in various measures of students' knowledge. Lewis Mandell, dean of the university at Buffalo's School of Management, has conducted a national survey on the financial understanding of twelfth grade students for the Washington, D.C.-based nonprofit Jump$tart Coalition. (1) The survey, carried out in 1997, 2000, and 2002, measures twelfth graders' knowledge of money, income, saving, spending, and credit; it also inquires about students' personal finance such as use of money and credit. The 2002 survey was administered to 4,024 twelfth graders in 183 schools nationally; on average, these students answered 50.2 percent of the questions correctly. The average score in the 2000 survey was 51.9 percent; in 1997, it was 57.3 percent. Students' knowledge and understanding, as measured by the Jump$tart survey, has been declining nationally. A study conducted for Northwestern Mutual Financial Network by the market research firm Harris Interactive, titled Generation 2001: The Second Study, (2) focused on a national, representative sample of college seniors. It was a follow-up to a study conducted in 1997 when the students of the class of 2001 were freshmen. The results offer some insight into the attitudes of this generation, including attitudes related to financial matters. Here the results reveal important knowledge gaps and a disconnect between attitude and behavior: * Nearly half (48 percent) of college seniors felt ""not very knowledgeable"" or ""not very knowledgeable at all"" regarding financial matters. * Despite their lack of knowledge, nearly three-quarters believed it was very likely that they would eventually be able to afford the life-style of their childhood. * While college students rated home ownership, life insurance, 401(k)s, and IRAs as important financial instruments, far fewer expressed high levels of knowledge about these instruments. * These college students held, on average, three credit cards each, and most had already incurred a significant debt load. Why Do People Appear to Fail to Act in Their Financial Best Interest? A basic assumption of economics is that people act in their own best interest. But the evidence discussed previously suggests that when it comes to saving, investing, and credit management, people's knowledge is minimal and their actions may not be in their best interest. Why? Let's look more closely. We know that in other arenas, people often engage in risk-oriented behavior. Some people play football or compete in rodeos when they know they might injure their knees and backs, or worse. Some people smoke when they know smoking causes cancer and heart disease. Some people drive fast and change lanes recklessly even when they know that, in doing so, they could kill themselves and others. People do many things that do not appear to be in their best interest. They also incur risk by virtue of what they do not do-not eating a diet rich in fruits, vegetables, and fish, for example, despite well-known evidence about the health benefits associated with such a diet. Why might people fail to act in the best interests of their own health? Consider the costs and benefits involved-as they might be assessed by the people whose behavior seems, in these cases, to be irrational. For many among us, the cost of eating a healthful diet is immediate and real. … Collapse"
101075,1990,national tax journal,transfer pricing issues,transfer pricing issues,"Explains how transfer pricing rules have become a major factor in determining U.S. tax liability of multinational firms. Discusses foreign-owned U.S. corporations, worldwide activity of U.S. multinationals, and the application of formulary methods. Collapse"
101076,1990,national tax journal,health policy issues for the 1990s,health policy issues for the 1990s,"Abstract On February 5, 1990, Washington Evaluators, an affiliate of the American Evaluation Association, convened a panel discussion of evaluators, representing both the public and private sectors, to discuss the challenges that evaluators in government and private practice must face in meeting the information needs of decision makers in the 1990s. At that panel discussion, Ron Carlson and Jay Bell discussed what they feel will be the most difficult policy and evaluation questions to be raised in the 1990s on the issue of access to health care for the uninsured and the underserved people in the Nation. Ron first presented a clear delineation of the questions to be raised in the debate over access to care and the role that evaluation data will play in that political debate. Jay followed with suggestions for evaluators to pay particular attention to the access issues, to alternative programs, and to proposed solutions; and for making adjustments in their approach to the design of evaluations. Collapse"
101077,1990,national tax journal,"franchises, intangible capital, and assets",franchises intangible capital and assets,Focuses on the treatment of intangible capital involved in franchising. Explores the problems facing both the franchisor and the franchisee.
101078,1990,national tax journal,corporate integration puzzles,corporate integration puzzles,Focuses on unresolved economic issues that may effect both the desirability of integration and how an integration system should be structured. Discusses the implications of the uncertainty for policy design. Collapse
101079,1990,national tax journal,fiscal federalism and the changing global economy,fiscal federalism and the changing global economy,No Result.
101080,1990,national tax journal,the state and local fiscal outlook: what have we learned and where are we headed?,the state and local fiscal outlook what have we learned and where are we headed,"Analyzes how state and local governments responded to the economic conditions and political movements of the 1980's. Speculates whether the 1990's will be an extension of the 1980's, or whether there are new forces that will break these trends. Collapse"
101081,1990,national tax journal,raising revenue by taxing activities with social costs,raising revenue by taxing activities with social costs,"Surveys some results of current research on the social costs of smoking, abusive drinking, and pollution. Attempts to provide some background to tax economists, practitioners, and policymakers who may be unfamiliar with what health economists and other social scientists have to say about the costs of drinking and smoking, and what environmental economists have to say about taxes on pollution. Collapse"
101082,1990,national tax journal,the forces shaping tax policy today,the forces shaping tax policy today,Representative Bill Gradison explains that two of the most important issues shaping tax policy are foreign economic competition and social policy.
101083,1990,national tax journal,social security: the labrea tar pits of public policy,social security the labrea tar pits of public policy,Discusses the proposed cuts of social security and what the outcome of those cuts may be. Also discusses cutting the payroll tax and privatization.
101084,1990,national tax journal,the rise and fall of the medicare catastrophic coverage act,the rise and fall of the medicare catastrophic coverage act,Lays out the basic chronology of the legislature that led to the Medicare Catastrophic Coverage Act which was a major piece of social legislature in the Reagan administration which was repealed a year and half later. Discusses some lessons to be learned from that experience. Collapse
101180,1990,public choice,the impact of citizen influence on local government expenditure,the impact of citizen influence on local government expenditure,"This paper examines the impact of three measures of direct citizen influence — the initiative, referendum, and recall — on the level of local public expenditure for a national sample of communities with 10,000 persons or more. Two types of statistical tests are performed to analyze the role of the median voter model and to measure the effect of these governmental characteristics on the level of public spending.Like earlier literature, this paper finds only modest effects of these structural characteristics on local government expenditure. Alternative methodologies are needed to explore the ambiguities which exist in many of the previous studies. Collapse"
101181,1990,public choice,efficient use of reference group cues in a single dimension,efficient use of reference group cues in a single dimension,"If there are groups whose endorsements voters can use as positive (or negative) cues, we demonstrate that voters do not need to know anything directly about candidate positions to be able to identify the candidate whose issue positions and performance is likely to be closest to the voter's own preferences. In one dimension we show that, given certain simplifying assumptions, voters are best off adopting the choice recommended by the single reference group to which they are closest. We also show that even a decision by reference groups not to endorse any candidate may be informative to voters. Collapse"
101183,1990,public choice,fiscal variables and growth:  a cross-sectional analysis,fiscal variables and growth   a cross sectional analysis,"This paper examines the impact of the key fiscal variables — taxes, expenditures, and deficits — on economic growth performance, using a reduced-form model and cross-sectional data for a sample of 76 developed and developing countries for the period 1972–81. Its simultaneous consideration of fiscal variables overturns the results of some existing studies. While taxes seem negatively associated with GDP growth, they are concommitant with a higher rate of growth when their benefits in terms of reducing deficits are taken into account. The positive association of government expenditures with GDP growth is rendered negative when their impact on deficits are factored in. Deficits are contractionary, and deficit-reducing tax increases and expenditure cuts are positively associated with growth. A balanced budget expansion of taxes and expenditures is negatively associated with growth. When separating the sample into low-, middle- and high-income countries, these results hold only for the second group indicating that the level of development influences the linkages between fiscal variables and GDP growth. Collapse"
101184,1990,public choice,state lotteries as duopoly transfer mechanisms,state lotteries as duopoly transfer mechanisms,"Government-sponsored lotteries can be traced back at least as far as Caesar Augustus, who instituted legalized gambling for the purpose of rebuilding Rome (Mote, 1984). We are also told that Queen Elizabeth introduced a lottery in 1549, and that lotteries were used to provide financial support for the Virginia Company's founding of Jamestown (Johnson, 1960). No less authority on the topic than Captain John Smith reported that lotteries were ""the reall and substantial food by which Virginia has been nourished."" (Johnson, 1960: 161.) Later, lotteries were used by the Continental Congress and several colonies to fund the Revolutionary War, and such venerable institutions as Harvard, Yale, Princeton, and Dartmouth benefited from their use (Kaplan, 1984). In more recent times, lotteries provided an important source of revenues for the depressed post-Civil War Southern states. But criticism regarding their moral implications and corruption in their operation is given as the explanation for Congressional action that foreclosed state lotteries in 1895 by prohibiting the use of the mail for distributing lottery tickets and information (Tillett, 1983). In that same year, Congress passed a national income tax, which was struck down by the court. Some might argue that the demise of state lotteries was related to the federal government's attempt to extend its ability to collect Collapse"
101185,1990,public choice,"cycle avoiding trajectories, strategic agendas, and the duality of memory and foresight:  an informal exposition",cycle avoiding trajectories strategic agendas and the duality of memory and foresight an informal exposition,"This paper considers the notion of cycle avoiding trajectories in majority voting tournaments and shows that they underlie and guide several apparently disparate voting processes. The set of alternatives that are maximal with respect to such trajectories constitutes a new solution set of considerable significance. It may be dubbed the Banks set, in recognition of the important paper by Banks (1985) that first made use of this set. The purpose of this paper is to informally demonstrate that the Banks set is a solution set of broad relevance for understanding group decision making in both cooperative and non-cooperative settings and under both sincere and sophisticated voting. In addition, we show how sincere and sophisticated voting processes can be viewed as mirror images of one another — embodying respectively, “dmemory” and “foresight.” We also show how to develop the idea of a “sophisticated agenda,” one in which the choice of what alternatives to propose is itself a matter of strategic calculation. Collapse"
101187,1990,public choice,investments in rent-seeking,investments in rent seeking,"In this article we examine the extent to which three minority groups were able to achieve selected neighborhood social and physical outcomes in the San Francisco mnetropolitan area. Ecological regressions were estimated to generate elasticities that measure the relative abilities of blacks, Hispanics, and Asians to convert education and income into desirable neighborhood environments. These regressions were interpreted in light of substantial differences between the three groups in levels of residential segregation. Results generally indicated a black disadvantage in the process of residential achievement, but it was not as dramatic as that found in earlier studies or as great as the levels of segregation would suggest. As in prior research, education was found to be the critical variable in explaining spatial differentiation and class stratification among blacks. In the United States, residences are allocated to persons and families through private housing markets. Since public housing comprises less than 2% of the nation's housing stock, and less than 1% of its housing starts (Adams 1987), virtually all households seeking a new residence enter the rental or sale market. Each household has a set of housing needs and desires based on its size, composition, life-cycle stage, and tastes; it also has a set of economic resources with which to achieve these desires, principally capital assets and income. Markets allocate households to specific residences through the mechanism of price; households purchase or rent the home that best suits their needs at the price they can afford (Berry & Rees 1969). According to neoclassic economic theory, the price of housing reflects the balance between aggregate demand and supply within local markets (Alonso 1964; Mills 1972). Researchers have pointed out, however, that housing is different from other commodities, and that these differences structure housing markets in distinct ways. First, residences are immobile; they are tied to a particular piece of land and cannot easily be consumed elsewhere (Logan & Molotch 1987). Second, they represent a very significant investment for most families, and for homeowners are the primary means of capital accumulation *Direct correspondence to the authors at the Population Research Center, University of Chicago, 1155 E. 60th Street, Chicago, IL 60637. i) The University of North Carolina Press Social Forces, September 1990, 69(1):15-32 This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 16 / Social Forces 69:1, September 1990 (U.S. Bureau of the Census 1986). Third, each residence is not only tied to a particular plot of land, it is also bound to a specific neighborhood (Massey et al. 1987) and, in turn, to a larger municipality (Logan 1978). Moreover, each geographic unit is associated with a social environment defined by the behaviors of residents and the service benefits provided by local government (Schneider & Logan 1982, 1985; Massey et al. 1987). Finally, houses, and the communities in which they are found, are more than mere commodities; they are also objects of powerful sentiments that influence judgments and condition decisions (Logan & Molotch 1987). In recognition of the fact that housing is immobile and linked to particular places, Charles Tiebout (1956) proposed a ""pure theory of local expenditures"" to account for the differentiated residential structure of cities. The theory rests on two key assumptions that households are free to move, and that places compete to attract them. Local governments offer different packages of tax costs, service benefits, and zoned environments from which home seekers choose. Over time, market competition produces a variety of service-tax-environment mixes and distributes households among them according to income, preferences, and wealth, yielding a residential structure differentiated by socioeconomic status, family life-cycle stage, race, and ethnicity. Although Tiebout's model recognizes some unique features of housing markets, it does not incorporate the fact that housing costs, and particularly home-ownership costs, reflect a substantial investment for families, or that homes, neighborhoods, and communities are the focus of strong emotional attachments. These traits give rise to what Stinchcombe (1965) calls ""communities of fate,"" where residents and institutions have large stakes not only in their own property, but in the property and characteristics of people in surrounding areas, making collective action highly likely. This propensity for collective action segments housing markets along social lines, so that individual choices are constrained by institutional practices and the collective behavior of others (Logan & Molotch 1987). The segmentation of housing markets, and the constraints it imposes on individual home seekers, are well illustrated by the case of U.S. blacks. The Tiebout model and its successors (Bish 1971; Peterson 1981) assume that households are free to move wherever their tastes and economic resources take them. Given this assumption, racial segregation is interpreted simply as the coincidental by-product of sorting based on income, wealth, and tastes (viz. Clark 1986). Considerable evidence, however, indicates that segregation does not stem from black preferences or low black-income levels. Public opinion polls show that blacks strongly endorse the principle of residential integration (Schuman et al. 1985) and express a clear preference for living in integrated neighborhoods, other things being equal (Farley et al. 1978). In spite of these tastes for integration, however, desegregation does not follow from the simple acquisition of socioeconomic resources sufficient to support spatial mobility. As education, income, and occupation rise, black-white segregation does not decline, but persists at a very high level (MIassey 1979, 1981; Denton & Massey 1988). Rather than reflecting tastes or socioeconomic status, black segregation appears to stem from constraints to black residential mobility imposed by the This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms Segregation and Neighborhood Quality / 17 collective behavior and institutional actions of whites. Studies of the real estate industry, for example, indicate that discrimination and prejudice are still widespread. Real estate agents systematically steer black home seekers away from white neighborhoods, and provide them less favorable treatment than whites (Molotch 1972; Pearce 1976; Wienk et al. 1979; Yinger 1986), actions which have been linked analytically to high levels of residential segregation (Galster 1986). At the same time, lending institutions have been found to finance a disproportionately small number of loans in black neighborhoods, even after objective social and economic factors are controlled (Taggart & Smith 1981; Pol et al. 1982; Leahy 1985). Moreover, in the few cases where blacks succeed in entering a white neighborhood, they are frequently met by organized white resistance and hostility (Hirsch 1983; Cass 1986; Bauman 1987), especially in working-class areas (Logan & Stearns 1981; Stearns & Logan 1986a); and if blacks succeed in establishing themselves, the neighborhood is quite likely to be avoided by subsequent white home seekers, resulting in eventual resegregation (Massey & Mullan 1984). Thus, the collective action of white residents and the institutional practices in the real estate and banking industries bifurcate urban housing markets along racial lines, fostering high levels of segregation despite a strong demand by blacks for integration. Some studies suggest that Hispanics face similar barriers, but to a lesser degree (Hakken 1979; James & Tynan 1986). The effect of a racially segmented housing market in creating and sustaining segregation is crucial to understanding the socioeconomic position of blacks in the United States. Barriers to spatial mobility are, in a very real way, barriers to social mobility. As Logan and Molotch (1987:49) point out, socioeconomic inequality among households and geographic inequality among places are not independent; the two systems of hierarchy reinforce one another: ""High status within the social hierarchy can bring access to the most desirable places ... and a guarantee of a rewarding future for whatever place one controls. At the same time a high status for one's geographical place means the availability of resources . . . that enhance life chances generally."" In a similar vein, Giddens (1980:107-12) argues that segregation is a core mechanism of class structuration, since it concentrates people of low status in space and ensures the maintenance of behaviors and orientations detrimental to success in the larger society. In this article we demonstrate how segregation structures the neighborhood environment achieved by three minority groups in one large urban area. We estimate the degree to which blacks, Hispanics, and Asians are able to convert socioeconomic achievements into selected neighborhood outcomes within the San Francisco-Oakland Standard Metropolitan Statistical Area (SMSA) in 1980. The analysis proceeds in three phases. First, we establish the degree of segregation experienced by blacks, Hispanics, and Asians in the San Francisco SMSA. We then consider the extent to which the degree of segregation faced by each group affects its ability to convert status attainments into neighborhood social outcomes. Finally, we evaluate the degree to which each group is able to convert status attainments into different physical characteristics of the neighborhood environment. This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 18 / Social Forces 69:1, September 1990 Collapse"
101188,1990,public choice,committees and the core of the constitution,committees and the core of the constitution,"728 www.thelancet.com Vol 380 August 25, 2012 methodological training for members of the trial team and for research ethics committee members. Kenter and Cohen request that trial applications be reviewed by competent experts, while acknow ledging that such expertise is unlikely to be evenly distributed within the EU. They do not suggest how this gap should be fi lled. This challenge has been a major issue for one of the subgroups of our OECD Global Science Forum Working Group. We recommend the development of global core competencies as a compendium of required knowledge and skills for investigators and other members of the clinical trials team, research ethics committees, regulatory bodies, and sponsors. These training programmes should be open to the general public. Standardised, internationally recognised accredited qualifi cations in patient-oriented research should be defi ned in connection with these competencies. In this way, we can improve the training of everybody involved in clinical trials. Collapse"
101189,1990,public choice,the pits and the core:  simple collective decision problems with concave preferences,the pits and the core simple collective decision problems with concave preferences,"This essay extends the theory of simple collective decision problems to spatial games in which (contrary to the traditional assumption) each agent's preferences are concave, in the sense that the alternatives that the agent does not prefer to any particular reference alternative together constitute a convex set. Such concave preferences might characterize decision problems in which, say, a site must be selected for some obnoxious facility, such as a prison, garbage dump, or facility for managing hazardous materials. The results indicate that, under these conditions, the (weak a-)core can be structurally unstable, changing discontinuously with apparently minor perturbations of the decision problem. The main theorem identifies a curious property of the core when the set of feasible alternatives is compact and convex and each agent's preferences are strictly concave. Namely, a point in the feasible set's interior can belong to the core only if there is no feasible alternative that makes every member of any winning coalition strictly worse off. In this sense, an interior point belongs to the core only if it lies in ""the pits."" Collapse"
101190,1990,public choice,the structure of the banks set,the structure of the banks set,"We study the impact of dust evolution in a protoplanetary disk (PPD) around a T Tauri star on the disk's chemical composition. For the first time, we utilize a comprehensive model of dust evolution, which includes growth, fragmentation, and sedimentation. Specific attention is paid to the influence of grain evolution on the penetration of the UV field in the disk. A chemical model that includes a comprehensive set of gas-phase and grain-surface chemical reactions is used to simulate the chemical structure of the disk. The main effect of grain evolution on the disk's chemical composition comes from sedimentation and, to a lesser degree, from reduction of the total grain-surface area. The net effect of grain growth is suppressed by the fragmentation process which maintains a population of small grains, dominating the total grain surface area. We consider three models of dust properties. In model GS, both growth and sedimentation are taken into account. In models A5 and A4, all grains are assumed to be the same size (10?5?cm and 10?4?cm, respectively) with a constant gas-to-dust mass ratio of 100. As in previous studies, the three-layer pattern (cold midplane, warm molecular layer, and hot atmosphere) in the disk-chemical structure is preserved in all models, but shifted closer to the midplane in models with increased grain size (GS and A4). Unlike other similar studies, we find that in models GS and A4, the column densities of most gas-phase species are enhanced by 1-3 orders of magnitude relative to those in a model with pristine dust (A5), while column densities of their surface counterparts are decreased. We show that column densities of certain species, such as C2H, HC2n+1N (n = 0-3), H2O, and some other molecules, as well as the C2H2/HCN abundance ratio, all of which are accessible with Herschel and ALMA, can be used as observational tracers of early stages of the grain evolution process in PPDs. Collapse"
101191,1990,public choice,probability calculations for transitivity of simple majority rule with anonymous voters,probability calculations for transitivity of simple majority rule with anonymous voters,"Let P(n, 4) denote the probability that there is a pairwise simple majority rule winner on four candidates with n voters under the impartial anonymous culture condition. Similarly, let Pt(n, 4) denote the probability that the complete ranking obtained by pairwise simple majority voting on four candidates with n voters is transitive under the impartial anonymous culture condition. We obtain representations for P(n, 4) and Pt(n, 4), and computed values for each for odd n with 3 ≤ n ≤ 19. Collapse"
101192,1990,public choice,general fund financing versus earmarked taxes:  an alternative model of budgetary choice in a democracy,general fund financing versus earmarked taxes an alternative model of budgetary choice in a democracy,"This paper presents an alternative model of budgetary choices in a direct democracy. Our model is not only more consistent with the underlying assumptions of consumer theory, but also is easier to apply to the study of problems such as the impact of lump-sum subsidies, changes in the costs of production, and so forth, on the equilibrium levels of collective variables.The model appears to account for the hitherto inexplicable perverse results that plague many of the general fund financing models discussed in the literature to date.We also show that if one corrects for the current incompatibility of assumptions about decision-making at the micro level, then the resulting budget under GFF is always greater or equal to that under earmarked taxes. This is a much less ambiguous result than that obtained in previous models where anything could happen. Collapse"
101193,1990,public choice,collective decision making and the limits on the organization's size,collective decision making and the limits on the organizations size,"This essay clarifies the relationship between the “technology” of organizational decision making and the limits on the size of the group of decision makers within the organization. Viewing the number and quality of decision makers, and the time required for decision making as inputs in the production of collective decisions, we show that there exist generic organizational forces that offset the incentive to unlimited expansion of the organization. Even in a long run competitive environment with perfect markets for managers, unlimited duplication of the firm may not be economically feasible. We first analyze in a general setting and then illustrate in two stylized examples, the interplay between individual decisional quality, time required for an individual decision, direct and indirect costs of decision making, and the optimal number of decision makers (for example, management size). Collapse"
101195,1990,public choice,"boards of trustees, agency problems, and university output",boards of trustees agency problems and university output,"Concluding commentsThe analysis and empirical evidence in this paper indicate that the structure of the boards of trustees of state universities influences the provision of higher education. The structure of the boards is important because it helps to define the constraints on the board members and on the internal agents of the universities. An implication of this study is that public universities can be made to function more like private ones by placing them under separate governing boards.These results are especially interesting when examining the trends regarding board structures. The trend over this century across the states has been toward increasing the number of universities under the jurisdiction of a single board. This analysis suggests that the trend is a response to political pressure from educators, not from taxpayers and consumers of higher education. Collapse"
101196,1990,public choice,"shirking, representation, and congressional behavior:  voting on the 1983 amendments to the social security act",shirking representation and congressional behavior voting on the 1983 amendments to the social security act,"SummaryOur central goals at the outset of the paper were three: (1) to report on the relative significance of a sophisticated measure of constituent economic interest and a commonly used variable, ADA score, that purports to measure the personal ideology of the candidate; (2) demonstrate that the constituent economic interest variable should be adjusted to account for the fact thatvoters, not citizens, are the only effective principals in influencing a legislator's voting activities; and (3) call into question, on both theoretical and empirical grounds, the claim that legislators shirk their responsibilities to voters by voting their own ideological preferences. In order to evaluate our efforts, consider Table 5. For a large majority (15) of the 18 relevant runs, the ideological variable is significant. Our measure of constituent economic interests does not eliminate the explanatory power of the ideological voting variable, but this does not indicate shirking. As opposed to shirking, we may observe ideological voting because (1) it provides brand name capital, (2) it represents the ideological preferences of the constituents, or (3) it acts as a measure of median voter economic preferences. Further, ADA scores do not allow us to differentiate between these competing explanations.Table 5. Comparison of House and Senate resultsEconomic variableIdeological variableHouseSenateTotalSignificantNot significantSignificantNot significantKalt-Zupan Insignificant——4—4Kalt-Zupan Significant332—8Peltzman Insignificant——3—3Peltzman Significan——3—3Total3312018 For 11 of the 18 models one of the economic variables accounts for a significant portion of the variance in the dependent variables. The results derived from our measure of constituent economic interests contradict most findings of the LASI school and raise questions about the validity of the empirical characterization of constituent interests in that research.A breakdown of the results by chamber indicates that significant differences in the degree of ideological voting between the House and Senate may exist. This is important in that most research has focused only on the Senate where ideological voting is more prevalent. For the House, Table 5 reveals the constituent economic interest variable isalways significant, and in fully one-half of the relevant regressions it is theonly significant variable, knocking ADA out of the race. As noted earlier, the insignificance of ADA is some indication of the absence of ideological shirking though its significance may indicate only measurement error, voter ideology, or reputational capital. In the Senate, the results are more evenly split, though it is clear that the adjusted (for reelection constituency) economic interest variable is an improvement. ADA is significant in all 12 Senate regressions, and the respective economic variables are significant in 5, or just under half.This side-by-side comparison is provocative, though it remains to be tested in detail. But our preliminary conclusions can be stated as follows. First, as Peltzman (1984) suggested, a better specification of economic interest and constituency representation reduces, though it does not eliminate, the role of the ADA variable in the Senate. Second, we find evidence that ideological shirking, if it exists, is much smaller in the House. In fact, from an institutional perspective, it can be argued that economic interests are dominant, since House districts are smaller and more homogeneous. Further, the shorter terms for House members may make them more directly accountable to voters, and smaller groups of voters may force a lesser reliance on pure ideological campaigning and require a more personal presentation of self. Collapse"
101197,1990,public choice,the simple analytics of slack-maximizing bureaucracy,the simple analytics of slack maximizing bureaucracy,No Result.
101198,1990,public choice,pressure politics and government spending in belgium,pressure politics and government spending in belgium,No Result.
101199,1990,public choice,economic performance and political popularity in the republic of ireland,economic performance and political popularity in the republic of ireland,"This paper develops a model of discrete choice to analyse the choice of voters among a number of political parties. It then applies the model to an empirical analysis of the relationship between a government's economic performance and its political popularity for the Republic of Ireland over the period 1974–1987.Within this general statement the paper makes three contributions. First, it sheds light on a hitherto unknown phenomenon — namely the nature of the relation between economic performance and political popularity in Ireland. Second, it does this within the context of analysing the reactions of different types of voters viz. voters of all social classes and then of social classes ABC1 and C2DE. Third, the empirical work is grounded firmly in a choice theoretic model involving optimal choices between discrete alternatives. Collapse"
101200,1990,public choice,a constitutional view of legislative pay,a constitutional view of legislative pay,"As an academic comparative constitutional lawyer, I come to the recent Constitutional Advisory Panel report and the issue of whether and how the New Zealand Bill of Rights Act 1990 (NZBORA) should be reformed from a particular – perhaps idiosyncratic – perspective. This is viewing the NZBORA as an influential version of a new general model of constitutionalism. This model grants to legislatures ultimate responsibility for the resolution of controversial rights issues while at the same time seeking to improve the rights sensitivity of the legislative process and increasing the rights protective powers of courts as compared with the traditional institutional form of parliamentary supremacy. At least in theory and aspiration, this general model provides an Collapse"
101201,1990,public choice,a brief empirical note on the tiebout hypothesis and state income tax policies,a brief empirical note on the tiebout hypothesis and state income tax policies,"Some years ago, Tiebout (1956: 418) hypothesized that ""... the consumervoter may be viewed as picking that community which best satisfies his preference pattern for public goods ... the consumer-voter moves to that community whose local government best satisfies his set of preferences."" Based upon this argument by Tiebout, this brief note seeks to empirically examine the impact of state income tax policy upon the geographic mobility of consumer-voters in the United States; more specifically, we attempt to determine whether the existence of a state income tax significantly influences the in-migration of consumer-voters. Naturally, the analysis includes suitable non-tax control variables to reflect other factors which may influence the migration decision calculus. Collapse"
101202,1990,public choice,protectionist policies as the regulation of international industry,protectionist policies as the regulation of international industry,No Result.
101204,1990,public choice,on the limits to rent-seeking waste,on the limits to rent seeking waste,No Result.
101205,1990,public choice,family ties and social security in a democracy,family ties and social security in a democracy,"Sabrina P. Ramet, ed. Gender Politics in the Western Balkans. Women and Society in Yugoslavia and the Yugoslav Successor States. University Park, PA: The Pennsylvania State University Press, 1999. 343 pp. $55.00, cloth. $18.95, paper. Gender Politics in the Western Balkans is a collection of fifteen articles on various aspects of gender politics in Yugoslavia and its successor states. Written by historians, social scientists, literary critics and human rights activists, the essays address both the socio-economic and the political issues that have affected women in the region. The book is divided into four uneven parts: Overview (two chapters); the Interwar Era, World War 11, and the Socialist Era (four chapters); Post-Socialist Republics (six chapters); and Literature and Religion (three chapters). The editor, Sabrina P. Ramet, reminds us in her introduction that ""in the course of the twentieth century, Yugoslav women have lived under six different systems: dynastic monarchy (in the years up to 1918), constitutional monarchy (most of the years between 1918 and 1941), fascist occupation (1941-45), communist one-party rule (1945-90), flawed democracy (Slovenia and Macedonia since 1990), and nationalism (Croatia, Serbia and Bosnia since 1990)"" (p. 5). The emergence, nature, disintegration, and the various mutations of the Yugoslav, Serbian, Croatian, Slovenian, Bosnian, and Kosovar nation states form the historical and socioeconomic background framing the other issues addressed in the collection. These include the South Slav family structure; underdevelopment, poverty and economic scarcity; women's struggles; changing constructions of identities based on gender, nationality/ethnicity, and sexual orientation; and the role of war and other crises in both the deconstruction of the existing gendered (and ethnic) order and the (re)construction of a new one. The South Slav family structure and dynamics, in particular the syndrome of the ""self-sacrificing micro-matriarchy,"" are introduced by Andrei Simic in Chapter Two. The author analyzes the power relations within the typical Yugoslav family from an anthropological perspective in terms of three dyads: husband/wife, mother/son, and daughter-in-law/mother-in-law. ""In the absence of an affectual tie to her husband, and as a reaction to the dominance of her mother-in-law,"" Simic argues, ""the young wife cultivates unusually strong reciprocal links with her children"" (p. 21). In time, the close relationship between an elderly mother and adult sons) enables the mother to exert considerable influence and power both within and outside the family. The author characterizes the resultant gender system as a ""cryptomatriarchy."" In her comment on Simic's article in the concluding ""Afterword,"" Branka Magas attributes the continuing influence on urban life of machismo and cryptomatriarchy to the acute housing shortage during the communist period, the relatively recent industrialization, and, above all, ""the low labor mobility caused by the life-long job security characteristic of all communist societies. An unintended outcome of this `socialist gain' was the fact that rules governing kinship in the countryside were extended to the cities-with the difference that, in the latter, matri-locality as well as the usual patri-locality became the norm. In its desire to prevent the rule of capital, the communist system thus conserved older norms of life"" (p. 289). Cryptomatriarchy has continued to flourish in contemporary Serbia, with paradoxical effects. Sarana Papi6 describes Milo""sevic's Serbia as a society characterized by ""the equality of powerlessness,"" where ""women are treated as an inexhaustible resource, not only in the everyday art of survival but as the procreative saviors of the dying Nation"" (p. 164), while men continue to be seen as ""natural"" warriors. However, Papi also points out that ""Milogevid's exclusive masculinized power emasculates all other men; economically and politically"" (p. … Collapse"
101206,1990,public choice,"bureaucracy, inefficiency, and time",bureaucracy inefficiency and time,"Consider General Motors Corporation. GM seems like a giant Sequoia tree rotting slowly from the top. Theories abound as to why: myopic management; hubris; politics, vertical integration, inefficiency, outdated plant and equipment, the Icarus Paradox (Miller 1990), resistance to change; permanently failing organizations (Meyer and Zucker 1989), the unions; and so forth. GM is a dinosaur (Loomis 1993) stuck in a time warp with a “gargantuan bureaucracy” (Kerwin 1998, p. 26) that, as a high cost producer of low quality cars, is well off the efficiency curve. And it is not that there isn’t motive. The industry is very competitive and everyone in the industry knows GM is below the curve. Further, GM has spent billions trying to get back on the curve—some say they have spent more than the total asset value of Toyota. What is not working at GM and can organization science explain it? Like Gaul, organization science is divided into three parts: rational, natural, and open systems (Scott 1998). The rational system view, the visible hand Chandler (1977) calls it, puts the blame on managers. The natural system view—the invisible hand—tells us that the emergent structure is apparently defeating whatever good ideas the managers do come up with. And the open systems view? It focuses on environmental effects and boundary transactions. Paradigm proliferation (Donaldson 1995) further delineates views within Scott’s broad framework. Much as one might like some of the newer paradigms, Pfeffer (1997) cautions that much of the paradigm proliferation in organization science results from fads and fashion. He quotes himself 16 years earlier, saying, “If we use relatively simpler processes and models the world will appear to be simpler and more certain.... We overlook the potential for finding simpler models to describe the world” (1981, p. 411). So, in this chapter I reduce Scott’s framework to four driving forces: adaptive tension, self-organization (by managers or nonmanagers), interdependency effects, and multilevel coevolution. Ironically Pfeffer (1997) decries the dangerous liaison with economics while simultaneously calling for simplicity. The one thing economists’ penchant for mathematical models has created is a constant drive for simplicity. They focus on just a few key variables otherwise the mathematics becomes intractable. Following the direction of current philosophy of science, embodied in Campbellian Realism (McKelvey this volume), I not only follow Pfeffer and the economists in emphasizing parsimony, but also take a step in the direction of a formal model-centered organization science by framing my complexity theory application to firms in terms of computational models.1 Campbellian Realism calls, in part, for scientists to coevolve the development of theory and model so as to maximize “experimental adequacy” tests—the theory predicts model behavior and the model allows testing of the intricacies of the theory. Section 2 develops: (1) Self-Organization Theory—If the level of adaptive tension falls outside a region defined by the chaos theorists’ “critical values” (Cramer 1993),2 the resulting complexity field will not support the emergence of structures necessary for constructive adaptation; and (2) Complexity Catastrophe Theory—If the conditions of complexity catastrophe exist, Friedman’s (1953) natural selection based constrained maximization or Campbell’s blind variation, selection, and retention (BVSR) processes may function properly and yet fail to produce the kinds of intrafirm behavior necessary for survival and growth in a selectionist competitive context. Together these theories state that (1) critical value effects creating emergent structure in the mid range of adaptive tension; and (2) complexity effects on the flat and jagged extremes of rugged landscapes combine to produce a nonlinear inverted U effect on organizational performance relative to adaptive tension and complexity. Section 3 illustrates how to set up the groundwork for testing experimental adequacy. The model frameworks come from Kauffman (1993). I draw on both his Boolean statistical mechanics and his NK[C] model. I conclude that (1) self-organization and complexity catastrophe theories offer useful insights into the prolonged poor performance of large complex organizations such as GM; and (2) computational modeling approaches offer a basis for testing the Collapse"
101207,1990,public choice,the optimal subsidization of baptists by bootleggers,the optimal subsidization of baptists by bootleggers,No Result.
101223,1990,quarterly journal of economics,consumer investment in product-specific capital: the monopoly case,consumer investment in product specific capital  the monopoly case,"For many pairs of complementary products, one good might usefully be called a consumption good, and the other a capital good. A concrete example of such complements is gasoline and automobiles. Because of complementarity, investment in automobiles depends on the price of gas. Since cars bought today will be around for several years, there is a dynamic link between the current price of gas and future demand for gas. OPEC has a strategic incentive to set a low oil price today since a low price promotes investment in big cars and other oil-fueled equipment which, in turn, increases the future profit of the oil industry. Consumer investment in product-specific capital is a feature of the markets for many products, especially if one takes a broad perspective of what this capital decision can be. For instance, continuing the example of the gas market, the decision to reside far from work is analogous to the decision to buy a big car since (1) the decision may be influenced by the current price of gas and (2) the decision affects an individual's future demand for gas. As another example consider the demand for phone service. In response to a low price of phone service, businesses makes capital decisions such as the purchase of computer telemarketing machines and other phone equipment. The businesses may also configure their marketing strategy to use (human) phone contact rather than direct personal contact, and such a strategy involves investment in human capital. These investment decisions all tend to make the future demand for phone service relatively inelastic. This paper presents a model in which there are capital goods, ""cars,"" which can be used for two periods, and a consumption good, ""gas,"" which fuels the cars. Big cars are gas guzzlers; i.e., car size and gas are complements. Consumers select the size of their car on the basis of the current price of gas and their expectations of the future Collapse"
101224,1990,quarterly journal of economics,export subsidies as an outcome of the management-labor conspiracy,export subsidies as an outcome of the management labor conspiracy,"A mercantilistic trade policy is often introduced as a tool to generate more jobs in some key industries. When employment in an export (import) industry is perceived to be low, the government would be tempted to adopt measures of export promotion (import protection) to increase it. By effectively guaranteeing employment, however, these policies would reduce incentives for the private sector to reach efficient labor contracts. For example, the labor union might refuse to sign a profit-sharing contract with management and unilaterally demand high wages, believing that the resulting loss in competitiveness would not affect the well-being of its members, since the government would protect the industry. It is also conceivable that management and the labor union conspire to reach inefficient contracts so as to invite government intervention. In certain cases trade policies only transfer income without any allocative effect. This is not merely a theoretical curiosum. Free trade advocates have often pointed out that protectionist policies would have deleterious effects on the pattern of collective bargaining in the trade-sensitive sectors.' This paper constructs simple models to illustrate this point for export subsidies, and so it can be viewed as an addition to the literature that aims to bring attention to the pitfalls of the new mercantilism, such as Eaton and Grossman [1986], Dixit and Grossman [1986], and Horstman and Markusen [1986]. Unlike these studies, however, the government policies considered here are reactive in the sense that the private sector moves first, anticipating the government's response. Dixit and Kyle [1985], Carmichael Collapse"
101225,1990,quarterly journal of economics,the information in the longer maturity term structure about future inflation,the information in the longer maturity term structure about future inflation,"This paper provides empirical evidence on the information in the term structure for longer maturities about both future inflation and the term structure of real interest rates. The evidence indicates that there is substantial information in the longer maturity term structure about futureinflation: the slope of the term structure does have a great deal of predictive power for future changes in inflation. On the other hand, at the longer maturities, the term structure of nominal interest rates contains very little information about the term structure of real interest rates. These results are strikingly different from those found for very short-term maturities, six months or less, in previous work. For maturities of six months or less, the term structure contains no information about the future path of inflation, but it does contain a great deal of information about the term structure of real interest rates. The evidence in this paper does indicate that, at longer maturities, the term structure of interest rates can be used to help assess future inflationary pressures: when the slope of the term structure steepens, it is an indication that the inflation rate will rise in the future and when the slope falls, it is an indication that the inflation rate will fall. However, we must still remain cautious about using the evidence presented here to advocate that the Federal Reserve should target on the term structure in conducting monetary policy. A change in Federal Reserve operating procedures which focuses on the term structure may well cause the relationship between the term structure and future inflation to shift, with the result that the term structure no longer remains an accurate guide to the path of future inflation. If this were to occur, Federal Reserve monetary policy could go far astray by focusing on the term structure of interest rates. Collapse"
101226,1990,rand journal of economics,the economic incidence of the interstate commerce act of 1887: a theoretical and empirical analysis of the short-haul pricing constraint,the economic incidence of the interstate commerce act of 1887  a theoretical and empirical analysis of the short haul pricing constraint,"This article concerns the economic incidence of the Interstate Commerce Act of 1887 (ICA). Our focus is the short-haul pricing constraint, a provision of the ICA that prohibited railroads from charging higher rates to isolated, primarily agrarian shippers than it charged to intercity shippers of similar commodities. Utilizing the event study methodology, we find that the impending passage of the ICA generated a distribution of abnormal returns to railroads and shipping firms that is consistent with the theoretical implications of our analysis of the short-haul pricing constraint (SHPC). However, early interpretations of the SHPC by the Interstate Commerce Commission reduced some of the abnormal returns to railroads in a manner that is inconsistent with the hypothesis that the short-haul pricing constraint was an important mechanism of early railroad regulation. The analysis does support a multiple-interest interpretation of the Interstate Commerce Act and has implications for the positive theory of regulation. Collapse"
101227,1990,rand journal of economics,firm behavior in franchise monopoly markets,firm behavior in franchise monopoly markets,"This article presents empirical evidence concerning the value of franchise bidding competition as a means of controlling natural monopoly behavior. The analysis focuses on the cable television industry, using data collected in a survey of local government officials in cabled communities throughout the continental United States. One important potential problem raised by critics of franchise bidding is the ability of franchise winners to engage in ex post opportunistic behavior by reneging on the promises that they made in order to win the franchise contract. The results of my analysis suggest that although franchise competition does lead firms to engage in some degree of opportunistic behavior, the extent of opportunism is not severe. Furthermore, reputation effects appear to play a role in constraining firm behavior, while rate regulation actually seems to exacerbate ex post behavioral problems. Collapse"
101231,1990,rand journal of economics,the performance of long-term contracts: further evidence from coal markets,the performance of long term contracts  further evidence from coal markets,This article extends my previous work on price adjustment in long-term(This abstract was borrowed from another version of this item.)
101237,1990,regional studies,graduation of fertility schedules:  an analysis of fertility patterns in london in the 1980s and an application to fertility forecasts,graduation of fertility schedules an analysis of fertility patterns in london in the 1980s and an application to fertility forecasts,"London's average total fertility rate (TFR) stood at 1.75. Using a cluster analysis to compare the 1985-1987 fertility patterns of different boroughs of London, demographers learned that 5 natural groupings occurred. 4 boroughs in a central London cluster have the distinction of having a low TFR (1.38) and late fertility (average age of 29.58 years). The researchers attributed these occurrences to the high levels of employment and career attachment and low rates of marriage among women in this cluster. 2 inner city boroughs constituted the smallest cluster and had the largest TFR (2.37), mainly due to high numbers of births to the ethnic minorities. The largest cluster consisted of 12 boroughs located mainly along the periphery with 2 centrally located boroughs (TFR, 1.79). Some of the upper class outer boroughs characterized another cluster with a TFR of 1.61. Another cluster made up of inner and outer boroughs in east and southeast London had a ample proportion of manual worker (TFR, 2.04). Social class most likely accounted for the contrast in TFRs between the 2 aformentioned clusters. Demographers observed that cyclical fluctuation of fertility occurred as opposed to secular trends. Due to these fluctuations, demographers used autoregressive moving average forecast models to time series of the fertility variables in London since 1952. They also applied structural time series models which included regression variables and the influence of cyclical and/or trend behavior. The results showed that large cohorts and the increase in female economic activity caused a delay in the modal age of births and a reduction in the number of births. Collapse"
101240,1990,regional studies,training as an instrument of regional policy:  the new panacea?,training as an instrument of regional policy the new panacea,No Result.
101241,1990,regional studies,the impact of the scottish enterprise reforms on vocational education and training,the impact of the scottish enterprise reforms on vocational education and training,No Result.
101245,1990,regional studies,piercing the corporate veil: the closure of wisconsin steel in south chicago,piercing the corporate veil the closure of wisconsin steel in south chicago,"CLARK G. L. (1990) Piercing the corporate veil: the closure of Wisconsin Steel in South Chicago, Reg. Studies 24, 405–420. Restructuring is usually associated with technological change, the transformation of the workplace, and the communities affected by cross-plant capacity rationalization. However restructuring should also be understood as a corporate strategy – specifically the actions of management to protect and foster their core economic interests while limiting the costs of adjustment by shifting those costs to others. A case study of International Harvester Company's restructuring strategy which led to bankruptcy and closure of Wisconsin Steel in South Chicago in the early 1980s is the basis of the paper. The significance of plant-closing pension liabilities in corporate restructuring is emphasized, as is the regulation of restructuring with respect to those liabilities by the United States federal government's Pension Benefit Guaranty Corporation. Analytically, the case is interpreted, in part, a... Collapse"
101246,1990,regional studies,regional financial sector models:  an application to the northern ireland financial sector,regional financial sector models an application to the northern ireland financial sector,"HUTCHINSON R. W. and McKILLOP D. G. (1990) Regional financial sector models: an application to the Northern Ireland financial sector, Reg. Studies 24, 421–431. This paper presents an analysis of recent theoretical developments in the treatment of regional financial markets. It starts from a position where regional financial markets are fully integrated in a national system, with regional financial institutions having a neutral effect on their regional economy. It then indicates how interregional differences in information costs can result in a degree of inter-regional financial market segmentation, creating conditions under which regional interest rates can diverge from national rates. The theoretical analysis also demonstrates how market segmentation, resulting from financial institutions with elements of regional autonomy, can impact upon a region's ability to undertake profitable investment. The applicability of these models is then tested employing the Northern Ireland financial sector as the control.... Collapse"
101249,1990,regional studies,the channel tunnel and regional development:  policy responses in britain and france,the channel tunnel and regional development policy responses in britain and france,No Result.
101254,1990,regional science and urban economics,panel data analysis of housing choices,panel data analysis of housing choices,"Abstract Housing choices have almost exclusively been analyzed either by using survey data gathered at a specific point in time or by using time-series of aggregate data. Panel data permits use of both time-series and cross-sectional variation, thereby providing a substantially superior identification and seperation of the various economic and demographic mechanisms underlying housing choice behavior which are often confounded in the snapshot analysis of cross-sectional data and by aggregation in time-series analysis. In this paper, a longitudinal discrete choice model of the choice of housing tenure and size is presented and estimated using five linked cross-sections of the Panel Survey of Income Dynamics, 1977 to 1981. We employ the conditional fixed effects multinomial logit model [FEMNL, Chamberlain (1980)] in order to account for time-invariant heterogeneity across households. These panel data results differ in many respects from results obtained from single and pooled cross-sections. First, price and income elasticities appear substantially overestimated in cross-sectional analysis as opposed to time-series and panel data analysis. Second, life-age effects are confounded by calendar-time specific effects and therefore may yield implausible results in cross-sectional analysis. In general, the influence of demographic variables appears to be understated in cross-sectional estimation. Collapse"
101274,1990,review of economic studies,repeated games with long-run and short-run players,repeated games with long run and short run players,"This paper studies the set of equilibrium payoffs in repeated games with long- and short-run players and little discounting. Because the short-run players are unconcerned about the future, each equilibrium outcome is constrained to lie on their static reaction (best-response) curves. The natural extension of the folk theorem to games of this sort would simply include this constraint in the definitions of the feasible payoffs and minmax values. In fact, this extension does obtain under the assumption that each player's choice of a mixed strategy for the stage game is publicly observable but, in contrast to standard repeated games, the set of equilibrium payoffs is different if players can observe only their opponents' realized actions. Collapse"
101286,1990,review of economics and statistics,do socialist countries suffer a common business cycle?,do socialist countries suffer a common business cycle,"The nature and characteristics of economic fluctuations in Eastern European Centrally Planned Economies are analyzed. When cycles are identified by the deviation from a fitted deterministic trend, they are seen to coincide temporally. This common variation is found for Net Material Product (NMP) and Investment. The implications of the common variation of CPE cycles are discussed. Possible explanations of this phenomenon are discussed with emphasis on a possible link via trade. We then examine the possibility that the time series contain unit roots. We are unable to reject this hypothesis for the variables in question. This suggests using procedures for detrending nonstationary time series suggested by Beveridge and Nelson. Such an analysis is performed and the implications are discussed. We find that there remains some common variation in the cyclical component of output, but to a lesser extent. The implications of these findings for future research on CPE cycles are discussed. Collapse"
101291,1990,review of economics and statistics,credit rationing and private transfers:  evidence from survey data,credit rationing and private transfers evidence from survey data,"This paper investigates the connection between credit rationing and private intergenerational transfers. The research is motivated by the idea that private transfers may be a source of funds for consumers who have difficulty borrowing from financial intermediaries. This idea has important implications for consumer behavior, and economists have begun to think about it, but they have given it little empirical attention. Using the 1983 Survey of Consumer Finances, we find that private transfers do tend to be targeted toward consumers who face credit rationing. But we also find that a substantial fraction of U.S. consumers are liquidity-constrained even if one allows for the possibility of private transfers. Collapse"
101294,1990,review of economics and statistics,specification of the technology for neoclassical investment theory:  testing the adjustment costs approach,specification of the technology for neoclassical investment theory testing the adjustment costs approach,"The paper provides an empirical investigation into the nature of adjustment costs and their implications for modelling and investment process. In particular, their role in today's most popular models of investment, the rational flexible accelerator and Tobin's q, is considered. In the analysis it is assumed that costs of adjusting the level of quasi-fixed inputs affects the firm's operations independently of specific optimal decision rules for investment. The results show that the pattern of adjustment costs is consistent with the solution of the optimization problem faced by the firm. However, such structure is more complex than what is usually postulated in the literature. Finally costs of adjustment represent a significant portion of the unit cost of new capital goods. Collapse"
101298,1990,review of economics and statistics,a monte carlo evaluation of the box-cox difference transformation,a monte carlo evaluation of the box cox difference transformation,The Box-Cox difference transformation permits the selection of either the first difference or percentage change form of a time series regression model. Monte Carlo evidence on the small sample properties of the transformation parameter A indicates that the difference transformation works quite well even in samples of size 30. Likelihood ratio testing is compared to an asymptotically equivalent alternative Lagrange Multiplier test. It is shown that values of R2 can often be higher for the incorrect transformation. Collapse
101299,1990,review of economics and statistics,estimation of a linear sur model with unequal numbers of observations,estimation of a linear sur model with unequal numbers of observations,This paper clarifies the differences in the alternative estimators of the error covariance matrix X in the SUR model with unequal numbers of observations. It identifies a sample statistic which represents their essential differences and hence is a useful guide for the choice of an estimator in practice. The paper also presents an alternative estimator of E based on the specification in Telser (1964). Collapse
101300,1990,review of economics and statistics,"a simple, consistent estimator for disturbance components in financial models",a simple consistent estimator for disturbance components in financial models,"Many recent papers have estimated components of the disturbance term in the ""market model"" of equity returns. In particular, several studies of regulatory changes and other policy events have decomposed the event effects in order to allow for heterogeneity across firms. In this paper we demonstrate that the econometric method applied in some papers yields biased and inconsistent estimates of the model parameters. We demonstrate the consistency of a simple and easily-implemented alternative method. Collapse"
101301,1990,review of economics and statistics,alternative methods of depreciation and the reliability of accounting measures of economic profits,alternative methods of depreciation and the reliability of accounting measures of economic profits,"Most U.S. firms use straight-line depreciation; some use accelerated depreciation. A series of Monte Carlo experiments were conducted to show how a proposed switch to annuity depreciation would affect accounting estimates of two measures of economic profitability: the rate of return and q, the ratio of a firm's market value to its replacement cost. Annuity depreciation significantly improved the accounting rate of return but had little effect on estimates of q. Some of the improvement in the rate of return and almost all the improvement in q could be obtained if all firms used straightline depreciation. Collapse"
101302,1990,review of economics and statistics,durable asset depreciation:  a reconciliation between hypotheses,durable asset depreciation a reconciliation between hypotheses,"Previous analyses of depreciation have resulted in conflicting claims and evidence concerning the depreciation patterns of durable assets. We demonstrate that heterogeneity in asset quality and usage rates can reconcile disparate results. Specifically, we add both usage and care to the flexible estimation model of Hulten and Wykoff (1981a). By accounting for systematic changes in these additional dimensions and controlling for sample bias and scrappage value, a near-geometric pattern emerges which can otherwise be obscured. The impact of usage (as distinct from age) has implications for the measurement of cost (and hence productivity) and for modeling replacement decisions. Studies measuring the total capital stock and its implied service flows are numerous in the economic literature (Jorgenson, 1971). An accurate measurement of the aggregate capital stock is often essential in analyses of investment. consumption. and productivity at the firm, regional and national levels. But measurement of the stock is difficult, owing to heterogeneity of both the capital stock and how it is used. Also critical is measuring how the capital depreciates as it ages. Despite its importance and the amount of research conducted on depreciation, no clear consensus exists about the depreciation patterns followed by different types of capital goods. Jorgenson and his associates have advocated the use of a geometric depreciation pattern in studies of investment behavior. In addition, several analyses of market data for used capital assets appear to support the geometric pattern, which implies a constant depreciation rate (see Hulten and Wykoff (1981a, b); Wykoff (1989); Hall (1971)). A number of articles have been written challenging the constant rate assumption as both implausible and misleading. Feldstein and Rothschild (1974), for example, point to the unrealistic requirements for asset decay and replacement that are necessary for a geometric pattern. Penson, Hughes, and Nelson (1977) suggest that the rapid decline in productive capacity immediately after purchase is inconsistent with the physical capacity of individual assets. These authors and others (e.g., Taubman and Rasche, 1969) have Received for publication August 2, 1988. Revision accepted for publication August 23, 1989. * Oregon State University and Laurits R. Christensen Associates, respectively. Special thanks go to Moshe Kim, Frank Wykoff, Wes Musser, Michael Kuehlwein and two anonymous reviewers for helpful comments on earlier versions of this paper and to Bette Bamford for typing assistance. Oregon Agricultural Experiment Station Technical Paper No. 9009. Collapse"
101303,1990,review of economics and statistics,on some sample path properties of intra-day futures prices,on some sample path properties of intra day futures prices,"This paper develops a time-series model for continuous time asset prices and then uses tick-by-tick data from Treasury bill futures to develop both a definition and test for efficiency in the continuous time case. The results suggest that intra-day data on futures prices do not behave like a Markov Renewal process; rather, lagged values of futures prices do have some predictive power. In addition, trading times are not useful in predicting futures prices. Finally, we estimate the bid-ask spread and show that even after adjusting for this spread, the serial dependence between current and lagged returns remains. The multitude of studies concerning efficiency in futures markets support the proposition that a Martingale approximation is reasonable for most commodity and capital asset markets, while the same data reject Gaussian processes as an appropriate model.1 All of these studies, however, use either close to close prices or open to open prices in the estimation process. The choice of daily data is arbitrary; a natural question concerns whether, based on continuous time data, futures prices can be shown to be realizations of continuous time Markov processes and whether they can be represented as stochastically linear processes. In this paper, we use intra-day, tick-by-tick, data on Treasury bills futures and develop both a definition and a test for efficiency in the continuous time case. Observations on continuous time prices yield two separate time series: the trading prices and times. As a result, we claim that the standard procedures for tests of market efficiency must be replaced by two separate necessary conditions; the first is the usual condition that successive price changes are independent; the second requires that the recurrence times between trades obey a Poisson process. We apply the above definitions to intra-day futures prices for Treasury bills for a 57 day period in 1983. The results indicate that the Markov model does not hold for intra-day futures prices but the trading times do seem to behave approximately as a Poisson process. Received for publication January 29, 1988. Revision accepted for publication July 19, 1989. * City University of New York and State University of New York at Stony Brook, respectively. We gratefully acknowledge financial support for this research from the Center for the Study of Futures Markets at Columbia University. Stephanie Dieringer provided invaluable help as a research assistant for this project. 1 Several studies have examined the martingale property. For a summary of these results see, for example, Kamara (1982). 2 See, for example, Fama (1965), Mandlebrot (1963), Stevenson and Bear (1970), and Neftci and Policano (1984). Some studies that do analyze intraday data include Feinstone (1985) and Hinich and Patterson (1985). Collapse"
101304,1990,review of economics and statistics,worker responses to occupational risk of cancer,worker responses to occupational risk of cancer,"Worker Responses to Occupational Risk of Cancer Author(s): James C. Robinson Source: The Review of Economics and Statistics, Vol. 72, No. 3 (Aug., 1990), pp. 536-541 Published by: The MIT Press Stable URL: http://www.jstor.org/stable/2109365 Accessed: 26-05-2016 22:31 UTC Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at http://about.jstor.org/terms JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to The Review of Economics and Statistics This content downloaded from 169.229.32.36 on Thu, 26 May 2016 22:31:45 UTC All use subject to http://about.jstor.org/terms Collapse"
101306,1990,review of economics and statistics,a test of executive behavior in the local public sector,a test of executive behavior in the local public sector,"This paper provides an empirical framework to test three competing theories regarding the behavior of public executives in the local public sector. These alternative theo- ries are the public interest, median-voter and expense preference (or surplus-maximizing Leviathan) models. This study tests these various theories by determining whether aggregate property values, the median-value of housing or administra- tive expenditures is a more significant determinant of the salary of a representative school executive in Connecticut. The empirical results lend some support for the expense preference model (Leviathan model) of public executive be- havior. Collapse"
101339,1990,scandinavian journal of economics,economies of scale and scope in thrift institutions: the case of finnish cooperative and savings banks,economies of scale and scope in thrift institutions the case of finnish cooperative and savings banks,"Accounting data on operating costs and outputs for cooperative and savings banks in 1983 and 1984 are used to provide empirical evidence on the cost structure of thrift institutions in Finland. A modified translog cost model is fitted to the data. Economies of scale and scope are estimated based on the coefficients of the cost model. Using advances and bills as outputs, the results indicate that cost curves for both cooperative banks and savings banks generally tend to be L-shaped at the plant level and U-shaped at the firm level. Scope diseconomies are found in the joint production of advances and bills. This evidenceindicates that natural monopoly conditions are not present in the Finnish thrift industry. Collapse"
101343,1990,scandinavian journal of economics,the benefit rule for a pure public good in the presence of a social security program,the benefit rule for a pure public good in the presence of a social security program,"public good whereby the sum of the marginal rates of substitution between the public good and some numeraire private good for all agents who benefit frqm the public good must be equal to the corresponding marginal rate of transformation, i.e., XMRS = MRT. This rule can be implemented in a decentralized fashion if the government has enough financing instruments available, e.g., person-specific lump sum taxes and government debt. However, whether or not the first-best benefit rule can be implemented when the government only has control over relative prices is an open issue. Pigou (1947), Diamond and Mirrlees (1971), Dasgupta and Stiglitz (1971), Atkinson and Stern (1974), Pestieau (1974), and more recently, Wildasin (1979, 1984, 1985), King (1986), and Batina (1987) have considered the effect of decentralizing the government's policy on the optimal first-best benefit rule for a public good. It is generally true that if the government cannot completely control the economy because of a lack of policy instruments, then the first-best benefit rule governing the provision of the public good must be modified as a result. Collapse"
101345,1990,scandinavian journal of economics,"tax rates, government expenditures and labor supply: clarifying the record",tax rates government expenditures and labor supply clarifying the record,"A central claim of ""supply-side"" economics is that in general equilibrium, the income effect of a change in the income tax rate washes out and only the substitution effect remains. In the words of Ehrenberg and Smith (1985): ""There would be a substitution effect with no overall income effect, and labor supply would increase. The aggregate supply-of-labor curve, then, must be upward sloping."" [p. 160, emphasis in the original.] Although there has been a substantial literature on this subject, the literature is sometimes confused. In this paper, a simple model is used to sort through the literature, correct mistaken assertions, derive a new result and collect earlier results in a compact way. Collapse"
101472,1990,urban studies,the wage effects of residential location and commuting constraints on employed married women,the wage effects of residential location and commuting constraints on employed married women,"It has been argued that greater spatial constraints are imposed on the job searches of women workers and that these greater constraints account for some of the gender wage gap. All researchers agree that women commute shorter distances to work than men. In addition, some researchers have argued from indirect evidence that two-earner households give greater weight to husbands' job opportunities when choosing a residential location. In this paper, we use data on two-earner households from the 1980 Public Use Microdata Sample (PUMS) of the US Census for the Detroit and Philadelphia SMSAs to quantify the effects of residential location and of gender differences in commuting behaviour on the gender gap in wages. First, we find that within white households wives encounter relatively greater spatial variation in wages than their husbands but that there is less of a gender difference among black households. Second, we analyse the simultaneous effects of commuting and residential constraints on wages. We find for both cities, and for both blacks and whites, that the gender wage gap is not changed in any significant way by altering women's intrametropolitan residential and job locations. Collapse"
101473,1990,urban studies,poverty as a constraint to citizen participation in urban redevelopment in developing countries:  a case study,poverty as a constraint to citizen participation in urban redevelopment in developing countries a case study,"Urban redevelopment and infrastructural rehabilitation have been suggested as the solutions to the rapidly declining urban centres in developing countries. In order to qualify for external loans and aid for such redevelopment efforts and to ensure the replicability of such projects, the principles of cost-recovery and cost effectiveness have been advocated. Many World Bank assissted urban development projects have been implemented, based on these principles. Unfortunately, these principles have become difficult or perhaps impossible to achieve in most of the past urban development ventures. Our hypothesis is that the abject poverty of the people and a wrong assessment of what the beneficiaries can afford may be the most important factors which make the principles unrealisable. The case study from Ibadan, Nigeria, reported in this article, confirms this hypothesis. The paper explores the capacity and the willingness of people to pay for urban improvement programmes. The study reveals that poverty is a major constraint to urban redevelopment in Nigeria. The people are found to be willing and able to contribute more in kind than in cash to urban redevelopment efforts. The paper identifies and discusses the implications of the findings for urban redevelopment in developing countries. Collapse"
101477,1990,urban studies,development in mid-wales:  a comment,development in mid wales   a comment,"The discovery of new vertebrate species in developed countries is still occurring at surprising rates for some taxonomic groups, especially the amphibians and reptiles. While this most often occurs in under-explored areas, it occasionally still happens in well-inhabited regions. We report such a case with the discovery and description of U. mahonyi sp. nov., a new species of frog from a highly populated region of New South Wales, Australia. We provide details of its morphology, calls, embryos and tadpoles, and phylogenetic relationships to other species of eastern Uperoleia. We also provide the results of targeted surveys to establish its distribution and provide observations of its habitat associations. As a consequence of these surveys, we comment on the likely restricted nature of the species' distribution and habitat, and place this in the context of a preliminary assessment of its putative conservation status, which should be assessed for listing under the IUCN's red list. We note this species, which is morphologically distinct, has gone unnoticed for many decades despite numerous ecological surveys for local development applications. Collapse"
101478,1990,urban studies,mid-wales:  choosing the right perspectives,mid wales   choosing the right perspectives,"The primary objective of Mid-Wales Development (MWD) is to halt the outward net flow of population. The 1981 census revealed an upward turn in population, and MWD aims to continue this upward trend and to encourage young people to remain in the area (Mid-Wales Development, 1986) . Recent developments in the economy of Mid-Wales to fulfil this objective are eloquently described by Day and Hedger (1990), who also quintessentially advocate the case for future development to continue the process . It is important to establish how the analysis of the performance of the MidWales economy should be advanced, and how MWD's contribution should be assessed. A broad view of performance is suggested by Day and Hedger (1990), covering such items as job creation, stability of employment, type of employment, quality of training, level of skills and reduction in unemployment, as well as the distribution of development throughout the settlement pattern and the promotion of cultural identity and national and international reputation in the Welsh heartland. Of course development can be defined as embracing all of these items, and should be regarded as more than just job creation. However, the study by Willis et al. (1986), reported in Willis and Saunders (1988), restricted the analysis of the impact of MWD's advance factory programme, 1981-85, to that on employment, primarily because this was the brief given by MWD for the study! It should not be suggested that employment is the only development benefit provided by MWD, but only that it may be one of the easier items to measure! The difficulty in quantifying other development items is reflected in Day and Hedger's arguments about what they perceive to be serious shortcomings in MWD's operations . But leaving aside judgemental issues on the process by which Mid-Wales should develop and the strategy MWD should adopt, e.g. growth centres versus a wider spread of development, manufacturing versus service-based employment, concentration of administrative functions versus dispersion, employment for in-migrants versus local population, etc ., three particular methodological issues in the paper by Day and Hedger are worthy of discussion : Collapse"
101481,1990,urban studies,"categorical, multivariate analysis of intra-urban migration",categorical  multivariate analysis of intra urban migration,"A two-step paradigm is employed to analyse intra-urban migration patterns within Israeli towns. In step 1 of the paradigm, census-derived predictor macro-variables descriptive of towns are classified according to the intersection of two nominal dimensions. The three levels of dimension 1 correspond to the three factors emergent from a factor analysis of the predictor variables. These are labelled as representing either economic level, demographic composition or social features of the towns. Dimension 2 was a priori defined as being dichotomous, as it represented each variable's enhancing (a positive β-coefficient) or inhibiting (a negative β-coefficient) effect on intra-urban migration rates. Six cells result from the intersection of the two dimensions. Multivariate regression analysis is carried out within each level of the two dimensions. Regression analysis within the inhibitor level of dimension 1 reveals that four variables inhibited urban residential change: home-ownership, relatively high income, religiosity level of the town and median age of the town's population. These variables are distributed among the three levels of dimension 2. These results partially parallel findings concerning inmigration to Israeli towns that economic variables are crucial to locational decisions. However, with respect to intra-urban moves, economic considerations are less salient. Implications concerning basic urban policy decisions are discussed. Collapse"
101505,1990,world development,the structural adjustment of politics in africa,the structural adjustment of politics in africa,"Recurring civil conflicts in Africa demonstrate reference behaviors that may be explained through integrative causal mechanisms and dynamic feedback processes between peacekeeping, aid, and development interventions at the nexus between security and development. In previous work, I have demonstrated a theoretically grounded approach for combining individual agency and system-level dynamics at the nexus of security-development policy domains for evaluating impact of interventions on resiliency of various actors in instances of recurring armed civil conflict. My theoretical model incorporates individual agency with system dynamics to operationalize a resiliency framework for policy analysis of interventions by regional and international actors. This model has been further developed and tested in an in-depth case study of conflict in Somalia from 1990 to the present day, based on field work in East Africa with government representatives, soldiers, aid workers, and development specialists in six different countries. Important new feedback loops discovered through field interviews help to explain recurring conflict and the sensitivity of combatant as well as societal resiliency to different vectors for implementing intervention strategies. These insights are extrapolated to hypothesize what leads to the three different representative system behaviors for civil conflict, and how data from recent cases in Africa map to these behaviors. Persistent armed intrastate civil conflicts in the developing world present threats to global security interests that raise difficult policy questions of when and how third party interventions achieve their objectives -­‐-­‐ considering normative, material, economic, and political factors. In recent years, scholarly research has improved understanding of the macro-­‐level conditions under which political instability is likely to break out, the dynamics of conflict escalation due to repression and instrumental violence, and the factors that impact conflict duration and termination. However, the ability to accurately predict where and when political instability will erupt into violent civil conflict, and what policies will be most effective to prevent conflict, are elusive goals of both academic and policy communities. In recent years, more than 200 independent variables have been quantitatively explored in the literature using cross-­‐country comparative analyses to improve understanding of the conditions that pose the highest risk of political instability and armed civil conflict. There is some degree of consensus on the significance of fewer than thirty of these variables, and a high degree of consensus on no more than seven (Dixon, 2009; Sambanis, 2002). Discrepancies and inconsistencies around contested variables are most commonly attributed to different theoretical frameworks, data limitations, lack of methods for exploring complex interaction effects between variables, different methods used to operationalize measurements, and scaling effects (Buhaug & Lujala, 2005; Collier & Hoeffler, 2001; Dixon, 2009; Sambanis, 2002). At the country level, highest risk of conflict and instability is generally agreed to be strongly and positively correlated to conditions of poverty coupled with a large population (but not population growth or density), economic contraction, weak government institutions and infrastructures (especially in anocracies or partial democracies), heavy reliance of the export sector on primary commodities, political change, and a recent history of armed conflict (Collier & Hoeffler, 2004, 2005; Collier, Hoeffler, & Sambanis, 2005; Collier & Sambanis, 2005; Dixon, 2009; Fearon, 2005; Gates, Hegre, Jones, & Strand, 2006; Goldstone et al., 2010; Ross, 2006). As a result, humanitarian aid and economic development are often pursued as part of conflict prevention strategies. In a world of shrinking resources, however, the linkage between security and development is an ongoing area of research. While some general, common themes have been developed, consensus around causal mechanisms and policy solutions is lacking (Tschirgi, Lund, & Mancini, 2010). Three types of connections between security and development dominate the literature: security as an objective of development, security as an instrument in achieving development goals, and development as an instrument for achieving security goals (Stewart, 2004). Broad conclusions linking thematic and case studies suggest that these connections cannot be considered independenlty of one another (Paul Collier, 2003; Tschirgi et al., 2010): 1. Structural development factors invariably introduce risks of intrastate conflict -­‐ although the patterns are different depending on context. 2. At country level, political uncertainty and instability emerge as causes rather than consequences of development failures and insecurity (and therefore provide a key to their remedy). There is a security-­‐politics-­‐development nexus that is highly context specific. 3. External factors, both regional and international, have such influence that country level factors alone cannot explain conflict and development nor provide solutions . The causal loop diagram shown in Figure 1, based on on the Collier-­‐Hoffman model of the so-­‐called “Conflict Trap”, illustrates the interdependencies between these broad conclusions. In addition to the need for better understanding of these system dynamics, there is a gap in understanding conflict dynamics at the micro-­‐level, and how those dynamics interact with interventions at the sub-­‐national level to effect macro-­‐level conflict dynamics. The lack of understanding is evidenced by two disturbing trends regarding civil conflict: (1) in spite of a substantial decline in the number of armed civil conflicts over the past two decades and increasing levels of external intervention, today’s Figure 1 Causal loop diagram of key factors causing conflict in the Collier-­‐Hoeffler model civil conflicts last longer than in the past and result in higher deficits in human security;1 and (2) today’s civil conflicts are increasingly likely to re-­‐occur after wars stop -­‐ with around half of civil wars being due to post-­‐conflict relapses. Present-­‐day examples can be found across multiple continents and diverse geo-­‐political and conflict settings that include Myanmar (Burma), India, Pakistan, Thailand, the Philippines, Iraq, Afghanistan, Mali, South Sudan, Yemen, Central African Republic, the DRC, Nigeria, and Somalia. Data on violent events/year in Africa from 1997-2014 seems to indicate that the thematic conclusions linking security and development are valid. Moreover, the data suggests that one of four reference behaviors characteristic of system dynamics describe most of the patterns seen in 1 Human security concerns the protection and wellbeing of individuals and communities, in contrast to more 2 Data is from the Armed Conflict Location and Event database (Raleigh, Linke, Hegre, & Karlsen, 2010) the countries experiencing significant armed conflict during that time period. Reference Behavior 1, Overshoot and Collapse, is represented by the conflict in Sierra Leone, where a steep rise in violent events/year peaks over a period of 1-3 years, and is followed by logarithmically decreasing count over a period of at least ten years (Figure 2). Other countries in this category during this time period are Angola, Burundi, Uganda, and Rwanda. Reference Behavior 2, Adaptive Resilience-D, is represented by the conflict in Liberia, where the number of violent events oscillates about a relatively flat or slightly declining mean with a time period of about 15 years (Figure 3). Another country in this category during this time period is Chad. Collapse"
101517,1990,world development,some economic determinants of third world professional immigration to the united states: 1972-87,some economic determinants of third world professional immigration to the united states  1972 87,"Abstract We estimate reduced form equations for immigration to the United States by engineers, natural and social scientists, and physicians from 18 Third World countries. Explanatory variables include income, real GDP growth, graduation in the United States, and study in each country of origin. Additional explanatory variables are foreign student enrollment in the United States, lagged immigration, total immigration from each country, and a dummy variable which accounts for implementation of restrictions on permanent visas in 1972–1973. Results generally conform to expectations, but differ across occupations and with model specification. Most income statistics are unexpected, however, and the strong correlation of US income with graduation in the United States suggests that there is a simultaneity problem in models which include income. Because immigration appears to respond to labor shortages in the United States and surpluses in countries of origin, our results suggest that US immigration quotas have not prevented Third World professionals from responding to economic incentives. Collapse"
101522,1990,world development,after the decision: implementing policy reforms in developing countries,after the decision implementing policy reforms in developing countries,"Abstract Implicit in many reform proposals is a model of the policy process that is roughly linear: a proposed reform gets on the agenda for government action, a decision is made on the proposal, and the new policy or institutional arangement is implemented, either successfully or unsuccessfully. This article presents an alternative, interactive model of implementation that focuses on the conflict and reactions that are evoked by efforts to bring about changed policy or institutional contexts for development, and the resources that policy makers and managers are likely to require to sustain a reform in the face of such reactions. Central to the analysis is the assertion that characteristics of the reform being implemented will largely determine the kind of conflict it engenders, where such reaction is likely to become manifest, and what resources are needed for sustainability. The analysis suggests a framework for the strategic management of reform initiatives. Collapse"
101561,1990,american economic review,beneficial concentration,beneficial concentration,"We determined whether the ability of sour orange seedlings to withstand saline irrigation water could be improved by the addition of calcium to the water. Sour orange (Citrus aurantium L.) seedlings were treated for 4 months with a nutrient solution containing either no NaCl, 40 mM NaCI, or 40 mM NaCl plus various concen- trations of CaSO 4, CaCl 2, or KCl. After 4 months, the NaCl alone reduced root and shoot dry weights by ≈ 30% with no leaf necrosis. Addition of 1, 5, or 7.5 m M CaSO4 to solutions containing 40 mM NaCl significantly inhibited the NaCl-induced reductions in shoot dry weight. Addition of 7.5 mM CaCl2 or 7 mM KCl to the NaCl solution reduced leaf Na, but increased Cl to the toxicity level; hence, growth was not improved. The beneficial effect of CaSO4 was mainly attributed to a reduction in the accumulation of Na and Cl below the toxicity level in the leaves (0.4% and 0.5%, respectively) without a major increase in total dissolved salts. This study demonstrated that the beneficial effect of adding Ca depended on the anion associated with the Ca salt. Calcium sulfate, but not CaCl2, was able to overcome the damaging effect of NaCl to sour orange seedlings. Collapse"
101562,1990,american economic review,horizontal mergers:  the 50-percent benchmark,horizontal mergers   the 50 percent benchmark,No Result.
101563,1990,american economic review,input market price discrimination and the choice of technology,input market price discrimination and the choice of technology,"Recent concerns over the effects of the Robinson-Patman Act' and so-called priceprotection policies such as most-favoredcustomer clauses (MFC's)2 on market performance have given economists new reasons to examine the welfare effects of third-degree price discrimination. In order to assess these effects correctly, it is imperative that one understand how price discrimination influences market behavior. Joan Robinson's (1933) work launched the formal inquiry into the welfare effects of third-degree price discrimination. Building on the intuition presented by Arthur Pigou (1932), she showed that, if a monopolist faces two independent linear demand curves, the use of price discrimination will not affect industry output but will reduce welfare. Richard Schmalensee (1981) extends these results to nonlinear demand curves and shows that an increase in total industry output is a necessary condition for price discrimination to be welfare improving. Hal Varian (1985) broadens these results by deriving upper and lower bounds on the welfare change due to the use of price discrimination. He shows that these results can be applied to markets in which there are nonzero cross price effects. All of this work examines how the ability of a monopolist to price-discriminate will affect the market outcome when all other characteristics of the market are treated as exogenous. Recently, two lines of research have extended this inquiry beyond the case of a monopolist in a market with exogenously fixed parameters. The first line considers the case of oligopoly. The work of Charles Holt and David Scheffman (1985) and Thomas Cooper (1986) has shown that restrictions on price discrimination imposed by the use of MFC's can facilitate collusion between oligopolists attempting to restrict output. This implies that third-degree price discrimination can be welfare-improving. The second line of research shows that price discrimination by a firm can affect nonprice decisions made by other market participants, thus affecting the market outcome. Michael Katz (1987) presents a model in which a large firm's ability to vertically integrate backward into the production of an input allows it to obtain a lower per-unit price from the supplier of the input than can be obtained by smaller firms without this ability. He shows that third-degree price discrimination reduces welfare unless it prevents inefficient backward integration. DeGraba (1987) shows that the use or nonuse of price discrimination by a national firm can affect nonprice decisions made by local firms that compete with the national firm. In this situation, third-degree price discrimination is welfare-reducing, because it induces local firms to produce a product that is ""overly differentiated"" from the product of the national firm. In all of the work cited above, price discrimination is important when sellers set prices in separate markets or charge different prices to different customers in the same market. The following analysis (which can be considered a contribution to the second line of research) suggests that price discrimination can be important even when a seller faces a single market in which all customers are identical. The intuition behind this result is that nonprice decisions made by downstream producers (such as the choice of technology) can be affected by the use or *Johnson Graduate School of Management, Cornell University, Ithaca, NY 14853. I thank Robert Frank, Robert Smiley, Richard Thaler, and the participants of the JGSM applied microeconomics workshop for their helpful comments. 'See William Baldwin (1987 pp. 438-40) for a good summary of the debate. 2See John Kwoka and Lawrence White (1989 pp. 196-7). Collapse"
101566,1990,american economic review,the measurement of international trade related to multinational companies,the measurement of international trade related to multinational companies,ERR
101568,1990,american economic review,auction institutional design:  theory and behavior of simultaneous multiple-unit generalizations of the dutch and english auctions,auction institutional design   theory and behavior of simultaneous multiple unit generalizations of the dutch and english auctions,"Historically, English and Dutch auctions have been used for the exchange of single objects such as works of art or single lots of a good such as produce, fish, or cut flowers. Where these institutions have been used for the exchange of multiple units, such as the Australian wool auction (using English rules), successive lots of the good are sometimes sold sequentially at auction. In some, but not all, instances this is because the goods are not identical, even though the various lots may be close substitutes (see Penny Burns, 1985). Where the goods are accepted universally as being homogeneous, as in the securities markets, multiple units are often commonly auctioned simultaneously. In the securities industry, orders are batched for simultaneous execution in multiple-unit auctions in what are referred to as ""call markets""; that is, the security is ""called"" for auction at a particular point in time. This type of market is used on the stock exchanges of Austria, Belgium, France, Germany, and Israel. Some of these are verbal, and some are sealed bid auctions. Although the U.S. organized exchanges are predominantly continuous rather than call markets (except that call markets are used each day to open trading in each listed security), there is a growing number of exceptions such as the proliferation, since 1984, of Auction Preferred Stock (Goldman, Sachs and Co., October 1984) and Money Market Preferred Stock (Lehman Brothers, July 1984). We now have Dutch Auction Rate Transferable Securities, called DARTS, Stated Rate Auction Preferred Stock, or STRAPS, and many more. After the initial subscription offering of this type of security, the market is called every 49 days to reset the preferred dividend rate using a multiple-unit auction. The exchange of shares and the dividend determination is based on the array of stated dividend rates at which existing holders and potential new holders are willing to sell and/or buy corresponding quantities. The dividend rate and exchange of shares every 49 days is executed using the uniform price or competitive sealed bid mechanism (Vernon L. Smith et al., 1980). The discussion to follow will be confined to this sealed bid form of the call market. Call markets provide temporal consolidation of trade orders or other forms of expressing the desire to buy and sell. By comparison with continuous trading, call markets offer both advantages and disadvantages (Robert A. Schwartz, 1988 pp. 442-6). The cited advantages include low cost of operating the exchange; information aggregation and presumed pricing efficiency; price stability; individual trades, which are thought to have a small impact on price; reduced price uncertainty; and, finally, nondiscriminatory pricing. However, there are offsetting disadvantages: (1) the market is inaccessible except at the time of call; (2) no bid, offer, contract, or price information is available until the results of the call are announced; and (3) there is transaction uncertainty because a submitted bid (offer) may be too low (high) to execute inside the supply-demand cross. These conditions are only partially alleviated if there is a secondary market between calls. These disadvantages may be significant. In September 1988, the Wall Street Journal published an article on the failure of a call market for the auction rate preferred stock *Economic Science Laboratory, University of Arizona, Tucson, Arizona. This material is based upon work supported by the National Science Foundation under grant no. SES-8320121. Collapse"
101652,1990,annals of regional science,"city size, prices, and efficiency for individual goods and services",city size prices and efficiency for individual goods and services,"Studies indicate that the cost of living either is the same or rises slightly with the size of cities. This suggests — though it does not imply — that prices of goods and services are as high or higher in larger cities than in smaller cities. But is this really so? And if so, does this imply that costs of distribution (and of the production of services) are higher in bigger cities?The relationship of prices to the sizes of cities is examined for 193 individual goods and services and for 23 broad groups. Price alone does not indicate efficiency. It is well-established that wages for the same work at given skill levels are higher in larger cities. Therefore, the total real inputs may be lower in a larger city, even if price is higher, which indicates better efficiency in larger cities. Study of individual goods and services, and of groupings of them, enables us to go beyond prices to learn which categories are offered more efficiently and which less efficiently in larger cities. We find that for most goods, economic cost decreases rather than increases with city size. Collapse"
101659,1990,annals of regional science,context specific media choice and barriers to communication in universities,context specific media choice and barriers to communication in universities,"Despite the research conducted in recent years in the field of information and communication economics, there is relatively little understanding of the impact of new electronic media on communication behaviour. The paper presents a methodology and empirical results on communication behaviour in a university setting. A general framework for communication behaviour is developed where (tele)communication media choice plays an important role. The media choice component of the conceptual framework is analysed in more detail. Special attention is given to the possible existence of barriers to communication. Testing of the media choice segment of the conceptual framework is being achieved by means of the stated preference approach using experimental design theory. The target population is composed of all academics associated with an Austrian university. The survey population is restricted to the academics associated with the University of Vienna, the Technical University of Vienna and the Vienna University of Economics and Business Administration. The sample design used relies on exogenous stratification. Empirical results are presented using multinomial logit models for a series of communication contexts. Collapse"
101664,1990,annals of regional science,barriers to communication: a literature review,barriers to communication a literature review,"THE PROFESSIONALIZATION OF TEACHERS: THE FIRST STEP TOWARD THE RESTRUCTURING OF VOCATIONAL EDUCATION MAY, 1990 ANGELA L. AVERY, B.S., UNIVERSITY OF MASSACHUSETTS M.Ed., SPRINGFIELD COLLEGE Ed.D., UNIVERSITY OF MASSACHUSETTS AT AMHERST Directed by Professor Kenneth A. Parker The purpose of the study was to address the Issues regarding professionalization of teachers in regional vocational—t.echnical schools in southeastern Massachu¬ setts. Professionalization was defined as the degree to which teachers participate in organizational decisions. The study was intended to determine the perceptions of vocational teachers, academic teachers, and administrators toward professionalization. The extent to which teachers in eight regional vocational—technical high schools were empowered was also explored. A review of the literature was incorporated into the design of the study. A survey was conducted at the eight schools. Five hundred two teachers and administrators responded (86 percent). The questionnaire measured six dimensions which included: horizontal and vertical communication. Collapse"
101728,1990,british journal of industrial relations,the impact of industrial relations legislation on british union density,the impact of industrial relations legislation on british union density,"The unionized share of the work force changed markedly in the United Kingdom between the 1970s and 1980s. In the 1970s density rose steadily, making the United Kingdom the most heavily organized large OECD country. In the 1980s, by contrast, density fell by 1.4 percentage points per annum -- a faster drop than in the rapidly de-unionizing U.S. or in Japan. What explains this turnaround - the severe recession of the 1980s? Shifts in the composition of employment from unionized manufacturing to services? The Thatcher government's industrial relations legislation? In this paper we investigate these questions with a quantitative analysis of 1945-1986 changes in British union density. In contrast to studies that concentrate on cyclical determinants of unionism (Bain and Elshiekh, Carruth and Disney, Booth (1983)) we focus on industrial relations legislation. We develop an index of the favorableness of labor laws to unionism and relate it to changes in density in time series regressions that control for inflation, unemployment, and the manufacturing share of employment, among other variables. As a further test, we develop an analogous labor law index for Ireland, whose industrial relations system is similar to the U.K.'s and which experienced a similar severe 1980s recession but which did not pass new laws to weaken unions, and contrast changes in density between the countries with differences in industrial relations law. Our major finding is that the Thatcher government's labor laws caused much of the 1980s fall in British union density. We present the evidence for this claim in three stages. Section 1 lays out the facts of changing union density in the U.K. and Ireland and examines structural explanations of the U.K. changes. Section 2 discusses the 1980s U.K. labor laws and develops an index of their likely impact on unionism. Section 3 presents our econometric analysis of the U.K. time series data. Collapse"
101731,1990,british journal of industrial relations,"quit rates and the impact of participation, profit-sharing and unionization: empirical evidence from uk engineering firms",quit rates and the impact of participation  profit sharing and unionization  empirical evidence from uk engineering firms,ERR
101732,1990,british journal of industrial relations,on the employment effects of introducing a national minimum wage in the uk,on the employment effects of introducing a national minimum wage in the uk,ERR
101735,1990,british journal of industrial relations,an index measure of british trade union density,an index measure of british trade union density,ERR
101740,1990,british journal of industrial relations,institutional aspects of youth employment and training policy in britain,institutional aspects of youth employment and training policy in britain,"Public policy towards youth employment and training in Britain during the past decade has been dominated by two themes: the quest to reduce youth relative pay, as part of a wider deregulation of the labour market, in order to increase access to jobs and training: and the neglect of apprenticeship in favour of the Youth Training Scheme. This paper analyses these policies in an institutional framework informed by the results of a recent research project on youth activity in industry in major EC economies (Marsden and Ryan 1986, 1988, 1989, 1991). The policy debate in Britain has tended to focus upon the effectiveness of lower youth pay at improving youth access to jobs and training (Wells 1983; Jones 1985; Junankar and Neale 1987). We accept the efficacy of lower youth pay but question its institutional viability. Youth employment and training policies must be well grounded in labour market institutions in order to achieve success. We argue that neglect of the institutional context accounts for the lopsided and partial success resulting from current policies in Britain, and that the revitalization of apprenticeship, either as such or in the equivalent form of a strongly upgraded public training scheme, has a great deal to offer. We begin with an outline of the relevant institutions. We trace their implications for outcomes in the youth market, contrast the divergent institutional directions taken by West Germany and the UK, and finally assess contemporary British policy towards youth activity in general and apprenticeship in particular. Collapse"
101892,1990,demography,what's happening to the family?  interactions between demographic and institutional change,whats happening to the family interactions between demographic and institutional change,"As soon as men and women … acquire the habit of weighing the individual advantages and disadvantages of any prospective course of action … they cannot fail to become aware of the heavy personal sacrifices that family ties and especially parenthood entail under modern conditions. (Schumpeter 1988/1942, pp. 501–502) Collapse"
101893,1990,demography,when is a father really gone?  paternal-child contact in father-absent homes,when is a father really gone   paternal child contact in father absent homes,"This research uses unique longitudinal data to examine the dynamics of the father’s presence or absence during a child’s first few years of life and consider the extent to which overt father-presence/absence statistics mask a continuing contact with the child’s father or other potential father figures. I document the extent to which (1) substantial proportions of children born to younger mothers never have had a biological father residing in the home, (2) “net” levels of fathers’ absence at various postbirth points mask significant “gross” flows of fathers in and out of the household, and (3) large proportions of children in homes lacking the biological father have potentially significant contact with absent fathers or new father figures. Collapse"
101894,1990,demography,"welfare benefits, economic opportunities, and out-of-wedlock births among black teenage girls",welfare benefits  economic opportunities  and out of wedlock births among black teenage girls,"FRANK F. FURSTENBERG, JR. University of Pennsylvania References Duncan, Saul and Greg Hoffman. 1990. ""Welfare Benefits, Economic Opportunities and OutSofS Wedlock Births Among Black Teenage Girls."" Demography 27: 519-35. Geronimus, Arline and Sanders Korenman. 1992. ""The Socioeconomic Consequences of Teen Childbearing Reconsidered. "" Quarterly Journal of Economics 107:1187-1214. Hoffman, Saul. 1998. ""Teenage Childbearing Is Not So Bad After All...Or Is It?"" Family Planning Perspectives 30: 236q3. Hoffman, Saul, E. Michael Foster, and Frank Furstenberg. 1993. ""Reevaluating the Costs of Teenage Childbearing."" Demography 30: 1-13. Collapse"
101895,1990,demography,adolescent sexual activity in the family context: the impact of older siblings,adolescent sexual activity in the family context the impact of older siblings,"Using approximately 2,000 sibling pairs from the ab] National Longitudinal Survey of Labor Market Experience of Youth, we examine the influence of an older sibling’s age at first sexual intercourse on the sexual initiation of a younger sibling. Hypotheses about differences by gender composition of the pair are tested, using a framework derived from social comparison theory and a two-stage failure-time model. Results provide evidence of a direct but modest-sized older sibling effect for white but not black youth. This effect is approximately equal in magnitude for same- and oppositesex siblings. Little support is offered for the greater salience and association of sexual activity for brother-brother as compared with sister-sister pairs. Collapse"
101896,1990,demography,determinants of contraceptive switching behavior in rural sri lanka,determinants of contraceptive switching behavior in rural sri lanka,"This study examines the influence of a selected set of determinants of contraceptive method switching in rural Sri Lanka. Of interest is the question of how change in contraceptive practice at the individual level can account for patterns observed at the aggregate level. Based on calendar data on contraceptive use over a 3-year period, collected for more than 3,000 married women in a 1986 survey, the multivariate analysis shows that women who attain all or a significant proportion of their desired fertility tend to switch to more effective methods. Women who experience method failure tend to switch methods, usually to a type that is more effective. The woman’s background determinants of age and education have small but significant effects on method switching, whereas the effect of household economic well-being is not significant. There is strong indication that rural couples are practicing contraception in a nonrandom fashion, switching methods in accordance with changes in their fertility motivations and contraceptive experience. Collapse"
101897,1990,demography,"fertility desires and fertility: hers, his, and theirs",fertility desires and fertility hers his and theirs,"The relationship between desired and achieved fertility may be misspecified by excluding husbands’ fertility desires or by confounding effects of shared desires with the resolution of conflicting desires. Using couple data from the classic Princeton Fertility Surveys, we find relatively large husband effects on fertility outcomes as well as unique effects of spousal disagreement. Wives and husbands were equally likely to achieve fertility desires, and disagreeing couples experienced fertility rates midway between couples who wanted the same smaller or larger number of children. These conditions do not hold, however, when we include willingness to delay births for economic mobility as part of the measure of fertility desires. Among couples who both wanted a third child, only husbands’ willingness to delay births had significant negative effects on birth rates. Collapse"
101898,1990,demography,residential preferences and population redistribution: 1972-1988,residential preferences and population redistribution  1972 1988,"In this article we examine the extent to which three minority groups were able to achieve selected neighborhood social and physical outcomes in the San Francisco mnetropolitan area. Ecological regressions were estimated to generate elasticities that measure the relative abilities of blacks, Hispanics, and Asians to convert education and income into desirable neighborhood environments. These regressions were interpreted in light of substantial differences between the three groups in levels of residential segregation. Results generally indicated a black disadvantage in the process of residential achievement, but it was not as dramatic as that found in earlier studies or as great as the levels of segregation would suggest. As in prior research, education was found to be the critical variable in explaining spatial differentiation and class stratification among blacks. In the United States, residences are allocated to persons and families through private housing markets. Since public housing comprises less than 2% of the nation's housing stock, and less than 1% of its housing starts (Adams 1987), virtually all households seeking a new residence enter the rental or sale market. Each household has a set of housing needs and desires based on its size, composition, life-cycle stage, and tastes; it also has a set of economic resources with which to achieve these desires, principally capital assets and income. Markets allocate households to specific residences through the mechanism of price; households purchase or rent the home that best suits their needs at the price they can afford (Berry & Rees 1969). According to neoclassic economic theory, the price of housing reflects the balance between aggregate demand and supply within local markets (Alonso 1964; Mills 1972). Researchers have pointed out, however, that housing is different from other commodities, and that these differences structure housing markets in distinct ways. First, residences are immobile; they are tied to a particular piece of land and cannot easily be consumed elsewhere (Logan & Molotch 1987). Second, they represent a very significant investment for most families, and for homeowners are the primary means of capital accumulation *Direct correspondence to the authors at the Population Research Center, University of Chicago, 1155 E. 60th Street, Chicago, IL 60637. i) The University of North Carolina Press Social Forces, September 1990, 69(1):15-32 This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 16 / Social Forces 69:1, September 1990 (U.S. Bureau of the Census 1986). Third, each residence is not only tied to a particular plot of land, it is also bound to a specific neighborhood (Massey et al. 1987) and, in turn, to a larger municipality (Logan 1978). Moreover, each geographic unit is associated with a social environment defined by the behaviors of residents and the service benefits provided by local government (Schneider & Logan 1982, 1985; Massey et al. 1987). Finally, houses, and the communities in which they are found, are more than mere commodities; they are also objects of powerful sentiments that influence judgments and condition decisions (Logan & Molotch 1987). In recognition of the fact that housing is immobile and linked to particular places, Charles Tiebout (1956) proposed a ""pure theory of local expenditures"" to account for the differentiated residential structure of cities. The theory rests on two key assumptions that households are free to move, and that places compete to attract them. Local governments offer different packages of tax costs, service benefits, and zoned environments from which home seekers choose. Over time, market competition produces a variety of service-tax-environment mixes and distributes households among them according to income, preferences, and wealth, yielding a residential structure differentiated by socioeconomic status, family life-cycle stage, race, and ethnicity. Although Tiebout's model recognizes some unique features of housing markets, it does not incorporate the fact that housing costs, and particularly home-ownership costs, reflect a substantial investment for families, or that homes, neighborhoods, and communities are the focus of strong emotional attachments. These traits give rise to what Stinchcombe (1965) calls ""communities of fate,"" where residents and institutions have large stakes not only in their own property, but in the property and characteristics of people in surrounding areas, making collective action highly likely. This propensity for collective action segments housing markets along social lines, so that individual choices are constrained by institutional practices and the collective behavior of others (Logan & Molotch 1987). The segmentation of housing markets, and the constraints it imposes on individual home seekers, are well illustrated by the case of U.S. blacks. The Tiebout model and its successors (Bish 1971; Peterson 1981) assume that households are free to move wherever their tastes and economic resources take them. Given this assumption, racial segregation is interpreted simply as the coincidental by-product of sorting based on income, wealth, and tastes (viz. Clark 1986). Considerable evidence, however, indicates that segregation does not stem from black preferences or low black-income levels. Public opinion polls show that blacks strongly endorse the principle of residential integration (Schuman et al. 1985) and express a clear preference for living in integrated neighborhoods, other things being equal (Farley et al. 1978). In spite of these tastes for integration, however, desegregation does not follow from the simple acquisition of socioeconomic resources sufficient to support spatial mobility. As education, income, and occupation rise, black-white segregation does not decline, but persists at a very high level (MIassey 1979, 1981; Denton & Massey 1988). Rather than reflecting tastes or socioeconomic status, black segregation appears to stem from constraints to black residential mobility imposed by the This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms Segregation and Neighborhood Quality / 17 collective behavior and institutional actions of whites. Studies of the real estate industry, for example, indicate that discrimination and prejudice are still widespread. Real estate agents systematically steer black home seekers away from white neighborhoods, and provide them less favorable treatment than whites (Molotch 1972; Pearce 1976; Wienk et al. 1979; Yinger 1986), actions which have been linked analytically to high levels of residential segregation (Galster 1986). At the same time, lending institutions have been found to finance a disproportionately small number of loans in black neighborhoods, even after objective social and economic factors are controlled (Taggart & Smith 1981; Pol et al. 1982; Leahy 1985). Moreover, in the few cases where blacks succeed in entering a white neighborhood, they are frequently met by organized white resistance and hostility (Hirsch 1983; Cass 1986; Bauman 1987), especially in working-class areas (Logan & Stearns 1981; Stearns & Logan 1986a); and if blacks succeed in establishing themselves, the neighborhood is quite likely to be avoided by subsequent white home seekers, resulting in eventual resegregation (Massey & Mullan 1984). Thus, the collective action of white residents and the institutional practices in the real estate and banking industries bifurcate urban housing markets along racial lines, fostering high levels of segregation despite a strong demand by blacks for integration. Some studies suggest that Hispanics face similar barriers, but to a lesser degree (Hakken 1979; James & Tynan 1986). The effect of a racially segmented housing market in creating and sustaining segregation is crucial to understanding the socioeconomic position of blacks in the United States. Barriers to spatial mobility are, in a very real way, barriers to social mobility. As Logan and Molotch (1987:49) point out, socioeconomic inequality among households and geographic inequality among places are not independent; the two systems of hierarchy reinforce one another: ""High status within the social hierarchy can bring access to the most desirable places ... and a guarantee of a rewarding future for whatever place one controls. At the same time a high status for one's geographical place means the availability of resources . . . that enhance life chances generally."" In a similar vein, Giddens (1980:107-12) argues that segregation is a core mechanism of class structuration, since it concentrates people of low status in space and ensures the maintenance of behaviors and orientations detrimental to success in the larger society. In this article we demonstrate how segregation structures the neighborhood environment achieved by three minority groups in one large urban area. We estimate the degree to which blacks, Hispanics, and Asians are able to convert socioeconomic achievements into selected neighborhood outcomes within the San Francisco-Oakland Standard Metropolitan Statistical Area (SMSA) in 1980. The analysis proceeds in three phases. First, we establish the degree of segregation experienced by blacks, Hispanics, and Asians in the San Francisco SMSA. We then consider the extent to which the degree of segregation faced by each group affects its ability to convert status attainments into neighborhood social outcomes. Finally, we evaluate the degree to which each group is able to convert status attainments into different physical characteristics of the neighborhood environment. This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 18 / Social Forces 69:1, September 1990 Collapse"
101899,1990,demography,rural-to-urban migration and child survival in senegal,rural to urban migration and child survival in senegal,"Analysis of the 1986 Senegal Demographic and Health Survey reveals that mothers may be able to improve their children’s survival chances by migrating from the countryside to the city. Children of urban migrants, however, continue to experience a much higher risk of mortality before the age of 5 than children of urban nonmigrants, even after the mother has lived in the city for several years. This migrant mortality disadvantage persists when controlling for numerous socioeconomic and fertility-related factors typically associated with child mortality in developing countries, which also serve as indicators of migrant selection and adaptation. Collapse"
101900,1990,demography,recent trends in the process of stratification,recent trends in the process of stratification,"Progress on child mortality and undernutrition has seen widening inequities and a concentration of child deaths and undernutrition in the most deprived communities, threatening the achievement of the Millennium Development Goals. Conversely, a series of recent process and technological innovations have provided effective and efficient options to reach the most deprived populations. These trends raise the possibility that the perceived trade-off between equity and efficiency no longer applies for child health--that prioritising services for the poorest and most marginalised is now more effective and cost effective than mainstream approaches. We tested this hypothesis with a mathematical-modelling approach by comparing the cost-effectiveness in terms of child deaths and stunting events averted between two approaches (from 2011-15 in 14 countries and one province): an equity-focused approach that prioritises the most deprived communities, and a mainstream approach that is representative of current strategies. We combined some existing models, notably the Marginal Budgeting for Bottlenecks Toolkit and the Lives Saved Tool, to do our analysis. We showed that, with the same level of investment, disproportionately higher effects are possible by prioritising the poorest and most marginalised populations, for averting both child mortality and stunting. Our results suggest that an equity-focused approach could result in sharper decreases in child mortality and stunting and higher cost-effectiveness than mainstream approaches, while reducing inequities in effective intervention coverage, health outcomes, and out-of-pocket spending between the most and least deprived groups and geographic areas within countries. Our findings should be interpreted with caution due to uncertainties around some of the model parameters and baseline data. Further research is needed to address some of these gaps in the evidence base. Strategies for improving child nutrition and survival, however, should account for an increasing prioritisation of the most deprived communities and the increased use of community-based interventions. Collapse"
101943,1990,econometric theory,worldwide rankings of research activity in econometrics:  an update: 1980-1988,worldwide rankings of research activity in econometrics   an update  1980 1988,"Fourteen leading international journals that publish econometrics articles are used to provide data on institutional activity in econometrics over the period 1980–1988. From this data base, institutional rankings are constructed separately for theoretical econometrics and all econometrics publications, according to standardized page counts of articles published in these journals. Some indication is given as to how institutional rankings have changed over this period. Collapse"
101944,1990,econometric theory,"a unified approach to robust, regression-based specification tests",a unified approach to robust  regression based specification tests,"This paper develops a general approach to robust, regression-based specification tests for (possibly) dynamic econometric models. A useful feature of the proposed tests is that, in addition to estimation under the null hypothesis, computation requires only a matrix linear least-squares regression and then an ordinary least-squares regression similar to those employed in popular nonrobust tests. For the leading cases of conditional mean and/or conditional variance tests, the proposed statistics are robust to departures from distributional assumptions that are not being tested, while maintaining asymptotic efficiency under ideal conditions. Moreover, the statistics can be computed using any √ T -consistent estimator, resulting in significant simplifications in some otherwise difficult contexts. Among the examples covered are conditional mean tests for models estimated by weighted nonlinear least squares under misspecification of the conditional variance, tests of jointly parameterized conditional means and variances estimated by quasi-maximum likelihood under nonnormality, and some robust specification tests for a dynamic linear model estimated by two-stage least squares. Collapse"
101945,1990,econometric theory,time series regression with a unit root and infinite-variance errors,time series regression with a unit root and infinite variance errors,"In [4] Chan and Tran give the limit theory for the least-squares coefficient in a random walk with i.i.d. (identically and independently distributed) errors that are in the domain of attraction of a stable law. This paper discusses their results and provides generalizations to the case of I (1) processes with weakly dependent errors whose distributions are in the domain of attraction of a stable law. General unit root tests are also studied. It is shown that the semiparametric corrections suggested by the author in other work [22] for the finite-variance case continue to work when the errors have infinite variance. Surprisingly, no modifications to the formulas given in [22] are required. The limit laws are expressed in terms of ratios of quadratic functional of a stable process rather than Brownian motion. The correction terms that eliminate nuisance parameter dependencies are random in the limit and involve multiple stochastic integrals that may be written in terms of the quadratic variation of the limiting stable process. Some extensions of these results to models with drifts and time trends are also indicated. Collapse"
101946,1990,econometric theory,"unbiased estimation of the mse matrix of stein-rule estimators, confidence ellipsoids, and hypothesis testing",unbiased estimation of the mse matrix of stein rule estimators  confidence ellipsoids  and hypothesis testing,We first present an unbiased estimator of the MSE matrix of the Stein-rule estimator of the coefficient vector in a normal linear regression model. The Steinrule estimator can be used with both its estimated MSE matrix and with the least-squares MSE matrix to form confidence ellipsoids. We derive the approximate expected squared volumes and coverage probabilities of these confidence sets and discuss their ranking. These results can be applied to the conditional prediction of the mean of the endogenous variable. We also consider the power of F-tests which employ the Stein-rule estimator in place of the least-squares estimator. Collapse
101947,1990,econometric theory,asymptotic expansions of the distributions of statistics related to the spectral density matrix in multivariate time series and their applications,asymptotic expansions of the distributions of statistics related to the spectral density matrix in multivariate time series and their applications,"Let { X ( t )} be a multivariate Gaussian stationary process with the spectral density matrix f 0 (ω), where θ is an unknown parameter vector. Using a quasi-maximum likelihood estimator null of θ, we estimate the spectral density matrix f 0 (ω) by f null (ω). Then we derive asymptotic expansions of the distributions of functions of f null (ω). Also asymptotic expansions for the distributions of functions of the eigenvalues of f null (ω) are given. These results can be applied to many fundamental statistics in multivariate time series analysis. As an example, we take the reduced form of the cobweb model which is expressed as a two-dimensional vector autoregressive process of order 1 (AR(1) process) and show the asymptotic distribution of null , the estimated coherency, and contribution ratio in the principal component analysis based on null in the model, up to the second-order terms. Although our general formulas seem very involved, we can show that they are tractable by using REDUCE 3. Collapse"
101948,1990,econometric theory,interpretation of graphs that compare the distribution functions of estimators,interpretation of graphs that compare the distribution functions of estimators,"In this paper I examine graphical comparisons of one-dimensional (or marginal) distribution functions of alternative estimators. It is shown that areas under the c.d.f. (cumulative distribution function) curve can be given a decision-theoretic interpretation as risk under a bounded absolute-error loss function. I also show that by a simple rescaling of the graph's axes, graphical areas are created which can be interpreted as risk under bounded squared-error loss. The bounded loss functions are applied to compare graphically and numerically the risk of exact distributions of the limited-information maximum likelihood and two-stage least-squares estimators in a simultaneous equations model. Collapse"
101949,1990,econometric theory,bandwidth selection in semiparametric estimation of censored linear regression models,bandwidth selection in semiparametric estimation of censored linear regression models,"Quantile and semiparametric M estimation are methods for estimating a censored linear regression model without assuming that the distribution of the random component of the model belongs to a known parametric family. Both methods require estimating derivatives of the unknown cumulative distribution function of the random component. The derivatives can be estimated consistently using kernel estimators in the case of quantile estimation and finite difference quotients in the case of semiparametric M estimation. However, the resulting estimates of derivatives, as well as parameter estimates and inferences that depend on the derivatives, can be highly sensitive to the choice of the kernel and finite difference bandwidths. This paper discusses the theory of asymptotically optimal bandwidths for kernel and difference quotient estimation of the derivatives required for quantile and semiparametric M estimation, respectively. We do not present a fully automatic method for bandwidth selection. Collapse"
101951,1990,econometric theory,on the sensitivity of a regression coefficient to monotonic transformations,on the sensitivity of a regression coefficient to monotonic transformations,This note presents a procedure which enables the user to check whether a monotonic transformation can change the sign of a regression coefficient. One possible use of this procedure is to examine the robustness of key regression coefficients. Collapse
101953,1990,econometric theory,efficient estimation of linear and type i censored regression models under conditional quantile restrictions,efficient estimation of linear and type i censored regression models under conditional quantile restrictions,"We consider the linear regression model with censored dependent variable, where the disturbance terms are restricted only to have zero conditional median (or other prespecified quantile) given the regressors and the censoring point. Thus, the functional form of the conditional distribution of the disturbances is unrestricted, permitting heteroskedasticity of unknown form. For this model, a lower bound for the asymptotic covariance matrix for regular estimators of the regression coefficients is derived. This lower bound corresponds to the covariance matrix of an optimally weighted censored least absolute deviations estimator, where the optimal weight is the conditional density at zero of the disturbance. We also show how an estimator that attains this lower bound can be constructed, via nonparametric estimation of the conditional density at zero of the disturbance. As a special case our results apply to the (uncensored) linear model under a conditional median restriction. Collapse"
101954,1990,econometric theory,"stationarity and persistence in the garch(1,1) model",stationarity and persistence in the garch(11) model,No Result.
101955,1990,econometric theory,the local power of the cusum and cusum of squares tests,the local power of the cusum and cusum of squares tests,"We consider the local power of the cusum and cusum of squares tests for structural change in the linear regression model. We show that the local power of the cusum of squares test equals its size for a wide class of structural changes, as compared to a nontrivial local power for the cusum test. The conventional ranking of these procedures is thus reversed. Collapse"
101956,1990,econometric theory,model-free asymptotically best forecasting of stationary economic time series,model free asymptotically best forecasting of stationary economic time series,"Given observations on a stationary economic vector time series process we show that the best h -step ahead forecast (best in the sense of having minimal mean square forecast error) of one of the variables can be consistently estimated by nonparametric regression on an ARMA memory index. Our approach is based on a combination of the ARMA memory index modeling approach of Bierens [7] with a modification to time series of the nonparametric kernel regression approach of Devroye and Wagner [16]. This approach is truly model-free, as no explicit specification of the distribution of the data generating process is needed. Collapse"
101957,1990,econometric theory,the fredholm approach to asymptotic inference on nonstationary and noninvertible time series models,the fredholm approach to asymptotic inference on nonstationary and noninvertible time series models,"A unified approach which I call the Fredholm approach is suggested for the study of asymptotic behavior of estimators and"" test statistics arising from nonstationary and/or noninvertible time series models. Some limit theorems are given concerning the distribution of (the ratio of) quadratic (plus linear) forms in random variables generated by a linear process that is not necessarily stationary. Especially, the limiting characteristic function is derived explicitly via the Fredholm determinant and resolvent of a given kernel. Some examples are also shown to illustrate our methodology. Collapse"
101958,1990,econometric theory,testing for a moving average unit root,testing for a moving average unit root,"This paper develops univariate seasonal unit root tests based on spectral regression estimators. An advantage of the frequency domain approach is that it enables serial correlation to be treated non-parametrically. We demonstrate that our proposed statistics have pivotal limiting distributions under both the null and near seasonally integrated alternatives when we allow for weak dependence in the driving shocks. This is in contrast to the popular seasonal unit root tests of, among others, Hylleberg et al. (1990) which treat serial correlation parametrically via lag augmentation of the test regression. Our analysis allows for (possibly infinite order) moving average behaviour in the shocks. The size and power properties of our proposed frequency domain regression-based tests are explored and compared for the case of quarterly data with those of the tests of Hylleberg et al. (1990) in simulation experiments. Collapse"
101959,1990,econometric theory,functional forms of characteristic functions and characterizations of multivariate distributions,functional forms of characteristic functions and characterizations of multivariate distributions,"During the Oxford Conference of the Econometric Society in 1936, Ragnar Frisch proposed a problem of characterization of distributions based on the property of linear regression of one linear function of random variables on the other. This problem has been solved, partially by Allen [1], and then completely by Rao [24,25], Fix [7], and Laha [13] relaxing the conditions imposed on the component random variables. The purpose of this paper is to solve the above mentioned problem for the multivariate case, characterizing multivariate distributions based on the multivariate linear regression of one linear function of not necessarily i.i.d. random vectors with matrix coefficients on the other. We make some mild assumptions concerning the component random vectors and the related constant matrices. It is shown that the property of multivariate linear regression yields a system of partial differential equations (p.d.e.'s) satisfied by the characteristic functions of the component random vectors. A general solution of this system of p.d.e.'s is given by certain functional forms. Special cases of the general solution give characterizations of the “multivariate generalized stable laws” and the multivariate semistable laws, and a method is presented to characterize the multivariate stable laws. Collapse"
101960,1990,econometric theory,strong consistency in nonlinear regression,strong consistency in nonlinear regression,Sufficient conditions are given to ensure the existence of a sequence of strongly consistent estimators of an unknown parameter for a nonlinear regression model. The parameter space includes all separable metric spaces but is not assumed to be compact. Collapse
101961,1990,econometric theory,additive interactive regression models:  circumvention of the curse of dimensionality,additive interactive regression models circumvention of the curse of dimensionality,"This paper considers series estimators of additive interactive regression (AIR) models. AIR models are nonparametric regression models that generalize additive regression models by allowing interactions between different regressor variables. They place more restrictions on the regression function, however, than do fully nonparametric regression models. By doing so, they attempt to circumvent the curse of dimensionality that afflicts the estimation of fully nonparametric regression models. In this paper, we present a finite sample bound and asymptotic rate of convergence results for the mean average squared error of series estimators that show the AIR models do circumvent the curse of dimensionality. The rate of convergency of these estimators is shown to depend on the order of the AIR model and the smoothness of the regression function, but not on the dimension of the regressor vector. Series estimators with fixed and data-dependent truncation parameters are considered. Collapse"
101972,1990,econometrica,common knowledge of an aggregate of expectations,common knowledge of an aggregate of expectations,No Result.
101973,1990,econometrica,a simple characterization of stochastically monotone functions,a simple characterization of stochastically monotone functions,"Philippe Bougerol (Université Paris VI) Random walks and semisimple groups A random walk on a group G is the product Sn = g1g2 · · · gn of random variables (gk) with values in G, independent and with the same distribution. In the first talk I will illustrate the fact that simple transformations of random walks in the (commutative) weight lattice of a semi simple complex group G can describe the Littelmann theory of finite dimensional representations of these groups. In the second talk I will show that some random walks on G itself give an interpretation of the geometric crystal of Berenstein and Kazhdan. By tropicalization, one recovers Littelmann theory. This is a survey of the following works: [1] Biane, Ph., Bougerol, Ph, O’Connell, N. Littelmann paths and Brownian paths. Duke Math. J. 130 (2005), no. 1, 127-167. [2] Biane, Ph., Bougerol, Ph, O’Connell, N. Continuous crystal and DuistermaatHeckman measure for Coxeter groups. Adv. Maths. 221 (2009) 1522-1583. [3] Chhaibi Reda, Modèle de Littelmann pour cristaux géométriques, fonctions de Whittaker sur des groupes de Lie et mouvement brownien (Paris 6 thesis) Marek Bozejko (University of Wroc law) Generalized Gaussian processes with application to noncommutative functional analysis In my talks I will consider the following subjects: (a) q-Gaussian processes, theta functions of Jacobi, second quantization and connections with new von Neumann algebras. Also we will consider connections with the free probability (case q = 0) and we give characterisation of (b) Free Levy processes. Also we present the Meixner laws and the role of the classical Nevanlinna-Pick theorem about analytic functions on the upper half-plane. Next we will give the role of (c) Hecke-YangBaxter operators for the construction of new models of noncommutative probability like monotone probability, Boolean probability and others. Connections with some new models of operator spaces and noncommutative Khinchine inequality will be also done. References: [1] M. Bozejko, On Lambda(p) sets with minimal constant in discrete noncommutative groups, Proc. Amer. Math. Soc. 51 (1975), 407-412. [2] M. Bozejko and R. Speicher, An example of a generalized Brownian motion, Comm. Math. Phys. 137 (1991), 519-531. [3] M. Bozejko and R. Speicher, Completely positive maps on Coxeter groups, deformed commutation relations and operator spaces, Math. Annalen, 300, 97-120(1994). [4] M. Bozejko and R. Speicher, Interpolation between bosonic and fermionic relations given by generalized Brownian motion, Math. Zeitsch., 222, 135-160. [5] M. Bozejko., B. Kummerer and R. Speicher, q-Gaussian Processes: Non-commutative and Classical Aspects, Comm. Math. Phys. 185, 129-154 (1997). [6] M. Bozejko, Ultracontractivity and strong Sobolev inequality for q-Ornstein-Uhlenbeck semigroup (−1 < q < 1); Infinite Dimensional Analysis, Quantum Probability and Related Topics 2 (1999), 203-220. [7] M. Bozejko and M. Guta, Functors of white noise associated to characters of the infinite symmetric group, Comm. Math. Phys. 229 (2002), 209-227. [8] M. Bozejko and W. Bryc, On a class of free Levy laws related to a regression problem, J. Funct. Anal. 236 (2006), 59-77. [9] M. Bozejko and E. Lytvynov, Meixner class of non-commutative generalized stochastic processes with freely independent values. I. Characterization, Comm. Math. Phys. 292 (2009), 99-129. [10] M. Anshelevich, S. Belinschi, M. Bozejko and F. Lehner, Free infinite divisibility for q-Gaussians, Math. Res. Lett.17 (2010), 909-920. [11] S. Belinschi, M. Bozejko, F. Lehner and R. Speicher, The normal distribution is free infinitely divisible, Adv. in Math. 226 (2011), 3677-3698. [12] M. Bozejko, Deformed Fock spaces, Hecke operators and monotone Fock space of Muraki, Demonstratio. Math. 45 (2012), 399-413. Kai-Uwe Bux (Bielefeld University) Noncrossing partitions and classifying spaces for braid groups (after T. Brady) Let G be a group. A classifying space for G is a CW-complex with fundamental group G and contractible universal cover. This cover is a contractible CW-complex with a free action of G. Conversely, any such contractible free G-complex gives rise to a classifying space. Classifying spaces are unique up to homotopy equivalence and their homotopy type is an important topological invariant of the group. Searching within this homotopy type for particularly nice representatives is an important part of geometric group theory. In this commissioned talk, I shall consider the case where G is the braid group Bn on n strands. Tom Brady has constructed a particularly nice classifying space for this group making essential use of noncrossing partitions. I shall explain his construction. Xiao-Wu Chen (University of Science and Technology of China, Hefei) Singularity categories, Leavitt path algebras and shift spaces To a finite quiver (= a finite oriented graph), one associates three objects of differential types: the singularity category of the corresponding finite-dimensional algebra with radical square zero, the Leavitt path algebra and the shift space. We will explain that these three objects are closely related to each other. Dmitri Finkelshtein (Institute of Mathematics, Kiev) Harmonic analysis on configuration spaces and related convolution algebras Harmonic analysis on the space of locally finite subsets (configurations) of underlying spaces is connected to the proper combinatorial-type convolutions on the space of functions of finite subsets. We consider two types of convolutions for such functions and study their properties and relations. The corresponding calculus w.r.t. one type of the convolutions is considered and the connections with the convolution of the corresponding states (measures) are established. We study also derivative operations w.r.t. this convolution and apply them for describing of the evolution for cummulants of the measures. Uwe Franz (Université de Franche-Comté, Besançon) The Quantum Symmetry Group of a Hadamard Matrix To each complex Hadamard matrix one can associate a unique ”quantum symmetry group” (or quantum permutation group, i.e. a subgroup of the free permutation compact quantum group S N). Many questions about the subfactors and planar algebras associated to a Hadamard matrix have an equivalent formulations in terms of its quantum symmetry group. In my talk I will present a probabilistic approach to characterising this quantum symmetry group and study several examples in small dimension. For 4×4 Hadamard matrices one obtains twists of the dihedral groups DN . Based on joint work with Teodor Banica, Franz Lehner, and Adam Skalski. Sergej Kuksin (École Polytechnique, Palaiseau) Analysis of the KdV equation and its perturbations It is known since 1960’s that ”the KdV equation is integrable”, but exact analytical meaning of this assertion became clear only recently. And even now it is not clear ”how integrable is KdV”. Right answer to this question is important for physics, where often not KdV but its perturbations are used. To study perturbed KdV exact properties of the transformation which integrates KdV and of the KdV hamiltonian are crucial. In my talk I will discuss the corresponding results and open problems. References: [1] T. Kappeler, J. Poschel, KdV & KAM, Springer 2003. [2] S. Kuksin, Gal. Perelman, Vey theorem in infinite dimensions and its application to KdV, DCDS-A 27 (2010), 1-24. [3] E. Korotyaev, S. Kuksin, KdV Hamiltonian as function of actions, arXiv 2011. Grigori Olshanski (Russian Academy of Sciences, Moscow) Determinantal measures and Markov dynamics My two talks are based on joint work with Sergey Pirogov, [7]. We construct a model of Markov dynamics for a one-dimensional lattice gas system with infinitely many interacting particles. The model possesses a stationary distribution, which is a determinantal measure meaning that its correlation functions are given by principal minors of a kernel K(x, y) on the lattice. (About determinantal measures, see e.g. Borodin [1], Soshnikov [8].) The kernel K(x, y) first appeared in Borodin–Olshanski [3]; it has the same general structure A(x)B(y)−B(x)A(y) x− y as the so-called integrable kernels from Random Matrix Theory. We call K(x, y) the Gamma kernel, because A and B are expressed through Euler’s Γ-function. In the equilibrium regime, reproducing the stationary distribution, the dynamical correlation functions of our dynamics also have determinantal form. That is, they are given by appropriate minors of an “extended” kernel K(x, s; y, t) with two space-time arguments, (x, s) and (y, t). The structure of this “extended Gamma kernel” is similar to that of the extended kernels from Random Matrix Theory (see Tracy–Widom [9]). The present work is related to previous works [4], [5], [6] by Borodin and myself, devoted to dynamical models for some continuous gas systems. Both continuous and lattice models in question originated from some problems of representation theory (see Borodin–Olshanski [2]). However, the methods used in the continuous case and in the lattice case are quite different. In the lattice case, our approach is based on a close connection between determinantal measures and their “noncommutative” analogs — quasifree states on the algebra CAR of canonical anticommutation relations. References: [1] Alexei Borodin, Determinantal point processes. In: The Oxford Handbook of Random Matrix Theory (Gernot Akemann, Jinho Baik and Philippe Di Francesco, editors), Oxford University Press, 2011, pp. 231–249; arXiv:0911.1153. [2] Alexei Borodin and Grigori Olshanski, Distributions on partitions, point processes and the hypergeometric kernel . Communications in Mathematical Physics 211 (2000), 335–358; arXiv: math.RT/9904010. [3] Alexei Borodin and Grigori Olshanski, Random partitions and the Gamma kernel . Advances in Mathematics 194 (2005), 141–202; arXiv: math-ph/0305043. [4] Alexei Borodin and Grigori Olshanski, Markov processes on the path space of the Gelfand-Tsetlin graph and on its bound Collapse"
101974,1990,econometrica,time consistency of fiscal and monetary policy:  a comment,time consistency of fiscal and monetary policy a comment,"IN AN IMPORTANT recent contribution, Persson, Persson, and Svensson (1987) (hereafter PPS) suggest that through careful restructuring of its nominal and real debt obligations, a government may be able to induce future governments to follow the monetary and fiscal policies that it regards as optimal today. The PPS argument builds on Lucas and Stokey's (1983) demonstration that in a special nonmonetary setting, the time inconsistency of optimal fiscal policy can be avoided through managing the term structure of real government obligations to the public. The basic idea of the PPS scheme for monetary economies is disarmingly intuitive: in addition to continually restructuring nominal and real debt obligations a la Lucas-Stokey, each government must ensure that the next government inherits a stream of nominal claims on the public whose present discounted value equals the stock of money. This equality, PPS argue, removes the incentive for surprise inflation or deflation, because such surprises would not affect the real net worth of the government. This note shows that the PPS prescription for avoiding time inconsistency, appealing as it is, is not generally sufficient. Even under the debt restructuring they recommend, optimal policy is likely to be time inconsistent. The main reason why their scheme fails is that the restrictions it imposes on government asset stocks satisfy first-order but not second-order conditions for an optimum. Because of the complex interactions between the current price level and future interest rates, a government can raise its objective function by moving several variables at once away from the levels planned by the previous government, even though price-level changes alone would not affect government net worth. We develop our argument using the model, notation, and equation numbers of PPS, to which the reader is referred for details. The maximization problem associated with Collapse"
101983,1990,econometrica,testing for a global maximum in an econometric context,testing for a global maximum in an econometric context,No Result.
101984,1990,econometrica,on the possibility of price decreasing bubbles,on the possibility of price decreasing bubbles,No Result.
101986,1990,econometrica,a globally stable price adjustment process,a globally stable price adjustment process,"By observing the quantity demanded at particular prices, a firm may learn about the parameters of its demand curve. In such an environment, price changes obstruct the learning process by inducing additional noise. The authors' paper constructs a dynamic model where a price-setting firm endogenously controls the speed of learning. The model provides a possible explanation for price inertia, as a stable pricing policy allows the firm to learn more rapidly, which improves future expected profits. Furthermore, even in the long run, learning continues to affect the firm's optimal price. Copyright 1990 by Royal Economic Society. Collapse"
102015,1990,economic geography,"discontinuity and the emergence of flexible production: garment production in toronto, 1901-1931",discontinuity and the emergence of flexible production  garment production in toronto  1901 1931,"The economic and spatial evolution of Toronto's garment industry during the early 20th century is examined. Two stages of development are outlined. The 1901–1915 period was marked by the rapid growth of large, vertically integrated clothing factories. This trend was reversed after 1915, however, when small, vertically disintegrated clothing firms began to recapture the market for ready-made apparel. These economic changes were accompanied by equally profound shifts in the geography of clothing production. An explanation for the discontinuous evolution of clothing production must include a careful investigation of the relationship between labor and capital, the nature of subcontracting, and the ethnic composition of the garment work force. In focusing on the clothing industry, this study highlights some of the limitations of the concept of Fordism as it is currently used in economic geography. Fordist forms of production and labor organization were introduced by garment manufacturers during the early 20th ... Collapse"
102019,1990,economic geography,an introduction to current research on latin american cities,an introduction to current research on latin american cities,ERR
102023,1990,economic geography,physician proletarianization and medical care restructuring in argentina and uruguay,physician proletarianization and medical care restructuring in argentina and uruguay,"Labor disenfranchisement has rarely focused on skilled labor such as physicians. In Latin America, physicians play a key role in social production and reproduction. This paper traces the loss of physician control over the conditions of their work since the turn of the century. Argentine and Uruguayan physicians manage to keep the medical industry free of government intervention that would reduce their income-earning potential. Medical care in both countries has responded to the forces of ethnicity, class, and changing medical technology. Bureaucratic complexity and numerous small medical programs allow physicians to hold down several jobs. The labor theory of value suggests that physician proletarianization is well underway in both countries. Specifically, a trend toward physician proletarianization is most evident in four key ways: the terms of criteria for entrance into the profession, autonomy regarding the terms and content of work, tools of labor, and the amount and rate of remuneration. The paper concludes that studies of physician production and reproduction in the Latin American city must go beyond ""doctor bashing"" if we are to understand the transformation of health care delivery. Collapse"
102057,1990,economic journal,an experimental examination of spot markets for electricity,an experimental examination of spot markets for electricity,ERR
102063,1990,economic journal,"demand for money in a dual-currency, quantity-constrained economy: hungary and poland, 1956-1985",demand for money in a dual currency  quantity constrained economy  hungary and poland  1956 1985,ERR
102069,1990,economic journal,the dynamics of inflation with constant deficit under expected regime change,the dynamics of inflation with constant deficit under expected regime change,"A model that describes the dynamics of inflation with a fully anticipated stabilization program is developed. It is shown that with a constant deficit, an economy tends to stay at the neighborhood of the lower of the two state inflation rates that are compatible with the deficit. Further, increasing inflation rates prior to the stabilization are compatible only with deficits that are lower than the maximal feasible staedy- state deficits, and with a stabilization program that reduces the demand for real balances. If these conditions are not met, inflation decreases towards the stabilization date. It is argued that the data generated by actual inflationary episodes fit the conditions described above and corroborate the model. Collapse"
102071,1990,economic journal,saving and rational expectations:  evidence for the u.k.,saving and rational expectations evidence for the uk,No Result.
102072,1990,economic journal,savings and rational expectations:  a correction and further observations,savings and rational expectations a correction and further observations,"The aim of MacDonald and Speight (I989), hereafter MS, was to empirically implement and test the rational expectations - permanent income model, REPI, with appropriate stationary variables. The paper was in essentially two parts. The first part dealt with the cointegration of disposable income and two measures of real consumer expenditure, whilst the second part contained some restrictions tests from a BVAR model with quasi-savings and the change in labour income. A subsidiary finding of our modelling strategy was the result that savings positively Granger, caused the change in labour income. Attfield, Demery and Duck (i 990), hereafter ADD, question the numerical accuracy of our labour income series (attributing our finding of a positive relationship between savings and the change in labour income to this inaccuracy), the appropriate theoretical measure of labour income, and the efficiency of our tests of the REPI. As our response to these points indicates, ADD's comments do not undermine the central findings of our previous paper. Indeed the striking finding of our previous paper, that once allowance for transitory consumption is made the REPI restrictions cannot be rejected, is reinforced in our current note. Collapse"
102073,1990,economic journal,the current state of undergraduate economics in the united kingdom,the current state of undergraduate economics in the united kingdom,"report of the economics profession, and to the Royal Economic Society's analysis of the relation between economics and business studies.2 In particular the survey was designed to discover the desired and actual characteristics of student intake, especially the position of 'A' level economics as a potential prerequisite and influence on performance, and to elicit information on departments' philosophies of education. The intention was less to produce a detailed statistical description of the current situation, than to gain an impression of how departments themselves saw the evolving trends, and how they proposed to deal with them. Questionnaire replies were received from 57 of the 75 institutions which offer single or joint honours degrees, and cross-checks with UGC and DCP sources indicated a coverage of over two-thirds of university economics graduates, and over 90g0 of those from polytechnics. From I 983 to i 987 the numbers of both graduates and entrants rose substantially. The average university department reported an increase of over 20% in admissions; the average polytechnic department, over a 250% rise. This expansion has now stopped and for the period from i988 to 1992 the average expected increase across both sectors is only 400. The survey shows that central government funding and the institutions' central decision-making apparatuses were equally the most important constraints on the growth of the subject. Student demand was the least important limitation. The results indicate buoyant demand, but suggest the existence of institutional obstacles to the translation of that demand into places, even if the system as a whole is made more responsive to student wishes. Thirty-nine of the reporting institutions have both management and economics courses, and sixteen of the respondents felt that the management degrees inhibited the growth of the economics programmes. This response was more common where the economics and management departments were administratively linked. As there are moves, particularly in polytechnics, to Collapse"
102075,1990,economic journal,the road to uruguay,the road to uruguay,No Result.
102076,1990,economic journal,departures from multilateralism:  regionalism and aggressive unilateralism,departures from multilateralism regionalism and aggressive unilateralism,"Multilateralism can mean many things to many people. For instance, it may mean balancing trade multilaterally, at whatever level of payments deficit or surplus one desires, as distinct from insisting on bilateral balancing with specific countries: the latter constituting a folly and a temptation that few politicians can resist but most economists will run from. But the two characteristics of the GATT-based multilateral trading system that spring to mind most readily to scholars of international trade, and which therefore I focus on here are, first the principle of non-discrimination, which implies the extension of MFN (mostfavoured nation) treatment to all GATT members; and second the principle that balanced, mutual, reciprocal concessions, and acceptance of new disciplines, amounting to 'first difference' reciprocity (Bhagwati, I988), be the method for achieving progressive movement toward free trade. This would rule out the aggressive use of power to extract either unrequited trade concessions or acceptance of new disciplines, therefore shielding the weak against the strong who would otherwise, in bilateral one-on-one confrontations, have an advantage constrained only by altruism or conscience. These principles are stated in idealised form. In reality, however, the GATT permits departures from MFN for (i) countervailing duties against foreign subsidies (CVD) and as anti-dumping (AD) measures aimed only at the offending parties (Article VI);' (2) developing countries (Part IV) who get Special and Differential treatment such as preferentially lower tariffs in developed countries that are GATT members, and (3) customs unions and free trade areas (Article XXIV).2 Collapse"
102077,1990,economic journal,non-discriminatory discrimination:  special and differential treatment under the gatt for developing countries,non discriminatory discrimination   special and differential treatment under the gatt for developing countries,"Developing countries have long been strong supporters of a non-discriminatory rule-based multilateral system, on the grounds that as economically small units they would otherwise be subject to bilateral pressure from larger powers. At the same time, they have equally been committed to special-and-differential (S & D) treatment for themselves under GATT rules. Their approach has been to argue that trade problems of developing countries are special from those of developed countries; balance-of-payments problems, viewed by them as endemic to low-income countries, make liberalisation difficult, and import substitution strategies suggest that liberalisation is, anyway, undesirable. Poor export prospects (export pessimism) imply they need different treatment from developed countries in order to grow; that is, trade preferences in developed country markets. Special and differential also became a bloc-wide strategy jointly pursued, reflecting the premise that developing countries are jointly and simultaneously special and differential in their trade problems. This is, however, changing. In the Uruguay Round developing countries have shown themselves remarkably adept at moving away from the earlier, somewhat passive, bloc-wide S & D approach. In a surprising range of negotiating groups, countries have actively pursued country over bloc-wide interests, and in an active way involving a willingness to take on new GATT disciplines. While S & D is far from having been abandoned as a principle of the trading system, the approach seems to be one of protecting and preserving what is there, rather than pushing for further enhancement. The reasons for these changes are, as always, multi-faceted. Strong growth performance in the more outward-oriented developing countries (Korea, and other NICs) has weakened the intellectual commitment to import substitution. What exactly S & D has yielded developing countries in concrete trade policy terms has also been queried. And over the years, the differences between developing countries (large/small, middle-income/least-developed, industrialised/commodity exports, agricultural importers/exporters, and others) have grown, to the point that in the Uruguay Round a grand coalition of all developing countries does not exist in any active sense. Collapse"
102272,1990,energy economics,some general equilibrium effects of declining crude oil production in australia,some general equilibrium effects of declining crude oil production in australia,"Abstract Crude oil production in Australia is expected to decline significantly during the 1990s. As a result, net imports of crude oil could increase sharply over the period. In this paper, macroeconomic, intersectoral and interindustry effects of a fall in domestic crude oil production are examined. General equilibrium effects are estimated using ORANI, a large multisectoral model of the Australian economy. By itself, the reduction in crude oil production would require restraint in domestic expenditure and a decline in Australia's real exchange rate in order to offset the expected rise in net crude oil imports. Furthermore, a contraction of the domestic oil industry would result in a small decline in gross domestic produce together with an expansion of other import-competing and export industries. It is important to note that other changes in the energy sector, such as the substantial expected increase in production of coal, uranium and LNG over the next decade, have the potential to more than offset the macroeconomic effects of reduced oil production. Collapse"
102273,1990,energy economics,the demand for energy in the large-scale manufacturing sector of pakistan,the demand for energy in the large scale manufacturing sector of pakistan,"Abstract The extent of interfuel substitution, as well as substitution between energy and non-energy inputs, in the large-scale manufacturing sector of Pakistan has been examined. The model has been estimated in two stages. In the first stage input demand for various energy components is estimated and hence an aggregate Divisia index is constructed. In the second stage this index is used as an instrument to estimate aggregate input demand for capital, labour and energy along with their price and substitution elasticities. It seems that there is little interfuel substitution. The results also show that energy and labour are substitutes while energy and capital are complement. Collapse"
102431,1990,growth and change,producer services development and the role of telecommunications:  a case study in rural washington,producer services development and the role of telecommunications a case study in rural washington,"Information-intensive producer services, which constitute one of the fastest growing components of the U.S. economy, have been identified as a potential contributor to economic development in rural areas. This issue is examined in a case study of a community in rural Washington State. The findings indicate that producer services have not been decentralizing to rural Washington, and that opportunities for producer services development in rural communities are limited because of the inaccessibility of markets, smaller pools of skilled labor, and the lack of agglomeration economies. Opportunities for producer services are greatest in large rural communities with high-quality telecommunications systems. Although the quality of telecommunications systems is important to the economic health of communities, advances in telecommunications can be a two-way street for rural America. While telecommunications improvements increase a rural community's access to information and make it possible for rural businesses to more easily serve non-local markets, they can also make it easier for firms located in urban areas to serve rural markets via branch offices or through the telecommunications system. Collapse"
102454,1990,industrial relations,socrates confronts final-offer selection,socrates confronts final offer selection,"Eavesdrop morning coffee at any major centre of evolutionary theory today, and you will find ‘parasite’ to be one of the commonest words in the language. Parasites are touted as prime movers in the evolution of sex, promising the final solution to that problem of problems, the puzzle that led G. C. Williams to proclaim in 1975 ‘a kind of crisis’ at hand in evolutionary biology (Hamilton, 1980; Tooby, 1982; Seger & Hamilton, 1988). Parasites seem to offer a plausible justification for the otherwise futile effort females put into choosing among posturing males (Hamilton & Zuk, 1982; but see Read, 1990). Frequency-dependent selection exerted by parasites is, according to one admittedly minority view, largely responsible for the high levels of diversity found in gene pools (Clarke, 1979). One might even extrapolate to a time when the entire metazoan body could come to be seen as a gigantic adaptation against microscopic pathogens. Collapse"
102456,1990,inquiry,payment reform:  sizing up its wrinkles and implications,payment reform sizing up its wrinkles and implications,"Latin America and the Caribbean Region experienced dramatic changes in the 1990s. Politically, all but one country, are governed by a democratically elected government. Economically, import substitution industrialization policies (ISI) followed in the past, were replaced by liberalization programs aimed at reducing inflationary pressures and creating a competitive environment. The significant increase in capital flows to Latin America in one single year, 1990, buried the 1980s as the “lost decade,” and the successful implementation of privatization programs region-wide prompted to affirm that the 1990s might constitute the “Latin America's decade.” Where does the euphoria come from? Is there any implicit promise to be derived from such international capital flows? Will the pattern be sustained? Has Latin America begun a new era? Are unfolding events on defiance of fundamentals? These and many other questions can be raised regarding the spectacular transformation of Latin America and the Caribbean, particularly when analysts still debate about the Mexican crisis of 1994, investors eagerly pursue the agenda of a second privatization wave, experts around the world get fascinated with the high-tech push found in Latin America, bankers apply Latin American lessons to deal with the currency crisis in Asia, and casual observers recognize the value-creation process added by Latin American entrepreneurs who challenge the most adverse circumstances. Indeed, Latin America and the Caribbean is a land full of promises and contrasts, where there exists a head to head competition between globalization and nationalism, the haves and the have-nots, capitalism and communism, literature and high-technology, markets and governments, East and West, North and South, myth and reality, and … “despair and hope.” There is no question, however, that Latin America and the Caribbean, being she a detached wide-land, is a region of great opportunity. Since the external debt crisis of 1982 and its aftermath, democracy, open markets, economic reform and privatization have blended to offer great expectations and opportunities for business and investment in the region. The new vision strongly questioned the status quo to render a new business environment to open the doors and light up the roads of the upcoming millennium. It is the purpose of the International Journal of Public Administration to offer to its readers, for the very first time, a special issue devoted entirely to the discussion of the new business environment of Latin America and the Caribbean. We are, therefore, grateful to all the authors who generously are sharing with us the findings from their scholarly research. Given the far reaching consequences of their contributions, we, as guest editors of this special issue, had no other choice but to incorporate the fruits yielded by this symposium of thirty-seven papers in four issues in one single volume. The papers have been sorted according to the following four focal points: Privatization of State Owned Enterprises; Mexico; Economic, Financial and Foreign Investment Issues; and Economic Integration, Trade and Cultural Issues. Part I of this special issue on “The New Latin American Business Environment” looks at one element of the broad economic strategy followed by most Latin American countries: Privatization of State Owned Enterprises. The role of governments is to provide the framework that will allow the private sector to create wealth. Notwithstanding, this partnership between the public and private sectors must ensure the inclusion of the poorer sections of the population. In many ways, the long-term sustainability of these economic programs will largely depend on this. The ten papers selected for this part, provide insight on how this phenomenon is affecting different Latin American countries. The first paper by Shamsul Haque argues that there is a need to analyze the social consequences of privatization programs. Further research is needed to identify the main advocates and beneficiaries of privatization programs. According to the author, “critical economic conditions have not improved significantly after privatization, and in many instances, the conditions have deteriorated.” About fifty percent of Latin America's population of 470 million people live under poverty. The late Sister Martin Byrne (1) documents in her paper, “Cananea Consolidated Copper Company from Nationalization to Privatization: 1972-1991 ,” the problems of ownership and management faced by La Cananea, a Mexican copper mine. Sister Byrne argues that “The Cananea mines were profitable under entrepreneurial and MNC ownership, but proved to be a financial drain on the government during the paraestatal period.” The third paper by Garcia and Dyner, examined the reform and regulation of electricity in Columbia. According to the authors, the regulatory framework adopted by the government is going to determine the success of these programs. Furthermore, “the challenge is the change of public intervention in the sector, so that it regulates, supports, and supervises the decentralized activities of the firms, and liberates resources to be invested in other areas.” Walter and Gonzalez provide interesting philosophical arguments on technology and human resources management derived from the cases of privatized companies in Argentina. The authors consider two variants, “systemic modernization and revamping of existing teams” to invite a reopening of the old debate on technological blending. They argue, however, that “to compete you do not necessarily need to ‘ be on the frontier.’” Joan B. Anderson examines, the “Privatization, Efficiency and Market Failure: Transforming Ecuador's Public Sector,” privatization in Ecuador through the shift experienced by development theory with respect to the role of the public sector. In this paper the author points out that “while careful privatization can be positive, privatizing monopolies like the electric utility and/or quasi-public goods like highways are likely to be detrimental to long run economic development.” Doshi identifies the successes and failures of the privatization program in Mexico by analyzing the cases of Mexicana Airlines, Aeromexico and Telmex. The author argues that even though the government was able to sell a number of state owned enterprises, a “successful” privatization program required appropriate macroeconomic policies and defining the role of foreign investment in economic development. One can argue then, that even though the size of the state is shrinking, its role is becoming more important. The article by Vetter and Zanetta analyze also the case of Argentina. The authors argue that in order to consolidate the economic reforms implemented by the national government, provincial reform has to take place. A number of important lessons were identified. John M. Kirk and Julia Sagebien present, in “Cuba's Market Rapprochement: Private Sector Reform - Public Sector Style,” the highlights of Cuba's process of transition towards a market economy by analyzing the conditions that lead to a market opening as well as the ends, the means and the actors of the ensuing process of economic reform. Walter T. Molano contributes a paper, “The Lessons of Privatization,” based on his book The Logic of Privatization: The Case of Telecommunications in the Southern Cone of Latin America by looking at privatization as a process that may end up in varied outcomes as seen from microeconomic-, macroeconomic-, and political perspectives of analysis. The focal point of Part II is Mexico. It is very clear that since the beginning of the decade, Mexico has made major efforts to transform its economy in order to play a more significant role in the global economy. Different attempts have been undertaken leading to: first, address the aftermath of the debt crisis of 1982; second, modernize and open the economy through a structural change that have included, among other programs, privatization, deregulation, fiscal deficit reduction, and trade liberalization: and third, change the political landscape. Ephraim Clark models, in his “Agency Conflict and the Signaling Snafu in the Mexican Peso Crisis of 1994,” the conflict as a government held option to default and introduce signaling by assuming that the Mexican government had monopolistic information on the economy's true situation. The author argues that “if steps had been taken in late 1993 and early 1994, the crisis element of the adjustment could probably have been avoided.” Blaine's article examines the role of foreign capital in economic development. By studying the Mexican case, the author answers a number ofvery important questions: How are once protected markets going to react to a large inflow of foreign capital? How did Mexican authorities deal with these inflows? What are some of the lessons that could be derived from the Mexican experience? Hazera's paper discusses the history and legal basis of Mexican financial groups. On the basis of various stock market and financial statement data, an examination is also made of the groups’ evolution from 1991 to 1994. Eugene M. Salorio and Thomas L. Brewer consider, in “Expanding the Levels of Analysis of FDI for Improved Understanding of Policy issues: The Case of Mexico,” both macro-, and micro-level shifts of analysis which mutually complement one another, and yield, for example, a “components profile” of disaggregated national level FDI flows which depends on the type of the project. The authors identify far reaching implications for public policy that may be extrapolated from the case of Mexico to the new business environment faced by the Latin American countries. Francis A. Lees suggests also, from another angle, that the crisis of December 1994 could have been avoided because the financial disequilibrium was clearly evident by mid-1994 just be looking at Mexico's GDP and balance of payments. C. Bulent Aybar, Riad A. Ajami, and Ma Collapse"
102457,1990,inquiry,monitoring the impact of medicare's physician payment reform,monitoring the impact of medicares physician payment reform,ERR
102458,1990,inquiry,urban-rural differences in medicare physician expenditures,urban rural differences in medicare physician expenditures,"Policymakers have long been concerned with urban-rural disparities in access to health care. These disparities may be particularly severe in the case of the elderly and others covered by Medicare. Descriptive tables show that the total volume of physician services provided to rural beneficiaries is more than 40% lower than the volume of physician services provided to urban beneficiaries. This result is fairly consistent across all types of care and sites of care. In our econometric analysis, we investigate the factors that may explain these differences in utilization. The results indicate that, with prices held constant, variations in demographic and economic characteristics are not the major reasons for the urban-rural gap. Differences in hospital and physician (particularly specialist) availability appear to be the main factors. Collapse"
102459,1990,inquiry,how have pps changes affected allocation of medicare spending for hospital care?  a case study of new york state,how have pps changes affected allocation of medicare spending for hospital care a case study of new york state,"Complex national factors went into the development of key policies of the federal prospective payment system, and the effects of these policies varied in different parts of the country. One state particularly affected by these changes, and for the most part in a positive way, was New York. This paper focuses on the Medicare PPS policy changes and their impact nationally. An analysis of the experience of New York state, which had been under a stringent hospital cost containment system before PPS, provides a laboratory to understand how key federal policies affected different types of hospitals in that state as well as nationally. Collapse"
102460,1990,inquiry,the lifetime cost of injury,the lifetime cost of injury,"The evolution of fatal fighting was investigated using evolutionary game theory. General considerations about the lifetime consequences of fighting behaviour are first discussed. A sequential assessment game is then developed in which a fight can end because a contestant either gives up or is killed. The results show that the balance between the value of the contested resource and the value of the future is one important factor influencing the frequency of death or severe injury in fights. When the value of a contested resource is similar to or greater than the value of the future, strategies will evolve that result in contestants being severely injured or killed during fights. When the value of the future is close to zero contestants will never give up after starting to fight; such fights will always be fatal for at least one of the opponents. Factors that could decrease the cost of fighting, such as the assessment of fighting ability, kinship and role asymmetries, have little or no effect in these situations. Finally, empirical data on fatal fighting are reviewed and compared with theoretical predictions. Data show that dangerous fighting, often resulting in severe injuries or death, generally occurs in situations where a major part of a contestant's lifetime reproductive success is at stake. Empirical data on fighting behaviour clearly show that the cost of fighting varies greatly between species. In most species the frequency of severe injury is small or even zero. However, in some, really dangerous fighting does occur in which a significant proportion of the fights end with one or both of the contestants being severely injured or killed (for references, see Discussion), How can this variation between species be explained? Recent theoretical studies of fighting behaviour have focused on non-dangerous fighting (e.g. Maynard Smith & Price 1973; Maynard Smith 1982) and today we know of several mechanisms that give rise to evolutionarily stable strategies that effectively limit the use of dangerous behaviour patterns during fights. One such mechanism is the assessment of fighting ability, which allows the weaker individual to give up before being injured. In contrast, less attention has been directed towards explaining cases of fatal fighting (but see Hamilton 1979; Treisman & Collins 1980; Thornhilt & Alcock 1983; Grafen 1987). One possible reason for fatal fighting is that species differ in their ability to inflict injuries such that fatal fights occur only between animals equipped with efficient weapons. However, the fact that many well-armed species do not fight to the bitter end suggests that a more strategic explanation must apply. This is further supported by the fact that fatal fights tend to occur only when the value of victory is high. Our aim in this paper is to investigate the 0003-3472/90/010001+09 $03.00/0 evolution of fatal fighting using evolutionary game theory. First, we investigate under what circumstances fatal fights may evolve, and second, theoretical examples (evolutionary stable strategies, ESSs) of such fights are studied using the sequential assessment game (Enquist & Leimar 1983, 1987; Leimar & Enquist 1984). WHEN WILL FATAL FIGHTING EVOLVE? To answer this question it is necessary to investigate the lifetime consequences of fighting behaviour. Let us study a simple model. Consider an individual in a particular population (with some specified distribution of strategies). Let its expected lifetime utility be V+ V0 if it is in the possession of a resource, and/I0 if it is not. We interpret V as the value of the resource and V0 as the value of the future. When facing a conflict over a resource, an individual using a strategy S has an expected lifetime utility Ut given by U,(S)=p(S)V+[1 -q(S)]Vo (1) where p(S) is the probability of victory, and q(S) is a factor controlling how the value of the future (II0) changes due to fighting. Incidences of injury or expenditures of energy and time make q positive for cases of biological interest (for instance, q could be the probability of fatal injury), The situation is sometimes more complex than described by this 9 1990 The Association for the Study of Animal Behaviour Collapse"
102461,1990,inquiry,relating health care market characteristics to the effectiveness of utilization review programs,relating health care market characteristics to the effectiveness of utilization review programs,"This paper presents an exploratory analysis of the relationships between several health care market characteristics and the effectiveness of utilization review programs directed at controlling hospital use and expenditures. The analysis is based on claims data from 223 insured groups over 12 quarters. Results indicate that UR is associated with larger reductions in utilization and expenditures in markets with low HMO enrollment, high admissions per capita, and low hospital occupancy rates. These results suggest that health care market conditions that permit unnecessary use are the ones under which UR programs can be most effective. Collapse"
102462,1990,inquiry,acute care hospital utilization under canadian national health insurance:  the british columbia experience from 1969 to 1988,acute care hospital utilization under canadian national health insurance the british columbia experience from 1969 to 1988,"This paper uses hospital separation abstracts to assess trends in acute care hospital utilization in British Columbia over the first 18 years of publicly funded health insurance in the province. Between 1969 and fiscal year 1987-88, the overall separation rate decreased by 16%, accompanied by a 23% decrease in average length of stay. For the elderly, the separation rate increased by 14% and three quarter of this increase was for surgical procedures, mostly new high-technology procedures. For the nonelderly, separation rates decreased by 25%. Lengths of stay decreased in both age groups. Over the last two decades overall separation rates in British Columbia were higher than or equal to separation rates in the United States, and lengths of stay were consistently higher in British Columbia. Since access to hospitals by the elderly is similar in the two countries, lower hospital costs in Canada result from factors other than lower overall hospital utilization or decreased access for the elderly. Collapse"
102463,1990,inquiry,the minnesota project:  a focused approach to ambulatory quality assessment,the minnesota project a focused approach to ambulatory quality assessment,"With national HMO quality assurance requirements pending for Medicare risk contracts, three HMOs in Minnesota established a working group with the state Department of Health to develop and test a new methodology proposed for quality of care review. A two-tiered system was developed for ambulatory chart review based on 15 hospitalization diagnoses having a potential for inadequate prehospital care. This system was applied to 796 cases from the HMOs (2% of admissions). Technical problems limited actual review to 673 of these cases. Although 304 (45%) of reviewed cases failed initial screening, physician review found only 22% of such failures (10% of reviewed cases) to represent probable quality of care problems. The approach appears to be feasible and unusually efficient. Although there is considerable variability that limits its potential use for interhealth plan comparison, the approach holds promise for quality assurance within an individual health plan. Collapse"
102464,1990,inquiry,multiple choice health insurance:  the lessons and challenge to employers,multiple choice health insurance the lessons and challenge to employers,"This paper offers a second opinion on the issues discussed by Stanley B. Jones in his paper, ""Multiple Choice Health Insurance: The Lessons and Challenge to Private Insurers"" in the Summer 1990 issue of Inquiry. Multiple choice of health plans is not containing costs of health care or insurance premiums because employers have not yet tried price competition with cost-conscious consumer choice. HMOs in multiple choice arrangements have not saved employers money because of the way employers manage competition. Effective management of competition must be an active process employing an array of tools to create incentives that reward production of high quality economical care. Collapse"
102466,1990,inquiry,utilization trends before and after pps,utilization trends before and after pps,"This article compares utilization trends for inpatient care before and after the introduction of Medicare's Prospective Payment System (PPS). Using discharges from a constant cohort of 419 hospitals over an eight-year period, I examine Medicare and non-Medicare utilization trends for the variables: total discharges, average length of stay (ALOS), average preoperative length of stay, average postoperative length of stay, percentage of patients using either ICU or CCU, and percentage of patients with consultations. Admission declines and the initial drop in ALOS are consistent with the initial program objectives and expectations. Post-PPS increases in ALOS, especially postoperative ALOS, are not consistent with PPS goals. Use of consultants and ICU/CCU facilities is not different before and after PPS. Collapse"
102467,1990,inquiry,how the medicare prospective payment system affects psychiatric patients treated in short-term general hospitals,how the medicare prospective payment system affects psychiatric patients treated in short term general hospitals,"This study's purpose was to monitor changes in hospital utilization and discharge patterns for a large national sample of Medicare psychiatric patients treated in short-term general hospitals from 1980 through 1987. We compare data on the Medicare sample with trends for psychiatric patients with Blue Cross or private health insurance who were treated in the same hospitals. Study results indicate that treatment patterns changed for the Medicare psychiatric patients. Average length of stay decreased, especially for patients treated in scatterbeds. The proportion of patients discharged to other hospitals increased, and the number of admissions dropped for those patients over age 75. Collapse"
102481,1990,international economic review,regulating without cost information:  a comment,regulating without cost information a comment,"Abstract This paper extends our companion paper [Part I - Laffont and Tirole (1990)] on the regulation of the rate of return and prices of a multiproduct firm in two directions. First, it studies optimal pricing by a regulated firm competing with regulated or unregulated rivals, with or without market power or other distorted pricing. It then focuses on the access pricing problem and its potential foreclosure effects. Second, it shows how optimal pricing can be decentralized to the regulated firm in the absence of cost and demand information. Collapse"
102482,1990,international economic review,regulating without cost information:  further observations,regulating without cost information further observations,"Abstract This paper extends our companion paper [Part I - Laffont and Tirole (1990)] on the regulation of the rate of return and prices of a multiproduct firm in two directions. First, it studies optimal pricing by a regulated firm competing with regulated or unregulated rivals, with or without market power or other distorted pricing. It then focuses on the access pricing problem and its potential foreclosure effects. Second, it shows how optimal pricing can be decentralized to the regulated firm in the absence of cost and demand information. Collapse"
102495,1990,international journal of forecasting,the role of judgment in macroeconomic forecasting accuracy,the role of judgment in macroeconomic forecasting accuracy,"Abstract This paper presents evidence on the role that judgmental adjustments play in macroeconomic forecast accuracy. It starts by contrasting the predictive records of four prominent forecasters who adjust their models with those of three models that are used mechanically. The adjusted forecasts tend to be more accurate overall, although important exceptions can be found. Next the article compares adjusted forecasts with those generated mechanically by the same model. Again, with some significant exceptions, judgmental adjustments improve accuracy more often than not. The article closes by considering whether macroeconomic forecasters should place more or less emphasis on their adjustments relative to their models. It finds a clear tendency for modelers to overadjust their models, illustrating what prominent psychologists have termed “the major error of intuitive prediction”. In short, model builders should not hesitate to adjust their models to offset their limitations but should also guard against the tendency to overestimate the value of their personal insights. Collapse"
102503,1990,international journal of forecasting,"the accuracy of oecd forecasts of the international economy: demand, output and prices",the accuracy of oecd forecasts of the international economy demand output and prices,"Abstract This paper examines the accuracy of forecasts of the international economy made by the OECD. Our large data set, comprising over 7,000 pairs of forecasts and outcomes, includes one-, two-, and three-step ahead semi-annual forecasts of the main components of demand, output and prices for Canada, France, Germany, Italy, Japan, the U.K. and the U.S.A. over the twenty-year period 1968–1987. Various measures of accuracy are computed; also a comparison is made with competing naive and time-series predictions. The analysis includes a full range of diagnostic checks on forecast performance, including rationality tests for unbiasedness, efficiency and consistency. Although there is considerable variation in the accuracy of these forecasts, they are generally superior to the naive and time-series predictions. Error is predominantly non-systematic. However, our analysis exposes exceptions, particularly forecasts of government consumption, and in some of the forecasts of fixed and inventory investment, the foreign balance and inflation. Accuracy in these cases could be improved by a simple linear correction, or by incorporating information contained in recent, known forecast errors. At least half the OECD forecasts fail one or more of the rationality tests. Collapse"
102588,1990,journal of accounting research,contingent fees for audit firms,contingent fees for audit firms,"In this paper, we examine how auditors and their clients respond to the introduction of report-contingent audit contracts in a model of the audit market. Such contracts might appear inherently undesirable because they seem to compromise auditors' independence. Nevertheless, auditors in the United Kingdom are permitted implicitly to accept such contracts, because they can take an ownership interest in the firms they audit. In 1988, the U.S. Federal Trade Commission (FTC) attempted to pressure the AICPA into allowing contingent fee contracts for all services (including audits), although the FTC subsequently compromised by insisting on allowing contingent fees only for nonaudit work.1 This controversy surrounding the desirability of contingent audit fees motivates our study. In our model, auditors affect the value investors assign to a firm by reporting an estimate of the firm's earnings. Investors care about bans Collapse"
102589,1990,journal of accounting research,the effects of time pressure and audit program structure on audit performance,the effects of time pressure and audit program structure on audit performance,"This paper addresses three questions. First, in auditing tests of details, do effectiveness and efficiency increase as time pressure is imposed? Second, does adding structure increase effectiveness, efficiency, and consistency? Third, are the effects of structure on audit effectiveness and efficiency constant across increasing time pressure? While psychology research relates to the independent performance effects of time pressure and program structure, both pressure and structure are imposed, for legal and economic reasons, in the audit environment (Cushing and Loebbecke [1986]). Therefore, this research attempts to determine whether and how the joint imposition of program structure and time constraints affects auditors' performance. One hundred seventy-nine staff auditors from a national public accounting firm participated in an experiment that involved implementing a partial year-end audit program testing details of inventory, a task which is an analogue of those normally performed by auditors under time pressure. Four levels of time pressure and two types of audit programs (structured and unstructured) were manipulated between subjects. Re- Collapse"
102591,1990,journal of accounting research,the predictive ability of geographic segment disclosures,the predictive ability of geographic segment disclosures,No Result.
102592,1990,journal of accounting research,corporate compliance with debt covenants,corporate compliance with debt covenants,"In this paper, I present and test a model of corporate compliance with the debt covenant requiring payment of a current obligation. This study adds to the empirical literature on forecasting financial distress by showing conditions under which debt default may be a rational economic decision by the firm's shareholders. This analysis departs from empirical financial distress studies which are often not linked to an underlying economic theory. The focus on covenant default also extends the literature which has modeled bankruptcy (Bulow and Shoven [1978]) and liquidation decisions (Titman [1984]) but which has not considered a common first symptom of financial distress-covenant default. Briefly, the model examines the costs that influence shareholders' decision to comply with or violate a particular debt covenant: payment of a current obligation. These costs include those associated with bondholders' reaction to possible default. The model predicts that shareholders will default when, holding other factors constant, (i) interest rates have declined relative to coupon rates stated in the debt issue; (ii) the probability of paying future obligations is high; and (iii) the cost of obtaining funds to pay the obligation is high. The model also predicts when stockholders and bondholders will renegotiate the debt contract and the amount of wealth transferred in the renegotiation. Finally, the Collapse"
102593,1990,journal of accounting research,experience and the ability to explain audit findings,experience and the ability to explain audit findings,No Result.
102594,1990,journal of accounting research,the effect of experience on auditors' memory errors,the effect of experience on auditors memory errors,"This paper reports results from a study of the effects of audit experience on the frequency of two types of memory error: failure to integrate and reconstruction. Failure to integrate is defined here as failure to make mental connections between separately received pieces of information; reconstruction is defined here as altering the mental representation of information to make it consistent with existing knowledge (or memories). I asked auditors to review model work papers containing contradictions they would have discovered in the absence of reconstruction or failure to integrate. Subjects were able to refer freely to the experimental materials during their reviews, as they could do on actual audit engagements during the work paper review process. Since the work papers related to several audit tests and took about one hour to review, subjects had to use their memories to relate critical observations made earlier to later pieces of evidence when attempting to formulate an accurate representation of the meaning of all the evidence taken together, including detection of the contradictions. I found that subjects at all levels of experience made memory errors, and that both types of memory error were related to experience. Inex- Collapse"
102602,1990,journal of the american statistical association,reporting delays and the incidence of aids,reporting delays and the incidence of aids,"To explore recent patterns in the HIV epidemic in young gay and bisexual men, we analyzed national AIDS surveillance data for men who have sex with men (MSM) 13 to 25 years of age. Estimates of annual AIDS incidence were calculated by adjusting the surveillance data for reporting delays, unreported HIV risks, and the 1993 change in the AIDS case definition. Between 1990 and 1995, estimated AIDS incidence in young MSM declined 29%, from 1400 to 1000 cases; however, trends in incidence varied greatly by race/ethnicity. Annual AIDS incidence decreased 50% in whites during this period (from 720 to 360), but fell just 2% in blacks (from 430 to 420) and rose 5% in Hispanics (from 220 to 230). Trends in incidence also differed by metropolitan statistical area (MSA) size. Between 1990 and 1995, AIDS incidence declined 37% in MSAs with populations of 2,500,000 people or more and 28% in MSAs with 1,000,000 to 2,499,999 people, but incidence decreased only 15% in MSAs with 500,000 to 999,999 people, and 13% in MSAs with 50,000 to 499,999. Incidence in nonmetropolitan (i.e., rural) areas was unchanged. Young black and Hispanic MSM now account for most young MSM with AIDS, and incidence in young MSM in small MSAs and rural areas has been relatively constant. Trends in AIDS incidence indicate that levels of HIV infection have remained persistently high in certain populations of young MSM, underscoring the immense need for HIV prevention programs targeted specifically toward these young men. Collapse"
102604,1990,journal of the american statistical association,the use of intentions data to predict behavior:  a best-case analysis,the use of intentions data to predict behavior   a best case analysis,"Abstract In surveys individuals are routinely asked to predict their future behavior, that is, to state their intentions. This article studies the relationship between stated intentions and subsequent behavior under the “best-case” hypothesis that individuals have rational expectations and that their responses to intentions questions are best predictions of their future behavior. The objective is to place an upper bound on the behavioral information contained in intentions data and to determine whether prevailing approaches to the analysis of intentions data respect the bound. The analysis focuses on the simplest form of intentions questions, those that call for yes/no predictions of binary outcomes. The article also discusses “forced-choice” questions, which are distinct from, but are sometimes confused with, intentions questions. A primary lesson is that not too much should be expected of intentions data. It is shown that intentions data bound but do not identify the probability that a person will behav... Collapse"
102605,1990,journal of the american statistical association,lognormal and moving window methods of estimating acid deposition,lognormal and moving window methods of estimating acid deposition,"Abstract The deposition of heightened levels of sulfuric and nitric acid through rainfall in the United States may adversely affect the environment. For example, soils may become toxic to native tree species because of soil acidification. Ecological effects models being built to study these potential problems have a need for regional deposition estimates with associated measures of uncertainty. However, statistical estimation of the deposition process is complicated by a strong spatial trend (mean nonstationarity) and apparently a spatial covariance structure dependent on location (covariance nonstationarity). The available data for calculating deposition estimates consist of several hundred point observations at irregularly spaced sampling locations across the United States. The spatial estimation technique of kriging is the foundation of four deposition estimation methods evaluated in this study. These are lognormal kriging with a single model of the spatial covariance structure (the variogram); single-... Collapse"
102610,1990,journal of the american statistical association,the adjoint projection pursuit regression,the adjoint projection pursuit regression,"Abstract Consider a projection pursuit regression model y = g(α + β x) + e, with an arbitrary monotonic link function g. We assume the link function is unknown; thus we can only estimate the direction of β, that is, the ratios among the components of β, say, β 1/β 2. The direction of β is a useful estimand that measures, say, how many units of x 2 is equivalent to one unit of x 1 in terms of potency in affecting the outcome y. Projection pursuit regression usually solves the equation cov(x, y − E(y | β x)) = 0. As an alternative, we propose to solve the adjoint equation cov(x − E(x | β x), y) = 0. The adjoint equation has the advantage that E(x | β x) might be easier to estimate than E(y | β x). We establish two main results for the population case. First, β is the unique solution (up to a multiplicative scalar) to the adjoint equation. Second, β satisfies a fixed-point property based on a modified least squares regression derived from the adjoint equation. We apply the population results to two important... Collapse"
102611,1990,journal of the american statistical association,bootstrap test for difference between means in nonparametric regression,bootstrap test for difference between means in nonparametric regression,"Abstract A bootstrap test is proposed for detecting a difference between two mean functions in the setting of nonparametric regression. Error distributions in the regression model are permitted to be arbitrary and unequal. The test enjoys power properties akin to those in a parametric setting, in the sense that it can distinguish between regression functions distant only n −1/2 apart, where n is the sample size. It has exceptional level accuracy, with level error of only n −2, and uses a very accurate estimate of the critical point of an exact test, being in error by only n −3/2 under the null hypothesis. The test admits several generalizations, for example to the case of testing for differences between several regression means. (This is a nonparametric regression analog of analysis of variance.) A simulation study using n as small as 15 corroborates the asymptotic result on level accuracy of the bootstrap test. Applications are illustrated with an example involving acid rain data. Collapse"
102612,1990,journal of the american statistical association,adaptive cluster sampling,adaptive cluster sampling,"Abstract In many real-world sampling situations, researchers would like to be able to adaptively increase sampling effort in the vicinity of observed values that are high or otherwise interesting. This article describes sampling designs in which, whenever an observed value of a selected unit satisfies a condition of interest, additional units are added to the sample from the neighborhood of that unit. If any of these additional units satisfies the condition, still more units may be added. Sampling designs such as these, in which the selection procedure is allowed to depend on observed values of the variable of interest, are in contrast to conventional designs, in which the entire selection of units to be included in the sample may be determined prior to making any observations. Because the adaptive selection procedure introduces biases into conventional estimators, several estimators are given that are design unbiased for the population mean with the adaptive cluster designs of this article; that is, the ... Collapse"
102614,1990,journal of the american statistical association,inference for near-integrated time series with infinite variance,inference for near integrated time series with infinite variance,"Abstract An autoregressive time series is said to be near-integrated (nearly nonstationary) if some of its characteristic roots are close to the unit circle. Statistical inference for the least squares estimators of near-integrated AR(1) models has been under rigorous study recently both in the statistics and econometric literatures. Although classical asymptotics are no longer available, through the study of weak convergence of stochastic processes, one can establish the asymptotic theories in terms of simple diffusion processes or Brownian motions. Such results rely heavily on the finiteness of the variance of the noise. When this finite variance condition fails, whereas many physical and economic phenomena are believed to be generated by an infinite variance noise sequence, the aforementioned asymptotics are not applicable. In this article, a unified theory concerning near-integrated autoregressive time series with infinite variance is developed. In particular, when the noise sequence {e t } belongs to... Collapse"
102615,1990,journal of the american statistical association,bias of autoregressive spectral estimators,bias of autoregressive spectral estimators,"Abstract Bias of the least squares estimator of the log of the spectral density of an autoregression attenuates the peaks of the estimator. Under the assumption of an autoregressive generating process of known finite order, we obtain an expression for the order 1/T bias, where T is the sample length of the observed series. This approximation is a sum of several simple functions of the unknown coefficients. When the spectral density has sharp peaks, one of these functions dominates the bias. The attenuation from this dominant component can be substantial when the spectral peak is well defined, and several examples illustrate this effect. Since the integral of the order 1/T bias components that are frequency dependent is 0, unbiased estimation of entropy to this order is possible for autoregressive processes. These bias expressions extend to autoregressive models in which the mean is a polynomial function of time. Similar results obtain for the log of the Yule-Walker spectral estimator, for which the order ... Collapse"
102616,1990,journal of the american statistical association,on bootstrap iteration for coverage correction in confidence intervals,on bootstrap iteration for coverage correction in confidence intervals,"Abstract Iterated bootstrap procedures may be used to reduce error in many statistical problems. We discuss their use in constructing confidence intervals with accurate coverage and show that bootstrap coverage correction produces improvements in coverage accuracy of order n −1/2 in one-sided intervals, but of order n −1 in two-sided intervals. Explicit formulas are provided for the dominant term in coverage error after iteration in each case. These results are used to compare various iterated bootstrap intervals and to assess the effect of bootstrap iteration on other indicators of interval performance, such as position of critical points and length of interval. We show that, for one-sided intervals, the coverage-correction algorithm yields critical points that are second-order correct. This is not the case for two-sided intervals, where second-order correctness is not crucial in obtaining high-order coverage accuracy. We also show that the asymptotic mean increase in length between the original and cove... Collapse"
102618,1990,journal of the american statistical association,estimation following a sequentially designed experiment,estimation following a sequentially designed experiment,"Abstract We investigate the design of experiments for the nonparametric estimation of the root of an unknown regression function. Approaches to this problem include the Robbins-Monro (1951), Venter (1967), and Lai-Robbins (1979, 1981) stochastic approximation procedures and Wu's (1985, 1986) sequential maximum likelihood estimators. Because the regression function is not assumed to belong to a parametric family, only experimentation near to the root is informative and the sequential design should converge to the root. After the sequential design has been generated, there are at least two distinct methods for estimation of the root: (a) estimate the root by the last design point, or (b) fit a parametric model. Except for Ruppert (1988) and simulation studies by Bodt (1985) and Bodt and Tingey (1990), all studies known to us of stochastic approximation procedures used the last design point as the estimator. Wu (1985, 1986) fit a generalized linear model. When the last design point is the estimator, then cle... Collapse"
102722,1990,journal of consumer research,reinterpretation of mere exposure or exposure of mere reinterpretation?,reinterpretation of mere exposure or exposure of mere reinterpretation,"n his article entitled ""The Logic of Mere Exposure: A Reinterpretation . . . ,"" Timothy B. Heath (1990) provides a useful tutorial on the difference between moderation and mediation (Baron and Kenny 1986; Brown 1989). Heath illustrates this distinction as it pertains to the research on mere exposure effects via the reinterpretation of a study reported by Anand, Holbrook, and Stephens (1988). By means of this discussion, he claims to show that Anand et al.'s empirical work ""unwittingly supported"" a conclusion that runs ""contrary"" to that intended. Specifically, he argues that the study by Anand et al. is ""more consistent"" with an independence hypothesis (in which affect may occur without cognition) than with a cognitive-affective model (in which affect depends on some kind of cognitive mediation). When Anand et al. (1988, p. 386) say that they ""seek evidence for cognitive mediation in the formation of affect,"" Heath assumes that such ""cognitive mediation"" must involve a process wherein objective familiarity (OF) -* cognition or subjective familiarity (SF) -* affect (as shown, e.g., in his Table 1). Thus, drawing on his Figure 1, Heath represents Anand et al. as concluding ""that OF causes cognition (SF), which in turn causes affect"" (1990, p. 242, repeated verbatim on p. 243) and proceeds to point out that ""this reasoning . . . suffers from two distinct shortcomings"" (p. 242). The second of these ""shortcomings"" hinges on a claim that the authors ""confuse tests of moderation (OF by SF interaction) with those of mediation (OF -* SF -* evaluation)"" (p. 243). We share this concern for the difference between mediation and moderation, and we acknowledge that Heath has identified an unfortunate looseness in our use of the term ""cognitive mediation."" However, though Heath's critique does distinguish correctly between moderation (as in ANOVA) and mediation (as in path analysis), it fails to note that Anand et al. used the term ""cognitive mediation"" in a more general sense to refer to processes of cognition that underlie the formation of affect. We now regret this somewhat casual use of the term ""mediation."" However, we did not anticipate that readers might construe our ANOVA approach from the viewpoint of path analysis. The reason we failed to anticipate this possibility is that it makes no sense to suggest that a straightforward ANOVA such as ours, in which some dependent variable (C) is affected by two factors (A and B) and their interaction (A X B), can permit any sort of meaningful interpretation as a path analysis in which A --BC or A -A X BC. For this reason, it never occurred to us to try to interpret our results via a path-analytic framework. Yet Heath represents our analysis as concluding that OF -* SF -* affect. In actuality, we never intended any such thing. Rather, we clearly indicated that ""the objective familiarity effect was moderated [i.e., qualified] by a significant objective X subjective familiarity interaction"" (1988, p. 389). Our explicit use of the term ""moderated"" (meaning ""qualified"") should have discouraged any critique based on ""mediation"" in the path-analytic sense. Indeed, we never claimed a path-analytic OF-SF-affect mediation of the type that Heath describes (and could not have done so without committing gross errors in logic). If one were to insist on adopting a path-analytic view of our study (a perspective that we definitely do not recommend), one would need to construct a model of the following form: Collapse"
102744,1990,journal of development economics,"child survival, height for age and household characteristics in brazil",child survival height for age and household characteristics in brazil,"""The impact of household characteristics on child survival and height, conditional on age, is examined using household survey data from Brazil. Parental education is found to have a very strong positive effect on both outcomes and this is robust to the inclusion of household income and also parental heights, which partly proxy for unobserved family background characteristics. We find that income effects are significant and positive for child survival but insignificant for for child height although the latter depends on identification assumptions. Parental height has a large positive impact on child height and on survival rates even after controlling for all other observable characteristics."" Collapse"
102745,1990,journal of development economics,"property rights, externalities, and resource degradation: locating the tragedy",property rights externalities and resource degradation locating the tragedy,"Abstract Resource degradation in the Third World is largely driven by the demands of farm households for fuelwood and land for agriculture. Since resources are often controlled through indigenous systems of property, the tragedy of the commons has been used to explain resource degradation. As a result, private property is suggested as a solution to resource degradation. A dynamic model capable of examining household incentives for resource use under private and common property is developed. Results of the model reject the conventional wisdom that gives rise to the presumed optimality of private (individual) property in natural resources, and the correlated indictment of group management regimes. Collapse"
102746,1990,journal of development economics,adoption of high yielding rice varieties in bangladesh:  an econometric analysis,adoption of high yielding rice varieties in bangladesh an econometric analysis,"Abstract In this paper we build two logistic type econometric models to explain HYRV diffusion rate in Bangladesh. Long-run potentials (ceiling), diffusion rates, and the effects of other economic variables on the adoption path are determined simultaneously within the model. Results from our final model indicate that the diffusion rate is not constant over time. Furthermore, rate and level of adoption are found to be influenced by flood damage, jute-rice price ratio and HYRV—local rice variety price ratio. An important outcome of our analysis is that the ceiling adoption level for Bangladesh has nearly been reached. Unless new HYRVs are developed with wider adaptability, especially for drought and flood prone areas, little scope exists for production increases through HYRV acreage expansion. This conclusion has significant policy implications for agricultural planners and development agents in Bangladesh. Collapse"
102747,1990,journal of development economics,rural-urban migration and the transition from traditional to modern agriculture,rural urban migration and the transition from traditional to modern agriculture,"Abstract This article analyzes a salient feature of LDCs, namely that of the coexistence of traditional and modern agricultural techniques during economic development. A three-sector, two-commodity general equilibrium model is specified and migration takes place from backward agriculture to a modern agricultural and a modern manufacturing sector. The consequences on urban unemployment, welfare, size of the backward sector etc. of general and selective labor and capital subsidies and a migration tax are clarified. A labor subsidy does not, under any factor intensity assumptions, have an unambiguously decreasing effect on unemployment. A tax on the urban labor force lowers unemployment and raises welfare. Collapse"
102748,1990,journal of development economics,"patterns of productivity growth in south korean manufacturing industries, 1963-1979",patterns of productivity growth in south korean manufacturing industries  1963 1979,"Abstract In this paper we estimate sources of labor productivity growth in 25 Korean manufacturing industries between 1963 and 1979. We find that less than half of the 11 percent annual increase in overall manufacturing labor productivity can be attributed to capital deepening, and that the importance of this factor and total factor productivity advance varied sharply across industries. Heavy industries accumulated capital per worker at a faster pace, and realized total factor productivity growth at a much slower rate, than others did. This contrast may be related to the extensive capital subsidies provided to the former as part of an import-substitution program. Also, the rapid total factor productivity growth in labor-intensive manufacturing industries was accompanied by rapid growth in average firm size, supporting the argument that the shift from craft to modern production techniques has been a major source of productivity growth. Collapse"
102754,1990,journal of development economics,food security policy in a stochastic world,food security policy in a stochastic world,"This study investigates the impact of agricultural policy on reducing the gap and ways to achieve food security in Iraq for the period (1990-2015). This study examines the impact of agricultural policy on reducing the gap and ways to achieve food security in Iraq for the period 1990-2015. The study of both red meat and poultry meat was considered as a major part of the community's food security. Two models were used in the analysis: the production policy model and the food gap model. In the production policy analysis, production was used as a dependent factor, consumption, And the coefficient of protection as independent factors. As for the food gap model, it was used as a dependent factor, production, local price for previous year, world price for previous year and consumption as independent factors. The regression function was applied to linear, logarithmic, logarithmic, and halved logarithmic variants. During its preference in terms of statistical and standard tests and their conformity to the logic of economic theory. The results of the analysis of the production policy of red meat and poultry meat showed that the consumption and import variable had   a significant effect, See the preparation of the population and the coefficient of protection was their influence in the moral red meat and insignificant in poultry meat. Collapse"
102755,1990,journal of development economics,on the coverage of public employment schemes for poverty alleviation,on the coverage of public employment schemes for poverty alleviation,"Abstract The cost-effectiveness of public employment as a means of poverty alleviation depends on policy design choices concerning coverage and wage rates. The choice is between schemes which aim for wide coverage at potentially low wages and schemes which ration participation so that more beneficiaries can escape poverty. Conditions are derived for ranking stylized policy alternatives in terms of a broad class of poverty measures. The preferred policy is shown to depend on the budget level, administrative cost, the initial wage distribution, and the policy maker's aversion to poverty. Empirical simulations for Bangladesh data generally support wide coverage for distributionally sensitivity poverty measures but limited coverage for other measures. Collapse"
102756,1990,journal of development economics,the interactive effects of mother's schooling and unsupplemented breastfeeding on child health,the interactive effects of mothers schooling and unsupplemented breastfeeding on child health,"Abstract This paper estimates a non-linear relationship between duration of breastfeeding and child height-for-age. For this sample of children, health benefits from unsupplemented breastfeeding differ by mother's education, with children of less educated mothers deriving the most gains. These results suggest that more educated mothers are able to provide wholesome substitutes to breastmilk without producing ill effects. Results are sensitive to the estimation technique used, with 2SLS estimates showing statistical significance and substantial health gains compared to OLS estimates. Collapse"
102758,1990,journal of development economics,"capital accumulation, income distribution and endogenous fertility in an overlapping generations general equilibrium model",capital accumulation income distribution and endogenous fertility in an overlapping generations general equilibrium model,"A study is conducted in attempts to increase the understanding of the links between macroeconomic effects and causes of population growth in formulating policy. An overlapping generations general equilibrium model is employed aggregating household decisions about fertility, savings, and investment in the human capital of children with the objective of studying intertemporal relationships among population growth, income distribution, inter-generation social mobility, skill composition of the labor force, and household income. As a result of endogenous fertility, the equilibrium path attains steady state from the second generation. Income tax transfer, child taxation, and social security taxation policies are also examined in the paper. A structural explanation is given for the inverse household income-child quantity and negative child quality-quantity relationships seen in developing countries. In a Cobb-Douglas economy, these relationships hold in the short-run, potentially working over the long-run in other economies. Overall, the model shows that group interests may hinder emergence of perfect capital markets with private initiatives. Where developing countries are concerned, these results have strong implications for population policy. A policy mix of building good quality schools, or subsidizing rural education, introducing a formal social security program, and providing high-yield, risk-free investments, banking, and insurance services to the poor is recommended. Collapse"
102759,1990,journal of development economics,"transactions costs, the size of firms and industrial policy: lessons from a comparative case study of the footwear industry in korea and taiwan",transactions costs the size of firms and industrial policy lessons from a comparative case study of the footwear industry in korea and taiwan,"Abstract A comparative case study of the footwear industry documents the greater extent of subdivision of large orders, the greater reliance on subcontracting, and the greater roles of small manufacturers and export traders in Taiwan relative to Korea. These differences are hypothesized to reflect rational, organizationally efficient private and public decisions in the face of divergent economic conditions at the outset of export-led growth, and associated higher costs of market transactions in Korea than Taiwan. The transactions costs hypothesis is shown to have superior explanatory power to the hypotheses that differences in product mix and in policy account for firm size differences. Collapse"
102760,1990,journal of development economics,evaluating agricultural price policy under dual market regimes and institutional constraints,evaluating agricultural price policy under dual market regimes and institutional constraints,"Abstract A recursive, multi-region, multi-commodity model emphasizing interaction between official and private markets is proposed to evaluate agricultural policy reforms in Sub-Saharan Africa. Supply is estimatd by linear programming representative farm models incorporating private and official market incentives, and official procurement and input rations. Demand is represented by a modified linear expenditure system incorporating rationed official supplies. A linear complementarity problem calculates the private market equilibrium subject to official market outcomes and institutional constraints. Results from a case study of Burkina Faso show that reforms improve agricultural performance, but depend on cotton exports to pay for fertilizer. Relaxing institutional constraints, such as foreign exchange allocations for fertilizer imports and capacity of official marketing systems, will be crucial for meeting reform objectives. Collapse"
102761,1990,journal of development economics,"development, structural changes and urbanization",development structural changes and urbanization,"Abstract This paper constructs an equilibrium model by formalizing a trade-off between the gains from trade based on increasing returns to specialization and transaction costs. The relationship between development, structural changes, and urbanization is investigated. In addition, the function of a free market in searching for the efficient market structure is explored. Collapse"
102763,1990,journal of development economics,north-south trade and southern industrialization,north south trade and southern industrialization,"Abstract This paper presents a simple model of international trade and growth between a mature, industrial region (the North) and a semi-industrial, primary exporting region (the South). Under capital autarky, per capita Southern industrial output is minimized if the North is at the Golden Rule point. Regardless of whether capital is internationally mobile, Northern transfers to the South will not necessarily raise Southern industrial output unless they reach a critical level. Additional transfers may crowd Northern private investment in the South out if this critical level is not attained. The effects of changes in saving behavior and technical progress are also studied. Collapse"
102764,1990,journal of development economics,import demand and non-tariff barriers:  the impact of trade liberalization:  an application to morocco,import demand and non tariff barriers   the impact of trade liberalization   an application to morocco,"Abstract Prediction of import response following liberalization measures is a particularly arduous task when extensive quantitative restrictions on imports are present. Quotas affect the responsiveness of imports to real exchange rates, tariffs and activity levels, and the combined effects of quotas and other variables are hard to gauge when data are only available for the constrained regime. In this paper, we show that theoretical results cannot unambiguously sign the effect of quotas on import responsiveness. We also estimate on Moroccan data import demand equations that take explicit account of the presence of quotas. The results suggest that quantity restrictions had a significant impact not only on the level of imports, but on their sensitivity to income and price variations as well. By estimating the demand for foreign goods under rationing we are also able to predict the behavior of imports in response to the elimination of tariff and non-tariff barriers to trade. Collapse"
102765,1990,journal of development economics,the effects of direct foreign investment in the presence of increasing returns due to specialization,the effects of direct foreign investment in the presence of increasing returns due to specialization,"Abstract This paper studies the external effects generated by foreign capital inflows in a small open economy, modeling explicitly the nature of the increasing returns involved. The focus is on the role played by specialized, differentiated business services in manufacturing production and how foreign investment - through its ability to increase the extent of the market and induce greater specialization in producer services - enhances the productivity of nationally-owned industry and raises national welfare. The gains yielded by foreign capital inflows within this context are shown to operate whether the economy is at full-employment or under unemployment. In the latter case, it turns out to be critical that one consider not only the direct industrial employment created by foreign investment but also the induced secondary employment generated in the service sector. Collapse"
102766,1990,journal of development economics,ldc creditworthiness and foreign capital inflows:  1980-86,ldc creditworthiness and foreign capital inflows   1980 86,"Abstract This paper estimates the determinants of foreign capital inflows and shows that they are jointly determined with LDC creditworthiness. The paper demonstrates that foreign capital inflows respond to government restrictions and the cost of foreign transactions. Previous research, based on single-equation models, has concluded that the amount of capital inflows is a positive indicator of LDC creditworthiness. By contrast, this paper estimates a simultaneous limited-dependent variable model, in order to capture the endogeneity of capital inflows. The estimate yields a reversal in sign for the coefficient of the capital inflows variable. The paper suggests that defensive lending may explain this finding. Collapse"
102777,1990,journal of econometrics,the role of multiplier bounds in efficiency analysis with application to kansas farming,the role of multiplier bounds in efficiency analysis with application to kansas farming,"Abstract As long recognized, the problem of efficiency involves both technical and economic facets. Determination of the technically efficient firms provides the base for economic analysis. Values in terms of prices or costs must be introduced into the problem to work towards finding firms which might be regarded as overall efficient. That problem came to the forefront in a 1984 study to find the best site for location of a Superconducting Super Collider (SSC) in the state of Texas. Application of a modern value-free frontier method called Data Envelopment Analysis (DEA) to the data, which was primarily engineering and geological in character, showed five of the six feasible sites were technically efficient. However, additional socioeconomic/environmental data provided ‘price-cost’ inequality bounds for the mathematical multipliers in the DEA problem. Including those bounds in the analysis, which was called an Assurance Region (AR), reduced the number of efficient sites from five to one. In 1988, the U.S. Department of Energy in national competitionactually selected the site identified by the bounding method for location of the SSC. In this paper, the AR concept is defined for efficiency analysis of the linear production possibility set. As applied here to 83 farms, we use only the special case of AR consisting of separate linear homogeneous restrictions on the input and output multipliers. When applied to the technically efficient farms, the AR principles reduced the number of candidates for overall efficiency from 23 to 8 in one case (Ratio Model) and from 44 to 13 in another case (Convex Model)."
102787,1990,journal of econometrics,testing nonnested euler conditions with quadrature-based methods of approximation,testing nonnested euler conditions with quadrature based methods of approximation,"Abstract Singleton (1985) proposed a test of the validity of an Euler equation specification which incorporates information on a nonnested set of Euler equations from an alternative economic model. The information about the alternative model is introduced into the testing framework by constructing a sequence of local alternatives to the Euler equations of the maintained model which are in the direction of the Euler equations of the alternative model. Singleton's test is easy to apply, but is designed to have power against an alternative that in many cases does not have a straightforward economic interpretation. In this paper, we reexamine the construction of local alternatives to Euler equations. We compare Singleton's approach with a method based on perturbing the data-generation process (DGP) of the maintained model in the direction of the DGP of the alternative model. The second approach leads to tests which are always of interest, but require the complete specification of the DGP. In this paper, we derive a test for discriminating between two nonnested sets of Euler conditions which have been estimated us ing Generalized Method of Moments (GMM). The test is based on the Encompassing Principle of Cox (1961) and Mizon and Richard (1986), and uses Tauchen's (1986) quadrature-based nethods for approximating the expectation of nonlinear functions of stationary random variables. Singleton's empirical examples are also reconsidered. Collapse"
102800,1990,journal of economics and business,across-discipline journal awareness and evaluation: implications for the promotion and tenure process,across discipline journal awareness and evaluation  implications for the promotion and tenure process,"Abstract We surveyed publishing business scholars to assess their familiarity with journals in the various fields taught in a college of business. Our results suggest that even well-read scholars are less knowledgeable about journals outside their fields. They know the top two or three journals in other fields and perceive them to be of good quality, but for journals below that they indicate that they have “no knowledge.” There are identifiable cohort groups in some cases that are more aware of each others' journals. There are also fields whose journals were less well known than others—business law, insurance, real estate, and, most notably, management information systems. We conclude that publishing faculty tend to be specialists. They have limited knowledge of journals outside their speciality. The promotion and tenure process, however, often assumes that faculty can evaluate the publication records of faculty from other specialties. Our results raise questions as to the validity of such a presumption. Collapse"
102801,1990,journal of economics and business,a remark on competitive speculation under uncertainty,a remark on competitive speculation under uncertainty,No Result.
102807,1990,journal of economic behavior and organization,workers' preferences for co-operatives versus private buy-outs,workers  preferences for co operatives versus private buy outs,"Abstract We present some survey results on the preferences of workers for a coop as opposed to a private buy-out if faced with the closure of their workplace. Although there has been quite a lot of theoretical discussion of this issue there is relatively little empirical evidence. Although hedged about by reservations we conclude that the principal factor that determines preferences are the‘job risk’ characteristics of the coop and the privately owned firm. To a very large extent a worker will prefer the coop to a private buy-out if it leads to increased job security and if the coop is considered to be viable. We also find that workers expected that everyone would work harder in the coop and that this, together with increased shop floor control of production, was expected to lead to higher earnings. Significantly, however, these expected differences in the work/earnings trade-off between the coop and the privately owned firm did not lead workers to prefer one to the other. Collapse"
102808,1990,journal of economic behavior and organization,producer's dilemma experiments:  comment,producers dilemma experiments comment,"As announced by the current editors Wataru Sakamoto and Hiroshi Yadohisa, the English journal of JSCS “Journal of the Japanese Society of Computational Statistics” (JJSCS) closes at this issue (Volume 30, Number 2), and is going to join a new journal “Japanese Journal of Statistics and Data Science” (JJSD) that will start in 2018 as an official journal of the Japanese Federation of Statistical Science Associations (JFSSA). All of six member societies will jointly contribute to the publication of JJSD. JSCS was established in 1986. I played the role of chief editor of JJSCS since then until 1990, and was responsible for the publication of volumes 1 to 3. The major purpose of establishing JSCS was to make a progress in computational statistics in Japan by providing a forum for people working in various aspects of computational statistics, e.g., statisticians engaged in the research and application of statistical theory and methods, computer scientists/engineers engaged in the development of statistical software, and statisticians/data analysts engaged in the analysis of data obtained with surveys, experiments or other means, and provide them opportunities for exchanging information and ideas and, if possible, for finding seeds for joint works among them. For producing better products as well as for getting more valuable outcomes using the products, it is vital to know what the counterpart really wishes between the producers and the users (or consumers) of everything including statistical methods and software. In those days our major interest was in the R & D of statistical program packages (SPP) as described in President Address (JSCS Japanese journal, Volume 1, Number 1) by the first president Chooichiro Asano. I myself was interested in R & D of SPP on personal computers with my colleagues, that is useful both for the data analysis and for the research work of statistical methods. There are three remarkable traditions in JSCS. I like these traditions and think that they have contributed very much to the achievement of the purpose of establishment mentioned above. One is social gathering at the time of scientific meeting. Every time it is organized in such a way that participants can talk freely in a comfortable atmosphere for exchanging information and ideas. The second one is the tradition that president of JSCS is elected alternately from academic and non-academic communities so that both sides will be satisfied equally in a long period. The third one is that it is very positive in international activities, for example, Japan and Korea Conferences, Japan and China Symposia, and also organizing international meetings such as 25 JSCS Symposium in Busan in 2011, two international scientific meetings for 30 Anniversary in Okinawa and Seattle in 2015 and 2016. Since around a decade ago the environment of statistics has been changing due to advances in the computer and information technology, and accordingly it has strong effect on statistical science especially computational statistics. Let us briefly review what happened. Collapse"
102809,1990,journal of economic behavior and organization,producer's dilemma experiments:  reply,producers dilemma experiments reply,"When a shoal of small fish is at risk of being attacked by a pike, an individual or a small group of fish may leave the safety of the shoal and approach the predator to inspect it to obtain information about the pending risk (Magurran et al. 1985; Pitcher 1992). A pair of fish that jointly carries out a predator inspection visit is probably in a Prisoner’s Dilemma (Milinski 1987, 1990, 1992). Both fish share the risk of being eaten. As one fish proceeds, however, it must rely on the other one’s following, although this other fish may be tempted to rush back and watch the leading fish’s fate from a safe distance. Assuming a Prisoner’s Dilemma, I have interpreted my experimental results with sticklebacks, Gasterosteus aculeatus (Milinski 1987), as evidence for a Titfor-Tat-like cooperation strategy that is based on reciprocity (Axelrod & Hamilton 1981; Nowak & Sigmund 1992). This interpretation was corroborated by further experiments and observations both with sticklebacks (Milinski et al. 1990a, b; Külling & Milinski 1992; Huntingford et al. 1994) and guppies, Poecilia reticulata (Dugatkin 1988, 1991; Dugatkin & Alfieri 1991a, b). A challenging alternative interpretation has been proposed by Connor (1996): ‘a model of by-product mutualism that includes partner preferences can account for all those phenomena during predator inspection in fish that have previously been attributed to a cooperative strategy, Tit for Tat, based on the Prisoner’s Dilemma’ (page 451). West-Eberhard (1975) defined byproduct mutualism as mutualism that is maintained by ordinary selfish behaviour incidentally benefiting neighbours. Individuals may increase the by-product benefits they receive by coordinating their actions (Connor 1996), for example, by jointly inspecting a predator. How can we test which hypothesis better explains the results? Theoretically, this is easily done because the two hypotheses are mutually exclusive. By-product mutualism is not feasible if the inspecting fish are in a Prisoner’s Dilemma, whereas Tit-for-Tat reciprocity does not make sense outside a Prisoner’s Dilemma. Connor (1996) is right when he says that unequivocal evidence has not been provided for the Prisoner’s Dilemma in this situation. I have always argued that a Prisoner’s Dilemma is possible in predator inspection and that there is some evidence for it (Milinski 1987, 1990, 1992), such as the two following facts. (1) Usually small groups or single fish inspect the predator (e.g. Magurran 1990; Pitcher 1992), although joining an inspector or a group should become increasingly attractive with each additional inspector because of the risk dilution (‘safety in numbers’) and the confusion effect. This observation seems to fit the Prisoner’s Dilemma paradigm, because Tit-for-Tat cooperation in an iterated Prisoner’s Dilemma is only stable in small groups, especially in pairs (Boyd & Richerson 1988). (2) In a Prisoner’s Dilemma, the highest payoff is gained by an individual that defects when the partner cooperates. Does a fish that avoids the risk and stays behind obtain enough information to have a higher net benefit (i.e. the fitness increase from information gained minus the fitness reduction from potential risk) than the fish that proceeds on its own? Supporting evidence is provided by experimental studies with minnows, Phoxinus phoxinus (Pitcher et al. 1986; Magurran & Higham 1988). Thus, it seems more likely than not that the Prisoner’s Dilemma is fulfilled. It is a Correspondence: M. Milinski, Abteilung Verhaltensökologie, Zoologisches Institut, Universität Bern, CH-3032 Hinterkappelen, Switzerland (email: milinski@esh.unibe.ch). Collapse"
102813,1990,journal of economic behavior and organization,"an economic, organizational and behavioral model of the determinants of ceo tenure",an economic organizational and behavioral model of the determinants of ceo tenure,"Abstract This paper seeks to determine the extent to which chief executive officer (CEO) reward systems align the interests of CEOs with interests of stockholders and, therefore, counter the agency problem. The theoretical framework used differs from previous research in two major areas. First, the model focuses on the determinants of CEO tenure rather than CEO compensation, an explicit acknowledgement that most firms do not rely solely on firm performance to determine executive compensation, but rather depend on the ‘going rate’ for CEOs. Second, the model integrates the theoretical constructs from economic, organizational, and behavioral theory. This interdisciplinary model should provide a broader perspective of the environment in which CEOs operate and minimize the pitfalls associated with prior research which has generally adopted the perspective of only a single discipline. The empirical results from this study show that the power structure of an organization and certain key behavioral traits of CEOs are important in the CEO reward system, independent of firm performance. Moreover, these factors even seem to supercede the influence of corporate performance in their impact on CEO tenure. Collapse"
102850,1990,journal of economic literature,a quick refresher course in macroeconomics,a quick refresher course in macroeconomics,No Result.
102851,1990,journal of economic literature,patent statistics as economic indicators: a survey,patent statistics as economic indicators a survey,"This chapter summarizes the basic characteristics of patent data as an innovation indicator and reviews some of the recent research using patent data, focusing on major developments since Griliches in 1990 [Griliches, Z. (1990). “Patent statistics as economic indicators: A survey”. Journal of Economic Literature 28, 1661–1707]. The first notable development is the availability of patent data on an increasingly global scale and the accompanying global spread of research using patent data. The availability of global patent data has increased the value of patent information in a number of ways. The second notable development is the significant expansion of research using citation information as well as better understanding of its nature. Citation information has been found to provide very useful information on the value of patents, although backward citation as a measure of information flow is found to be more controversial. The third major development is the extensive implementation of surveys such as the “innovation survey” of firms and the inventor survey. They have deepened our understanding of the usefulness and the constraints of bibliographic indicators based on patent. The forth development is better understanding of the nature of the patent system and the reformulation of patent data, a good example of which is the development of patent family data, based on priority information. Collapse"
102854,1990,journal of economic perspectives,the labor market status of black americans:  1939-1985,the labor market status of black americans   1939 1985,"A fter substantial economic gains near the time of World War I, black Americans experienced little progress during the interwar period, 1918-1939. When Gunnar Myrdal (1944) made his famous argument that the contradiction between Americans' espoused commitment to equal opportunity and whites' actual discrimination against blacks would pose an ""American Dilemnma"" for the coming era of democracy, serious students of race relations seldom disagreed about the dismal economic position of black Americans. But since Myrdal's prediction, there have been great changes in race relations and in blacks' economic status. In this paper, I document some of these changes and I discuss some arguments that have been offered as explanations of the changes in blacks' economic status. Among the empirical changes since 1939, three stand out: 1) three decades of rapid progress followed by two decades of relative stagnation in many aggregate measures of relative black earnings and incomes; 2) declining employment of black males; and 3) heightened inequality within the black population. Students of the economics of race relations must address a fundamental question: Why has relative black employment fallen when blacks' wages and their access to occupations and to institutions of higher education have improved so much? This question leads to a consideration of the extent to which these changes may be interpreted as changes in discrimination and its consequences; changes and differences in skills; and changes in the relative wages of workers of different skills. Collapse"
102855,1990,journal of economic perspectives,the role of human capital in earnings differences between black and white men,the role of human capital in earnings differences between black and white men,"D uring the past 50 years, the earnings of black men have increased substantially both in absolute terms and relative to those of white men, as shown in Figure 1. In 1940 the black-white weekly earnings ratio among men ages 25-34 was 49 percent; by 1980 it had increased to 79 percent. During the 1980s, however, the overall black-white earnings gap did not narrow, and at younger ages the gap actually widened. Although the 1980s is not the first decade since 1940 in which black men made little or no gains in relative earnings-the 1950s was another-the lack of progress in the 1980s is nevertheless puzzling and a source of concern. This article examines the factors underlying the differential in earnings between black men and white men, with a focus on the role of human capital. Since 1940, successive generations of black men entering the labor force have been increasingly more educated relative to white men, both in terms of years of school completed and in the quality of schooling obtained. As the education and income levels of black parents increased, racial differences in the transmission of human capital in the home were also likely to have narrowed. This convergence in educational differences combined with the migration of blacks out of the South contributed to the narrowing in the racial gap in earnings over the 1940-80 period, particularly in the first half of the period. But human capital factors-education and migration-were not the only source of convergence in black-white differences in earnings. The patterns of change in the earnings gap support the contention that declining labor market discrimination against blacks has also contributed to the rise in the black-white earnings ratio. Moreover, the acquisition of human capital itself has been strongly affected by societal and governmental discrimination, since blacks were long provided with greatly inferior schooling resources. Collapse"
102856,1990,journal of economic perspectives,the impact of affirmative action regulation and equal employment law on black employment,the impact of affirmative action regulation and equal employment law on black employment,"T he federal policy of affirmative action effectively passed away with the inauguration of the Reagan administration in 1981. The Supreme Court decisions in the summer of 1989 nailed down the coffin lid.' But affirmative action has carried more symbolic than real weight, and symbols have ways of persisting even when the body of law and regulation lies six feet under. The tenth anniversary of its passing is an appropriate time to consider what it achieved during its conflicted existence, and whether its possible resurrection is to be feared or welcomed. Affirmative action is one of the most controversial government interventions in the labor market since the abolition of slavery. In recent years, two major criticisms of affirmative action have found prominent voice. The first is that affirmative action does not work; therefore, we should dispose of it. The second is that affirmative action does work; therefore, we should dispose of it. My chief concern in this paper will be with the first of these criticisms. Was affirmative action successful in increasing employment opportunities for blacks? In this paper, affirmative action will refer to the provisions of Lyndon Johnson's Executive Order 11246 in 1965, as amended by Richard Nixon's Executive Order 11375 [3 C.F.R. 169 (1974)]. This focus is distinct from affirmative action required as a remedy by judicial decision, which is not the primary focus here. Collapse"
102857,1990,journal of economic perspectives,family change among black americans:  what do we know?,family change among black americans what do we know,"T he changes in family structures of black American households over the past three decades have been remarkable. In 1960, 33 percent of black children were not living with two parents. By 1988, the figure had risen to 61 percent. During the same period, the fraction of all black children born to an unmarried mother rose from 23 percent to over 60 percent. This paper examines the patterns of family change, briefly discusses their economic implications, and explores what is known about the economic reasons for those changes. Unfortunately, there remains no clear indication of why families changed so much, in part because of the modest amount of work in this field. But the data do reveal some important hints, and our hope is that this paper may serve to inspire more and better research. Collapse"
102858,1990,journal of economic perspectives,bimetallism revisited,bimetallism revisited,"This article reviews the local and systemic effects of crush injury. Within minutes to hours after extrication of survivors trapped under fallen masonry (and immediately following decompression of limbs), a massive volume of extracellular fluid is lost into the injured muscles, leading to circulatory failure. Solutes leaking out of damaged muscles cause a spectrum of metabolic disturbances. Chief among them are hyperkalemia and hypocalcemia which, synergistically, have a lethal cardiotoxic potential, particularly in hypotensive patients. Early volume replacement, preferably already started at the rescue site, may combat shock and correct the hyperkalemia. If urine flow is established, this regimen should be followed by a forced solute-alkaline diuresis for the prevention of myoglobinuric and uricosuric acute renal failure, which is a common and ominous late complication of crush injury. Preparation for future catastrophes occurring particularly in remote regions where an 'epidemic' of crush syndrome may be forecast, should include the setting up of a radio communications network to coordinate rescue and salvage operations and the forwarding of intravenous fluid bags and lines to the disaster site. Also, it is advisable to prepare artificial kidney devices which do not require pumps and electricity and which utilize a low dialysate volume for emergency temporary use, until conventional definitive medical facilities and services have been reestablished. Collapse"
102859,1990,journal of economic perspectives,on the economics of state lotteries,on the economics of state lotteries,". Dr. Mario S. Verani published a landmark article in 1990 describing the use of intravenous adenosine for stress myocardial perfusion imaging in patients with known or suspected coronary artery disease. In contrast to intravenous dipyridamole, the onset of coronary vasodilation with intravenous adenosine is almost instantaneous and can be stopped readily by discontinuation of infusion. Since this seminal publication, vasodilator stress with adenosine has become a routine clinical procedure world-wide. Mario S. Verani was born on September 29, 1943 in Cantagalo, Brazil. He grew up in the nearby mountain town of Nova Friburgo, Rio de Janeiro, Brazil, where his father was a lawyer and well-respected judge. In high school, Mario was an excellent student with a great interest in literature and writing. When Mario was 13 his 15-year old sister died from acute leukemia. This traumatic experience may have influenced his decision to apply to medical school. He was accepted at the highly competitive School of Medicine at the Federal University of the State of Rio de Janeiro (1962-1967). In medical school, Mario became reacquainted with a childhood friend Regina Rodrigues, who Mario married after her graduation. Following medical school Mario Verani was an intern in medicine at the Antonio Pedro University Hospital, Rio de Janeiro (1967) and subsequently he won a competitive research fellowship (19681970) at the Cardiovascular Research Center in Rio de Janeiro. Mario was interested in pursuing further medical training in the USA, with the intention of ultimately returning to Brazil. Mario and Regina passed the American ECFMG exam for foreignmedical graduates and both applied for medical residencies in the USA. In 1970, Mario was accepted to internal medicine residency programs in Ohio at the Daniel DrakeMemorial Hospital and Cincinnati General Hospital. During these residencies Mario became fascinated with cardiology. In 1972 Mario was accepted for a cardiology fellowship (1972-1974) at the University of Iowa College of Medicine, Iowa City. Here Mario worked with Dr. Melvin L. Marcus, an eminent cardiologist, coronary physiologist, and director of the University of Iowa Center for Research in Ischemic Heart Disease. In parallel with Mario, Regina was able to secure training positions for herself in pathology at Cincinnati and Iowa City. After Mario had completed his cardiology training, the couple returned to Brazil. Fully trained in echocardiography, Dr. Verani was offered a position as the director of the Echocardiography Laboratory at the Hospital dos Servidores in Rio de Janeiro (1974-1976). Nevertheless, the impact of their American experience was such that within a few months after their return, Mario and Regina filed applications for USA visas, that were granted in 1976. Dr. Verani returned to Iowa City as an Associate in Cardiology at the University of Iowa Hospitals. In 1977, The History Corner Members are listed in the Acknowledgements. Reprint requests: Frans J. Th. Wackers, MD, PhD, Yale University School of Medicine, Cardiology, New Haven, CT ; frans.wackers@yale.edu J Nucl Cardiol 2020;27:366–8. 1071-3581/$34.00 Copyright 2020 American Society of Nuclear Cardiology. Collapse"
102866,1990,journal of economic perspectives,puzzles:  professional advice and other hazards,puzzles professional advice and other hazards,"Hip O'Chondria visits his physician complaining of pain in his hip. The doctor explains that only two diagnoses are possible: either he has bursitis, which is curable with medication and rest, or he has degenerative arthritis, which requires surgery. The patient cannot observe which diagnosis is correct, but physicians can observe the true condition accurately. The correct treatment, medication or surgery, depends on the true condition. Mr. O'Chondria's physician explains to him that he has degenerative arthritis, but O'Chondria knows that his physician has an incentive to lie in order to perform the lucrative surgery, so he decides to seek a second opinion. Is the second opinion useful in helping Mr. O'Chondria learn the truth? Collapse"
102954,1990,journal of financial and quantitative analysis,bond pricing and the term structure of interest rates:  a discrete time approximation,bond pricing and the term structure of interest rates a discrete time approximation,"This paper studies the binomial approximation to the continuous trading term structure model of Heath, Jarrow, and Morton (1987). The discrete time approximation makes the original methodology accessible to a wider audience, and provides a computational procedure necessary for calculating the contingent claim values derived in the continuous time paper. This paper also extends and generalizes Ho and Lee's (1986) model to include multiple random shocks to the forward rate process and to include an analysis of continuous time limits. The generalization provides insights into the limitations of the existing empirical implementation of Ho and Lee's model. Collapse"
102955,1990,journal of financial and quantitative analysis,the dynamics of stock index and stock index futures returns,the dynamics of stock index and stock index futures returns,"In rational, efficiently functioning markets, the returns on stock index and stock index futures contracts should be perfectly, contemporaneously correlated. This study investigates the time series properties of 5-minute, intraday returns of stock index and stock index futures contracts, and finds that SP however, the effect is not completely unidirectional, with lagged stock index returns having a mild positive predictive impact on futures returns. Collapse"
102956,1990,journal of financial and quantitative analysis,"asymmetric information, collateral, and moral hazard",asymmetric information collateral and moral hazard,"In a credit market characterized by a priori asymmetric information, collateral not only can identify credit applicants but also can result in moral hazard involving the borrower's use of pledged assets. The borrower's other alternatives are to apply for unsecured bank credit and be priced as “average,” or to acquire financing by selling an asset and subsequently renting it for continued use. The optimal secured loan contract for higher quality firms is shown to involve overcollateralization. There is underinvestment relative to first best in maintenance of the pledged assets but overinvestment relative to the level that would be chosen without bank monitoring. Self-financing and unsecured credit are chosen by the intermediate and lowest quality groups, respectively. Collapse"
102957,1990,journal of financial and quantitative analysis,valuation effects of greenmail prohibitions,valuation effects of greenmail prohibitions,"Greenmail payments are widely viewed as actions designed by managers to perpetuate their tenure in office. This view, which suggests that greenmail prohibitions would enhance shareholder wealth, receives mixed empirical support in this paper. The average market reaction to charter amendments prohibiting greenmail payments is weakly negative, suggesting there is a value to maintaining managerial flexibility. Nonlinear maximum likelihood estimation, however, reveals a strong positive correlation between the market reaction and the firm's abnormal stock price runup over the three months just prior to the proxy mailing date. For the subsample of firms with a relatively large prior runup, the precommitment not to pay greenmail is value enhancing. If the prior runup reflects takeover rumors, then this evidence is consistent with the proposition that greenmail payments amidst takeover speculations are value decreasing. Collapse"
102958,1990,journal of financial and quantitative analysis,the systematic risk of discretely rebalanced option hedges,the systematic risk of discretely rebalanced option hedges,"This paper demonstrates that Black-Scholes option pricing model hedge positions that are risk free when rebalanced continuously will frequently exhibit substantial systematic risk when rebalanced at finite intervals. This systematic risk may have biased important empirical tests of the option pricing model. Moreover, this systematic risk means that the Black-Scholes option pricing model is inherently inconsistent with the discrete time version of the Capital Asset Pricing Model (CAPM). Collapse"
102959,1990,journal of financial and quantitative analysis,stock market seasonals and prespecified multifactor pricing relations,stock market seasonals and prespecified multifactor pricing relations,"Despite nonstationarities in the factor betas and factor prices of the Chen, Roll, Ross (1986) multifactor model, investors are rewarded for bearing risks associated with the change in expected inflation and industrial production in non-January months; however, variations in these factors have opposite influences on stock prices. These findings may partially explain why several recent studies fail to detect a significant non-January risk premium in the stock market, but this evidence is only suggestive since theoretical and statistical difficulties prevent precise interpretations of specific pricing relations in the Chen, Roll, Ross model. Collapse"
102960,1990,journal of financial and quantitative analysis,"price reversals, bid-ask spreads, and market efficiency",price reversals  bid ask spreads  and market efficiency,"We examine the behavior of common stock prices after a large change in price occurs during a single trading day and find evidence that the stock market appears to have overreacted, especially in the case of price declines; however, the magnitude of the overreaction is small compared to the bid-ask spreads observed for the individual stocks in the sample. We interpret this finding as being consistent with a market that is efficient after transactions costs are considered. Collapse"
102961,1990,journal of financial and quantitative analysis,stock returns before and after calls of convertible bonds,stock returns before and after calls of convertible bonds,"Ofer and Natarajan (1987) report negative, statistically significant cumulative average abnormal returns over five years following convertible bond calls. We show that these results are obtained only if returns preceding the call dates are used for market model parameter estimation. Returns preceding calls tend to be positive and unusually large. This means that predicted post-call returns, based on pre-call parameter estimates, are biased upward. Consequently, the corresponding abnormal returns are biased downward. We also discuss a corrected test statistic. We conclude that the evidence does not indicate market inefficiency in the stock price reaction to convertible calls. Collapse"
103014,1990,journal of human resources,"the intrahousehold demand for nutrients in rural south india: individual estimates, fixed effects, and permanent income",the intrahousehold demand for nutrients in rural south india individual estimates fixed effects and permanent income,"Good estimates of nutrient intake responses to prices and income are very useful for the evaluation of the numerous efforts to improve nutrition in many developing countries through price-subsidy and income-generation policies. We discuss three problems in standard estimates of these responses and then illustrate their implications for nutrient demand relations for a poor sample from rural south India. (1) Intra-household nutrient allocations usually are ignored. In this case nutrient intakes for females systematically have algebraically lower price elasticities than do those for males, which may leave the females particularly vulnerable at times of food shortages. (2) Unobserved fixed effects may bias the estimates of responses to observed variables. In this case not only the community fixed effects on which the previous literature has focused, but also household and individual fixed effects are important. Failure to control for them results in substantial algebraically upward biases in many estimated price responses. (3) Most previous studies use current instead of permanent income, which a priori may account for the low estimated income elasticities. In this case, however, the use of permanent income does not change the conclusion that the nutrient intakes responses to income are quite small. Collapse"
103017,1990,journal of human resources,is union job dissatisfaction real?,is union job dissatisfaction real,"The Characteristics of Business Owners (CBO) data base was created in 1987 by the U.S. Bureau of the Census. It was based upon a 1986 survey of 125,000 self-employed persons; it includes five self-contained samples of 25,000 each: 1) Hispanics, 2) blacks, 3) all other minorities, 4) females, and 5) nonminority males. Two versions of the CBO data files focus, respectively, upon 1) self-employed persons, and 2) small firms. For each observation, data were collected upon owner human capital, demographic traits, and financial capital investment in one's firm. Firm specific data are extensive, including sales, profits, employment, payroll, etc. Collapse"
103054,1990,journal of the japanese and international economies,the cost of capital in japan:  recent evidence and further results,the cost of capital in japan recent evidence and further results,"We extend our recent work measuring the cost of capital in Japan and the United States by considering several questions that such results raised. Among our findings are: (1) The small firm - large firm distinction appears to be more significant in Japan, not in the United States; (2) Correcting Japanese accounting statements for cross-holding raises the estimated Japanese cost-of-capital by about 1 percentage point; (3) Correcting Japanese accounting statements for unmeasured returns to land has a significantly more important effect: the most conservative correction we attempt raises the implied Japanese return to capital to parity with the United States during the mid-1980's. Collapse"
103061,1990,journal of law and economics,the economics of law firms:  a study in the legal organization of the firm,the economics of law firms a study in the legal organization of the firm,"OUR goal is to address several issues on the economics of the firm. In particular, we are interested in the survival value of ownership rules in competitive markets for complex services such as law, medicine, and accounting. Are sole proprietorships or partnerships more efficient in the delivery of these services to clients? How do small and large demanders organize their efforts to obtain the best deals in the marketplace? How does competition discipline suppliers to serve these uninformed consumers? In our view, two agency considerations are central to these issues: specifically, the creation of brand name or specific capital' and monitoring.2 In particular, we focus on the U.S. legal service industry, as it provides an interesting institutional framework to explore our questions. Lawyers practice law both in solo and as members of law partnerships of varying sizes. For example, as a percentage of all lawyers in private practice, the number of lawyers practicing in solo in the United States is substantial but has been declining over a recent twenty-year period: in 1960, 64 percent; Collapse"
103062,1990,journal of law and economics,effectiveness of the epa's regulatory enforcement:  the case of industrial effluent standards,effectiveness of the epas regulatory enforcement the case of industrial effluent standards,"IN the almost two decades since the initial wave of social regulation, the academic literature documented very few, if any, instances of a health, safety, or environmental regulation being an unqualified success. Indeed, in most cases, the problem is even more fundamental. The typical analysis of government regulation found that the regulation did not even fulfill its primary mission, much less pass a more demanding benefit-cost test. This absence of a well-documented case study of effective social regulation may be due, in part, to the particular set of regulations selected for analysis. There is certainly no inherent economic reason why such regulations cannot play a productive role in our economy. In the case of environmental quality, for example, the externality problems being addressed are not handled well by markets, implying that government regulation has at least the potential for playing a beneficial role. However, this potential will not be realized if the regulations are ill conceived or not effectively enforced, or if the environmental problem has no feasible solution. A brief review of past regulatory experiences may be instructive to put in better perspective the Environmental Protection Agency's (EPA) water pollution control effort-the focus of this article. Most of these detailed evaluations have been done with respect to agencies other than the EPA. Collapse"
103063,1990,journal of law and economics,the value of amtrak,the value of amtrak,"s of the Annual Symposium of the Czechoslovak Society of Histochemistry and Cytochemistry K O S I C E , C Z E C H O S L O V A K I A , 30 M A Y 2 J U N E , 1989 Light microscopic histochemistry of oxidases generating reactive oxygen species R. Gossrau 1, C. F. J. van Noorden 2 and W. M. Fredericks 2 1Department of Anatomy, Free University of Berlin, Berlin, Federal Republic of Germany and 2Laboratory of Histology and Cell Biology, University of Amsterdam, Amsterdam, The Netherlands Light microscopically, oxidases generating reactive oxygen species (ROS) are mostly localized using tetrazolium salt or coupled peroxidase methods. Electron microscopically, the Ce techniques are the procedures of choice. In an attempt to employ all these methods for histochemical studies of ROSgenerating oxidases using a light microscope, we improved the original light microscopic Ce-DAB procedure of Angermi~ller and Fahimi (J. Histochem. Cytochem. 36, 23-8, 1988). In the present study, we report on these improved methods for amine oxidases (AOX), xanthine oxidase (XOX), D-amino oxidase (AAOX), c~-hydroxy acid oxidase (HAOX) and NADH oxidase (NADHOX); the results are compared with those obtained with the tetrazolium and coupled peroxidase techniques for AOX, XOX, AAOX and HAOX in various tissues of adult rats. The sensitivity of the original Ce-DAB method was increased by the addition of Ni or Co ions and small amounts of H202 to the DAB visualization medium. With this technique, peroxisomal and non-peroxisomal oxidase activities were found at a number of sites where they could not be observed with DAB alone. Similar or identical findings were seen for HAOX using tetrazolium (with and without artificial electron acceptors and polyvinyl alcohol) and the modified Ce-DAB method. Different results were obtained for AOX and XOX, and especially for AAOX which was detectable only with the CeDAB and coupled peroxidase procedures. Attempts to localize NADHOX with the Ce-DAB method in the light microscope failed, except for neutrophil leucocytes; NADH is preferentially hydrolysed by neudeotide pyrophosphatase. In conclusion, tetrazolium, coupled peroxidase and CeDAB procedures should be used in parallel, to obtain as much information as possible on ROS-generating oxidases in the light microscope; light microscopically, the Ce-DAB method seems to be unsuitable for NADHOX localization outside neutrophil leucocytes. Supported by the German Research Foundation (Sfb 174). 0018-2214/90 $03.00 +.12 9 1990 Chapman and Hall Ltd. A comparison of different procedures for the estimation of Michaelis-Menten parameters in quantitative enzyme histochemistry M. Ruhnke Department of Anatomy, Free University of Berlin, Berlin, Federal Republic of Germany Quantitative enzyme histochemistry, e.g. microdensitometry, allows the determination of K~ and Vma x values in situ. These values seem to be more reliable than those determined in diluted homogenates. Nevertheless, it is important to use an error-free method for the estimation of Michaelis-Menten parameters. The linear transformations of Lineweaver-Burk, Eadie-Hofstee, and Hanes are those most widely used. For data obtained biochemically, it has been shown that several errors are associated with these procedures, and that methods such as the direct linear plot, or a direct fitting of a hyperbole to the measured data, are preferable. In order to show the relevance of these statements for quantitative enzyme histochemistry, the substrate dependence of the plasma membrane hydrolases dipeptidylpeptidase IV-glutamyl transpeptidase, unspecific alkaline phosphatase and C~-D-glucosidases were determined. For each enzyme apparent, I~ n and V .... values were estimated with the methods mentioned above. Generally, the direct linear plot and direct fitted curve were superior to other procedures. The linear transformations are helpful only when few data are available; among those, the Lineweaver-Burk plot is by far the weakest. Supported by the German Research Foundation (Sfb 174). Stability of azo-dyes: a very important factor in the quantitative determination of activities of hydrolases J. Smfdova and Z. Lojda Laboratory of Histochemistry, Faculty of Medicine, Charles University, Czechoslovakia, Prague Azo-coupling reactions are used in the quantitative biochemical and histochemical determination of activities of hydrolases. Post-coupling methods are used in biochemistry, and the azo-dye is estimated either after elution in ethylacetate (E) or after addition of a detergent (D). Simultaneous azo- Collapse"
103064,1990,journal of law and economics,the effect of ownership changes on the employment and wages of central office and other personnel,the effect of ownership changes on the employment and wages of central office and other personnel,"D URING the 1980s there has been a rapid increase in the rate of business ownership change in the United States. The value of the companies involved in such transactions increased almost sixfold between 1980 and 1986. The high rate of ownership change has stimulated growing interest (and in some cases concern) among policymakers, scholars, and the public about the causes and effects of ownership change, particularly about its effects on efficiency (hence ""competitiveness""). Some theories of ownership change, and anecdotal or fragmentary evidence concerning these changes, suggest that various kinds of ownership change-such as hostile takeovers, leveraged buyouts, and voluntary divestitures-result in reductions in administrative and managerial employment, both absolutely and relative to production employment. Carl Icahn, a prominent performer of hostile takeovers, claims that we have ""created a corporate welfare state. . . . companies are burdened by layers of vice presidents who not only don't produce, but are often counterproductive. . I and other 'raiders' usually eliminate the people who are most Collapse"
103066,1990,journal of law and economics,terminal railroad revisited:  foreclosure of an essential facility or simple horizontal monopoly?,terminal railroad revisited foreclosure of an essential facility or simple horizontal monopoly,"rOLLOWING the Supreme Court's 1986 decision in Aspen Ski,' there has been a renewed interest in the ""essential facilities doctrine."" The doctrine, as we understand it, is inconsistent with economic theory. Broadly stated, the doctrine says that, if assets cannot be economically reproduced by another firm but are economically essential to all producers of some good, then all producers of that good should have equal access to the assets. The doctrine presumes that the firm controlling such assets will not provide access equally to all firms. This problem is thought to be particularly acute in cases where the firm owning the asset is one among a group of competitors needing that asset to produce some other good. The case that has been cited as ""establishing"" the doctrine and the ""classic essential facilities case""2 is United States v. Terminal Railroad Association of St. Louis.3 Several recent articles4 on this case detail three important propositions: Collapse"
103067,1990,journal of law and economics,the effect of higher criminal penalties on antitrust enforcement,the effect of higher criminal penalties on antitrust enforcement,"IN 1974 Congress elevated penalties for criminal antitrust offenses from the misdemeanor to the felony level, an action that significantly increased the maximum penalties available to the courts.1 This legislation is one example of frequent congressional interventions that either regulate sentencing practices or are intended to raise penalties for particular crimes.2 This article will analyze the potential effects from such legislation and assess empirically the effect of the felony penalties on antitrust enforcement. The empirical work uses an original data set and a technique that has only recently been applied to analyses of litigation. In assessing the deterrent effects of the felony penalties, the following questions are relevant. First, how do the courts react to statutory changes in penalties? In this case, the courts followed congressional intent by sentencing those convicted to substantially longer jail terms and higher Collapse"
103068,1990,journal of law and economics,bureaucracy and politics in ftc merger challenges,bureaucracy and politics in ftc merger challenges,No Result.
103070,1990,journal of law and economics,cogeneration after purpa:  energy conservation and industry structure,cogeneration after purpa energy conservation and industry structure,"COGENERATION is the process of generating electricity as a by-product of the manufacture of another good, such as paper or steel. By-product electricity can be used by its producers and/or sold to another party. In its original form, state utility regulation made it difficult for nonutilities to sell by-product electricity outside their firm. Under the typical state public utilities act, any sale of power to another party subjects the firm to price and profit regulation, even if the power sale is small and unrelated to the firm's primary line of business. Also, the Public Utility Holding Company Act (PUHCA) prohibits nonutility ownership of utility properties.' In 1978 Congress passed the Public Utility Regulatory Policies Act (PURPA).2 This act exempts cogenerators from state regulation and Collapse"
103071,1990,"journal of law, economics, and organization",congressional influence on bureaucracy,congressional influence on bureaucracy,"In 2008 two government-sponsored enterprises, Fannie Mae and Freddie Mac, were placed into conservatorship due to insolvency. The financial bailout of the two publically traded corporations came at the expense of the American tax payer. This study investigates the relationship between direct and indirect government influence and the increasing risk taking of Fannie Mae and Freddie Mac from the late 1990’s through their conservatorship in 2008. As government-sponsored enterprises Fannie Mae and Freddie Mac have many special advantages that other publically traded companies did not possess. These advantages allowed Fannie Mae and Freddie Mac to increase their profitability. Theoretical literature regarding Congress and the bureaucracy suggests that the actions of bureaucrats can be linked to the preferences of Congressional members because bureaucrats are responsive to potential threats or perceived threats from the legislature. This theory is applicable to Fannie Mae and Freddie Mac, and is used to explain why the government was able to directly and indirectly influence the government-sponsored enterprises. Overall this investigation has determined that the United States government pursued a clear mission that determined to increase the availability of housing to all Americans, specifically to low-income and under-served individuals, through the use of the government-sponsored enterprises. Despite this link there is no conclusive data to show that the pursuit of this housing mission led Fannie Mae and Freddie Mac to operate in riskier business segments. This study has also found that motivation regarding profit-seeking and compensation structure provide a more Collapse"
103072,1990,"journal of law, economics, and organization",congressional influence on bureaucracy:  comment,congressional influence on bureaucracy comment,"John Ferejohn and Charles Shipan consider an issue central to the field of administrative law: the interrelationship between the three branches of government in producing public policy. Executive branch agencies do not perfectly represent the president's will. Rather, they are constrained by the possibility that Congress may pass laws restricting their discretion. Since congressional committees serve an agenda-setting function, they will not introduce legislation if they prefer the agency's policy to the law that would be enacted by Congress. Thus agencies take account of the positions of the relevant oversight committees. Judicial review can further constrain agency behavior if the courts can void executive action. The possibility of a presidential veto and of its override by a two-thirds vote interjects an additional strategic consideration into the choices of bureaucracies, courts, and committees. Congress will seek veto-proof bills, not ones that reflect the median preference of members. Ferejohn and Shipan model the courts as just one more political actor with a set of policy goals. The judges, however, are not able to impose their own preferences when reviewing legislation. Instead, in the FerejohnShipan model, when judges strike down an agency action, the prestatutory status quo prevails. Congressional action may follow such an invalidation. According to Ferejohn and Shipan, the courts act strategically by predicting the impact of their decisions on subsequent congressional action. Collapse"
103074,1990,"journal of law, economics, and organization",the commitment to seniority in self-governing groups,the commitment to seniority in self governing groups,"No sane man would for one moment think of making a graduate from West Point a field general, or one from Annapolis an admiral, or one from any university or college chief of a great newspaper, magazine, or business house. A priest or preacher who has just taken orders is not immediately made a bishop, archbishop, or cardinal. In every walk of life, ""men must tarry at Jericho till their beards are grown."" Champ Clark, Speaker of the U.S. House of Representatives Collapse"
103075,1990,"journal of law, economics, and organization","seniority, commitment, and self-governing groups",seniority  commitment  and self governing groups,"Kenneth Shepsle and Barry Nalebuff have written a challenging and insightful essay on ""The Commitment to Seniority in Self-Governing Groups."" Its challenge arises from the richness of its focal concepts: seniority, commitment, and self-governing. Its insights come from clarifying these concepts while working toward a theory that has more immediate empirical implications than current theories. Shepsle and Nalebuff regard their paper as a ""prolegomenon."" As a preliminary statement, it will undoubtedly be useful for future research. This note attempts to provide some additional guidance by highlighting some strengths and limitations of the Shepsle-Nalebuff analysis. My premise is that a useful theory of seniority should have clear and empirically motivated concepts, explicit assumptions, and results that can be translated into testable predictions. Comments are organized accordingly. Collapse"
103079,1990,"journal of law, economics, and organization","regulatory capture, public interest, and the public agenda: toward a synthesis",regulatory capture public interest and the public agenda toward a synthesis,"For at least 30 years, commentators have been engaged in a debate about what animates the regulatory process. Is the ultimate goal of regulation to pursue some conception of the general good, however mean-spirited, messy, and confused the process may seem at any given time? This hypothesis, widely known as the ""public interest"" theory of regulation, was dominant for many years, then left for dead by academics of the 1960s and 1970s, but has since been found alive, although in much weakened condition, in the 1980s. Or is regulation simply an arena in which special interests contend for the right to use government power for narrow advantage? This hypothesis, known variously as the capture theory, economic theory, or governmentservices theory of regulation, has been dominant for the past 25 years. Embedded in this question is another philosophically related one: What motivates a regulator-a legislator, commissioner, agency head, or bureaucrat-faced with a regulatory decision? Do such actors seek the ""best"" policy Collapse"
103080,1990,"journal of law, economics, and organization","regulatory capture, public interest, and the public agenda: toward a synthesis:  comments",regulatory capture public interest and the public agenda toward a synthesis comments,"Michael Levine and Jennifer Forrence show quite well that neither the ""capture"" nor ""public interest"" view of regulation is adequate. Their critique of the literature is useful and provides a sound basis for launching their ambitious inquiry. Their call for a synthesis is a welcome one-one in which, I might add, Roger Noll has joined, most recently in the Brookings Papers on Economic Activity (Noll:48-58). As Levine and Forrence seek to account for regulatory outcomes, they challenge us to think about why we have some outcomes and not others. We would all agree, I think, with the authors that we need to better explain when regulators are captured or act in the general interest. If we are to understand regulatory outcomes, we must explore in greater depth some of the subjects that Levine and Forrence raise in the latter part of their paper-namely, agenda-setting, slack, political dominance, and testability of hypotheses. I will consider each in turn. To account for regulatory outcomes, as this article seeks to do, I believe it is important to understand how issues become part of the public agenda. If, as Levine and Forrence argue, whether a regulatory outcome is captured or Burkean depends on whether the issue is on the public agenda, then we must attempt to understand how various actors seek to manipulate the pub- Collapse"
103081,1990,"journal of law, economics, and organization","slack, public interest, and structure-induced policy",slack  public interest  and structure induced policy,"Public service motivation (PSM) assumes that civil servants are characterized by an ethic to serve the public. In this journal, Perry (1996) identified a multi-dimensional scale to measure PSM, which has four components: attraction to policy-making, commitment to public interest, compassion, and self-sacrifice. But there is little research on the generalizability and applicability of the dimensions and scale of PSM in the other countries. The present study tests whether the structure of PSM observed in the United States by Perry (1996) can be generalized to Korea. Two independent samples (n1 = 294 and n2 = 290) are used for the scale validation. The statistical analysis applied confirmatory factor analysis (CFA) using Amos 5.0. It was found that the four-factor structure of PSM can be generalized in the Korean context but in the second-order model the factor of attraction to policy-making (APM) is doubtful whether it is indeed a valid dimension of PSM. Several reasons for this are discussed: In the Korean context, which differs from the United States, the rational motive might not be related to PSM; the rational motive itself is not part of PSM; the items of APM might not be appropriate to represent a rational base of PSM; and the negativelyworded items are not appropriate to assess APM. It is generally believed that many public employees are motivated by a sense of service not found among private employees (Houston 2000; Perry and Wise 1990). They are seen as motivated by a concern for the community and a desire to serve the public interest, and are more likely to be characterized by an ethic that prioritizes intrinsic rewards over extrinsic rewards (Crewson 1997). The concept of public service motivation (PSM) is used to explain the difference between public and private employees (Perry 1996; Perry and Wise 1990). In recent years a significant amount of research has examined the topic of PSM. The primary focus of the recent studies on PSM has been on identifying its nature and asking if it is characteristic of civil servants (Houston 2006). However, the measure of PSM is not fully examined. In this journal, Perry (1996) identified a multi-dimensional scale to measure PSM, which has four components: attraction to policy-making, commitment to public interest, compassion, and self-sacrifice. Perry’s measurement scale may be considered as representing the generally accepted model of PSM within the United States (Vandenabeele, Hondeghem, Maesschalck, and Depr 2004). But there is little research on the generalizability and applicability of the dimensions and scale of PSM in the other countries. National culture might influence the construct of PSM as found in North American studies. It is an empirical question whether it is possible to measure PSM in a different cultural context using the same approach. Assessing the applicability of frameworks developed in one country to other countries is an important step in establishing the generalizability of PSM theories. A major concern for using a scale developed in another country is its validity across societies (Hui, Lee, and Rousseau 2004). The present study tests whether the structure of PSM observed in the United States by Perry (1996) can be generalized to Korea. The purpose of this empirical investigation is to explore the content and factor structure of PSM in the Korean context and to cross-validate Perry’s (1996) scale. Two independent samples (n1 = 294 and n2 = 290) are used for the scale validation. Public Service Motivation PSM assumes that civil servants are characterized by an ethic to serve the public. They are committed to the public interest, and characterized by an ethic built on benevolence, a life in service of others, and a desire to affect the society (Houston 2006). PSM provides a useful basis for understanding public employee motivation (Perry 2000). According to Perry and Wise (1990, 368), PSM is defined as “an individual’s predisposition to respond to motives grounded primarily or uniquely in public institutions and organizations.” Brewer and Selden (1998, 417) describe it as “the motivational force that induces individuals to perform meaningful public service.” Rainey and Steinbauer (1999, 23) define it as “a general altruistic motivation to serve the interests of a community of people, a state, a nation or mankind.” Recently Vandenabeele, scheepers, and Hondeghem (2006, 15) define it as “the belief, values and attitudes that go beyond self-interest or organizational interest, that concern the interest of a larger political entity and that induce, through public interaction, motivation for targeted action.” Even though the definitions of PSM itself are slightly different according to the authors, a commitment to the public interest, service to others, and self-sacrifice underlie an understanding of PSM (Houston 2006). Perry and Wise (1990) formulated three propositions: (a) The greater an individual’s public sector motivation, the more likely it is that the individual will seek membership in a public organization. (b) In public organizations, public sector motivation is positively related to performance. (c) Public organizations that attract members with high levels of public sector motivation are likely to be less dependent on utilitarian incentives to manage individual performance effectively. PSM pertains to government employees. Public employees not only place a higher value on helping others, serving society and the public interest, and performing work that is worthwhile to society but also rank intrinsic rewards higher in importance than private sector employees (Crewson, 1997; Houston, 2000; Rainey 1982; Wittmer 1991). Crewson (1997) found that public-sector employees rate a feeling of accomplishment and performing work helpful to society and to others as more important job characteristics than do private-sector employees. Naff and Crum (1999) found a significant relationship between PSM and federal employees’ job satisfaction, performance, intention to remain in the government, and support for the government’s reinvention efforts. Houston (2000) showed that PSM does exist, and that public employees are more likely to place a higher value on the intrinsic reward of work that is important and provides a feeling of accomplishment. PSM is a modestly important predictor of organizational performance in testing a comprehensive model (Brewer and Selden, 2000). Brewer (2003) found that public employees score higher on attitudinal items related to social trust, altruism, equality, tolerance, and humanitarianism, and that they are more civically active as they perform more than one-third more civic activities than other citizens. Lee (2005) found that there is a statistically significant difference between public and private employees in terms of PSM using Korean cases and that the higher PSM among public employees is positively related to higher performance level. Houston (2006) found that, using data from the 2002 General Social Survey, government employees are more likely to volunteer for charity and to donate blood than for-profit employees are, and that PSM is more prominent in public service than in private organizations. PSM has rational, norm-based, and affective bases (Perry and Wise 1990). Rational motives are grounded in individual utility maximization, norm-based motives are grounded in a desire to pursue the common good and further the public interest, and affective motives are grounded in human emotion. A variety of rational, norm-based, and affective motives appear to be primarily or exclusively associated with public service. Rational motives are participation in the process of policy formulation, commitment to a public program because of personal identification, and advocacy for a special or private interest. Norm-based motives are a desire to serve the public interest, loyalty to duty and to the government as a whole, and social equity. Affective motives are commitment to a program from a genuine conviction about its social importance, and patriotism of benevolence. Perry and Wise (1990) describe these motives as psychological deficiencies or needs which can be satisfied by working in public institutions and organizations. A variety of rational, norm-based, and affective motives appear to be primarily or exclusively associated with public service. Perry (1996) identified four empirical components of the PSM construct as attraction to public policy making, compassion, commitment to the public interest, and self-sacrifice. Three of the subscales map directly to the motivational foundations (Perry 1996; 2000). Attraction to public policy making coincides with rational choice processes, commitment to public interest with normative processes, and compassion with affective processes. The outcome of Perry’s study (1996) was the development of a list of 24 items measuring four subscales of PSM. Recently, the components of PSM have been analyzed. Brewer, Selden, and Facer (2000) found that there are four different conceptions of PSM: Samaritans, communitarians, patriots, and humanitarians. The primary motives common to all of these are serving the public, making a difference in society, and ensuring individual and social equity but an interest in politics and policymaking is not a characteristic of any of these conceptions of PSM. After reviewing PSM in comparative perspective, Vandenabeele et al. (2004) concluded that PSM is a universal concept and all four dimensions of Perry (1996) can be found when describing the French and Dutch variants. Choi (2004) examined the relationship between PSM and ethical behavior, and suggested that only self-sacrifice in PSM is one of the critical factors that influence the ethical reasoning level of public servants in the United States. Lee (2005) found that, among Korean public employees, the component of attraction to policy making did not affect performance levels, but the other three components di Collapse"
103082,1990,"journal of law, economics, and organization",political institutions:  the neglected side of the story,political institutions the neglected side of the story,No Result.
103083,1990,"journal of law, economics, and organization",the problems with ppt:  comment,the problems with ppt comment,"See Comment page 101 In 2004, ministers of health from around the world gathered in Mexico City for the fi rst ever global ministerial summit on health research. Resolutions from that meeting were later approved by the 58th World Health Assembly in Geneva. Since then, many organisations, countries, and donors have been working to deliver on those recommendations. In November this year, an even broader group of ministers and other stakeholders will gather in Bamako, Mali, to assess progress and determine new priorities. The 2008 Global Ministerial Forum on Research for Health will provide a unique platform for ministers of health, science and technology, and social development to discuss research with global experts and stakeholders. Refl ecting this diversity, and the broad vision of research for health, the meeting is being co-organised by a partnership involving the Council on Health Research for Development (COHRED), Global Forum for Health Research, Government of Mali, UNESCO, World Bank, and WHO. On the road to Bamako 2008, it is appropriate to refl ect on where we have been, what progress we have made, and where we wish to go in the future. In 1990, the Commission on Health Research for Development noted that only 5% of global spending on health research went to problems aff ecting the poorest 93% of the world’s people, now known as the “10/90 gap”. The Commission recommended investment in essential national health research, inter national partnerships, and mechanisms to monitor progress, and noted that “research should not be limited to the health sector, but should also examine the health impact of development in other sectors and the socioeconomic determinants of health”. The Commission’s pivotal report was followed in 1993 by the World Bank’s World development report: investing in health, and by the establishment of COHRED to promote the conduct and use of essential health research in developing countries. In 1996, WHO’s Ad Hoc Committee on Health Research Relating to Future Intervention Options identifi ed best buys for investment, including research on maternal and child health, microbial threats, health systems, noncommunicable diseases, and injuries. The Global Forum for Health Research was created in 1998 to advocate for greater invest ments in health research to meet the needs of poor people. In 2000, the Global Forum’s annual meeting was co-convened with WHO and the World Bank in Bangkok, Thailand, subsumed under an International Conference on Health Research for Development, in which COHRED had a leading organisational role. Participants emphasised the strengthening of national health research systems as a key priority to reduce the 10/90 gap. 4 years after the Bangkok meeting, the Global Forum’s eighth annual meeting was held in Mexico City in parallel with the Ministerial Summit on Health Research organised by WHO. The two overlapping events focused on research to achieve the Millennium Development Goals (MDGs). Ministers attending the summit committed to three key priorities: health-systems research, securing public confi dence in research, and bridging the gap between knowledge and action. These developments might have helped to stimulate the recent explosion of innovative new ideas and initiatives in global health. In this context, two additional phenomena deserve mention. First, some innovative developing countries (IDCs) such as Brazil, China, India, and South Africa have become important producers of low-cost drugs and vaccines. Second, donors and developing countries have both begun, rightly, to embrace science and technology as key drivers of social and economic development. So what progress have we made? Overall, this is a story of both new resources and new challenges. Global investments in research and development for health have quadrupled since 1986 (fi gure), though Collapse"
103084,1990,"journal of law, economics, and organization",political institutions:  the neglected side of the story--comment,political institutions   the neglected side of the story  comment,"Although an understanding of bureaucracy is vital to the evolving theories of economic organization and politics, bureaucracy is a seriously neglected topic. Terry Moe is one of the few scholars who has addressed himself to the pertinent issues in an illuminating way (see especially Moe, 1990a). I concur with Moe's view that political scientists of the ""new institutionalist"" genre have given disproportionate attention to legislatures as compared with bureaucracies and that a more balanced approach that ""tackles the subject of bureaucracy in a serious way"" (Moe, 1990b:S250) is needed. Furthermore, I concur with the eight points of inquiry that he prescribes (Moe, 1990b:S250-S251). My own sense of the best way to reorient a field is to address the neglected questions and to offer insights and answers with which ""orthodoxy"" must come to terms. Moe's comparison of the American separation-ofpowers system with the parliamentary systems of other Western democracies-in terms both of the differences in their politics of structural choice and differences in the bureaucracies-is in that spirit. His contrast between presidents and legislators as it relates to bureaucratic design is also illuminating. His main point is that political institutions are distinctive and need to be addressed on their own terms. Much of the recent work that he reviews fails that test. Collapse"
103085,1990,"journal of law, economics, and organization","explaining administrative process:  normative, positive, and critical stories of legal development",explaining administrative process normative positive and critical stories of legal development,"Answers to these questions abound in the legal literature. In an influential article, Stewart argues, for example, that much of modern American administrative law is constructed to facilitate pluralist participation in administrative decision-making. More recently, Sunstein (1986) has argued that administrative law is designed to promote the republican value of deliberative rationality and to limit the influence of special-interest pleading. Indeed, the legal literature bristles with claims concerning the purposes of administrative process and processes, ranging from ""fairness"" to ""efficiency"" and utilizing a host of other ideas as well-""openness,"" ""accountability,"" ""legitimacy,"" and ""rationality,"" to name but a few. There has been little attention to the structure of these claims. This Collapse"
103086,1990,"journal of law, economics, and organization","political economy, administrative law:  a comment",political economy administrative law a comment,"In 1986, Dinwiddy and Teal analyzed the shadow exchange rate (SER) in a tariff-distorted, small economy containing two traded and one non-traded good. Contrary to their claim, I maintain a SER is inherent to their model even if the balance of payments constraint is ""binding."" It equals the value of the marginal consumption basket at market prices divided by its value at world or foreign exchange equivalent (FEE) prices. Shadow prices defined in welfare terms equal the products of the SER and (1) the world (or FEE) prices of goods, or (2) marginal products of factors at world prices. Copyright 1990 by The London School of Economics and Political Science. Collapse"
103087,1990,"journal of law, economics, and organization",positive and normative models of procedural rights:  an integrative approach to administrative procedures,positive and normative models of procedural rights an integrative approach to administrative procedures,"The gulf between legal ideals and reductionist PPT provides a challenge for PPT: Is PPT necessarily reductionist, arguing that legal realism is misconceived and irrelevant? If not reductionist, then how are the two to be integrated in a manner that does more than give lip service to legal idealists? Our answer is that the two approaches can be integrated. Attention to normative issues by reelection-minded politicians can explain self-imposed procedural limits, such as those enacted in the Administrative Procedure Act, by noting that such limits serve political ends as well as normative ones. Collapse"
103158,1990,journal of public economics,a distance function approach to price efficiency,a distance function approach to price efficiency,"This article examines the thermal power generation efficiency of ten Japanese electric power companies and the shadow prices of carbon dioxide (CO2) by employing a directional output distance function (DODF) with panel data for 1990–2011. We find that the shadow price of CO2 varies greatly between US$1.49 and US$288.82, depending on the company’s production strategy concerning energy supply and CO2 emissions. These shadow prices give us clues to understand how the electric power companies may respond to environmental regulations, such as environmental tax and emission trading systems. According to the DODF, an additional 53571 GWh of electricity could have been generated in 2011 at the cost of an increase of 40105 thousand tonnes of CO2, if the companies would have operated efficiently giving little consideration to CO2 emissions reduction. These increases are equivalent to 8.77 and 9.18 % of total electricity and CO2 emissions, respectively, from the ten electric power companies in 2011. On the other hand, if the companies would have operated efficiently and given first priority to CO2 emissions reduction, a further 58002 thousand tonnes of CO2, equivalent to 13.28 % of their total CO2 emissions in 2011, could have been reduced as a whole. Collapse"
103160,1990,journal of public economics,efficient windows and labor force reduction,efficient windows and labor force reduction,"Recently many U.S. firms have offered ""window"" plans that provide bonuses to a group of workers if the worker retires within a specified short time span. This paper examines a window plan at a Fortune 500 firm, and addresses two main issues. First, what was the effect of the window plan on departures? Second, assuming a variety of possible firm objectives, what would be the design of an efficient window plan? These questions are addressed using the retirement model in Stock and Wise [1988a, 1988b] . The model, estimated using data for an earlier year, predicts well out-of-sample the subsequent large increase in retirements under the window plan. We find that while the firm successfully maximized departures, if its goal was to minimize either expected future wage payments or the current cost per induced retirement, the firm could have saved more with efficient plans constructed using the model. One interpretation is that the firm was primarily interested in reducing the overall size of the labor force or in retiring older employees to allow promotion of younger employees. Collapse"
103223,1990,journal of risk and uncertainty,long-term environmental risks,long term environmental risks,"How well we manage long-term environmental risks depends on how well we understand them. Whether the risk managers are experts or laypeople, that understanding is typically limited. As a result, people must rely on judgment when making decisions about risks. Estimating how big risks are and how much reducing them is worth is an intellectual skill. After reviewing the behavioral principles that govern how people acquire such skills, this article offers several proposals for facilitating learning abut risks by improving the ways in which scientific data are created or presented. It also describes some pitfalls facing attempts to determine the quality of other people's understanding of risks, whether through direct study or more casual observation. Collapse"
103230,1990,journal of the royal statistical society: series a (statistics in society),the skills challenge of the nineties,the skills challenge of the nineties,"The risk of a woman dying as a result of pregnancy or childbirth during her lifetime is about one in six in the poorest parts of the world compared with about one in 30 000 in Northern Europe. Such a discrepancy poses a huge challenge to meeting the fifth Millennium Development Goal to reduce maternal mortality by 75% between 1990 and 2015. Some developed and transitional countries have managed to reduce their maternal mortality during the past 25 years. Few of these, however, began with the very high rates that are now estimated for the poorest countries-in which further progress is jeopardised by weak health systems, continuing high fertility, and poor availability of data. Maternal deaths are clustered around labour, delivery, and the immediate postpartum period, with obstetric haemorrhage being the main medical cause of death. Local variation can be important, with unsafe abortion carrying huge risk in some populations, and HIV/AIDS becoming a leading cause of death where HIV-related mortaliy rates are high. Inequalities in the risk of maternal death exist everywhere. Targeting of interventions to the most vulnerable--rural populations and poor people--is essential if substantial progress is to be achieved by 2015. Collapse"
103269,1990,land economics,markets for preservation:  old-growth and forest service auctions,markets for preservation   old growth and forest service auctions,No Result.
103347,1990,monthly labor review,"south african trade unions:  a historical account, 1970-90",south african trade unions   a historical account  1970 90,"The Emissions Database for Global Atmospheric Research (EDGAR) compiles anthropogenic emissions data for greenhouse gases (GHGs), and for multiple air pollutants, based on international statistics and emission factors. EDGAR data provide quantitative support for atmospheric modelling and for mitigation scenario and impact assessment analyses as well as for policy evaluation. The new version (v4.3.2) of the EDGAR emission inventory provides global estimates, broken down to IPCC-relevant source-sector levels, from 1970 (the year of the European Union’s first Air Quality Directive) to 2012 (the end year of the first commitment period of the Kyoto Protocol, KP). Strengths of EDGAR v4.3.2 include global geo-coverage (226 countries), continuity in time, and comprehensiveness in activities. Emissions of multiple chemical compounds, GHGs as well as air pollutants, from relevant sources (fossil fuel activities but also, for example, fermentation processes in agricultural activities) are compiled following a bottom-up (BU), transparent and IPCC-compliant methodology. This paper describes EDGAR v4.3.2 developments with respect to three major long-lived GHGs (CO2, CH4, and N2O) derived from a wide range of human activities apart from the land-use, land-use change and forestry (LULUCF) sector and apart from savannah burning; a companion paper quantifies and discusses emissions of air pollutants. Detailed information is included for each of the IPCC-relevant source sectors, leading to global totals for 2010 (in the middle of the first KP commitment period) (with a 95 % confidence interval in parentheses): 33.6(±5.9) Pg CO2 yr−1, 0.34(±0.16) Pg CH4 yr−1, and 7.2(±3.7) Tg N2O yr−1. We provide uncertainty factors in emissions data for the different GHGs and for three different groups of countries: OECD countries of 1990, countries with economies in transition in 1990, and the remaining countries in development (the UNFCCC nonAnnex I parties). We document trends for the major emitting countries together with the European Union in more Published by Copernicus Publications. 960 G. Janssens-Maenhout et al.: EDGAR greenhouse gas emissions detail, demonstrating that effects of fuel markets and financial instability have had greater impacts on GHG trends than effects of income or population. These data (https://doi.org/10.5281/zenodo.2658138, Janssens-Maenhout et al., 2019) are visualised with annual and monthly global emissions grid maps of 0.1× 0.1 for each source sector. 1 Historical evolution An essential component of the UN Framework Convention on Climate Change (UNFCCC, 1992) is the collection of nationally reported inventories and information on these greenhouse gas (GHG) emission inventory time series. At the time the UNFCCC was established, the 24 members of the OECD in 1990 and 16 other European countries and Russia were considered liable for “the largest share of historical and current global emissions of GHG” and taken up in Annex I to the UNFCCC. These Annex I countries and the European Union1 submit annually complete inventories of GHG emissions from the 1990 base year2 until the latest year for which full accounting is completed and reviewed (typically with a 2-year time lag), and these inventories are all reviewed to ensure transparency, completeness, comparability, consistency and accuracy3. This allows for most of these Annex I countries to track progress towards their reduction targets committed under the Kyoto Protocol (UNFCCC, 1997). Other (non-Annex I) countries are encouraged to submit their GHG inventories as part of their National Communications and Biennial Update Reports (BURs). The GHG inventories of nonAnnex I countries were required to cover CO2, CH4 and N2O emissions for 1 year (1990 or 1994), without specific documentation and only subject to a brief review. However, the Paris Agreement (UNFCCC, 2015) requires submission every 2 years of BURs4, which are subject to international consultation and analysis. Theoretically, UNFCCC should receive at the latest after 2 years national emissions inventories from each of the 197 countries, but as shown in Fig. 1a, not all countries did provide a national inventory and 154 countries did not provide a completed (i.e. year-2) time se1This includes the 28 Member States of the European Union (EU) as of 1 July 2013. 2For some economies in transition, another year such as 1988 or 1989 can be chosen under UNFCCC as the base year. These GHG emissions are mainly sources, but also include carbon stock sinks for which the human-induced part needs to be assessed with care (Grassi et al., 2018). 3These five principles of a good reporting practice are defined in the UNFCCC guidelines for national GHG inventory, e.g. https://pdfs.semanticscholar.org/3c30/ a1bd769dee5299746e0af825c7ab4ed55fba.pdf. EDGAR uses the term “comprehensiveness” to summarise these principles. 4The first BUR submitted should cover the inventory for the year no more than 4 years prior to the submission data, and subsequent BURs should be submitted every 2 years, but flexibility is given to the least developed countries and small island developing states. ries of inventories. In addition, many countries lack a welldeveloped statistical infrastructure, which is needed for an accurate bottom-up (BU) inventory. Figure 1b presents the latest year that is covered with a national inventory, with dates for quite a few countries more than 10 years ago: for most South-East Asian countries this is between 2004 and 2007 and for most African countries between 2000 and 2003. As such, the collection of national reports/communications does not provide a complete, consistent and comparable global dataset which can be used to understand the global budgets of the most important GHG emissions and their impact on climate. Very few bottom-up inventories of global anthropogenic emissions have been produced with continued effort for more than 2 decades. The Carbon Dioxide Information Analysis Centre (CDIAC) (Boden et al., 2017; Andres et al., 2014) and the Emissions Database for Global Atmospheric Research (EDGAR) (Olivier and Janssens-Maenhout, 2016; Olivier et al., 2016) provide global totals, whereas the IEA provides CO2 estimates from fuel combustion only and the FAO CH4 from agriculture only. While CDIAC ceased operation in September 2017, the Open-source Data Inventory for Anthropogenic CO2 (ODIAC) (Oda et al., 2018) continued to use the CDIAC data and combined these with geospatial proxies (including night light satellite maps) to provide CO2 grid maps, as EDGAR is also doing (using other geospatial proxies). In addition, the new Community Emissions Data System (CEDS) of Hoesly et al. (2018) builds upon existing inventories to provide a new gridded dataset of all emission species for the Climate Model Inter-comparison Programme CMIP6. The scientific community started to bring together these anthropogenic BU emissions with top-down estimates covering also the natural component to obtain the Global Carbon Budget (GCB) (Le Quéré et al., 2018) and the Global Methane Budget (Saunois et al., 2016). These budgets are important input for the periodic global stocktake that the Paris Agreement envisages from 2023 onwards (with the submitted inventories for 2021). Even though significant progress in inventory compilation has been made, the overall uncertainty of the global total has become larger over time because the share of emissions from non-Annex I countries (with less developed statistical infrastructure) increased from less than 40 % in 1990 to more than 60 % in 2012, as shown in Fig. 2. To support both science and policy making with the monitoring and verification of the GHG emissions, it is important Earth Syst. Sci. Data, 11, 959–1002, 2019 www.earth-syst-sci-data.net/11/959/2019/ G. Janssens-Maenhout et al.: EDGAR greenhouse gas emissions 961 Figure 1. (a) Inventory submission as received at UNFCCC (by January 2017) for all countries: expressed with the year of emission reporting in which the latest national communication to UNFCCC took place. (b) Inventory submission as received at UNFCCC (by January 2017) for all countries expressed with the latest year of emission that is covered in the inventory submitted to UNFCCC. Figure 2. Relative contribution of the Annex I and non-Annex I countries to the global total GHG emissions. The red, brown and orange dashed parts of the stack correspond to the non-Annex I share that increases from about 1/3 in 1990 to almost 2/3 in 2012. www.earth-syst-sci-data.net/11/959/2019/ Earth Syst. Sci. Data, 11, 959–1002, 2019 962 G. Janssens-Maenhout et al.: EDGAR greenhouse gas emissions that emissions are estimated by using comparable methodologies, consistent source allocation and comprehensive coverage of the globe. The EDGAR v4.3.2 global inventory illustrates the result of a bottom-up technology-based compilation of countryand sector-specific emission time series for 1970–2012. Furthermore, the monthly resolution and global grid maps at a spatial resolution of 0.1× 0.1 allow direct use in atmospheric models as well as in analyses of policy impacts. The first version of the Emissions Database for Global Atmospheric Research (EDGAR v2) answered the needs of the air quality community to map technological parameters of air pollution sources and was published by Olivier et al. (1996). Since then, several updated versions (Olivier, 2002) have been released (EDGAR-HYDE, EDGAR v3.2, EDGAR 3.2 FT2000). Driven by the development of scientific knowledge on emission generating processes and by the availability of more recent information, the EDGAR v4 datasets were constructed including new emission factors and additional end-of-pipe abatement measures. The specification of the combustion technology and its endof-pipe abatement is more important for air pollutants and aerosols than for GHGs. CO2 combustion emissions are fueldetermined and carbon capture and storage are not yet implemented at an opera Collapse"
103348,1990,monthly labor review,baseball labor relations:  the lockout of 1990,baseball labor relations the lockout of 1990,ERR
103352,1990,monthly labor review,contributions to savings and thrift plans,contributions to savings and thrift plans,"In this Beyond the Numbers, we review the evolution of private industry health and retirement plans (with a focus on defined contributions and savings and thrift retirement plans) and provide an analysis using recent estimates from the National Compensation Survey (NCS). Where applicable, we refer back to a 1990 article by George Stelluto and Deborah Klein in the Monthly Labor Review in which they discussed historical trends in employee compensation from the 1930s through 1980s and offered information on future expectations for compensation developments. Collapse"
103353,1990,monthly labor review,new benchmarks and sic codes for establishment survey,new benchmarks and sic codes for establishment survey,ERR
103398,1990,oxford bulletin of economics and statistics,strikes with asymmetric information:  theory and evidence,strikes with asymmetric information theory and evidence,No Result.
103468,1990,public choice,on democracy and debt,on democracy and debt,No Result.
103469,1990,public choice,the bias in centrally planned prices,the bias in centrally planned prices,"This study examined how information technology (IT) influences competitive strategies of thirdparty logistics (3PL) providers in mainland China. First, we examined the influences of IT on the firm’s IT advantage over its competitors. Second, the relationship between IT and competitive strategy was examined. The results show that IT has a significant influence on a firm’s IT advantage and its competitive strategy. INTRODUCTION Information technology (IT) offers opportunities to provide competitive operation advantages like logistical efficiency, effectiveness and flexibility (Sanders & Premus, 2002). Yet, due to the high cost of IT and lack of expertise, the IT adoption rate of logistics users is still low (Sum, Teo, & Ng, 2001). One implication of this low IT adoption rate for firms which use logistics is that there are ample opportunities for third-party logistics (3PL) firms to adopt IT and exploit this aspect of their services (Bhatnagar, Sohal & Millen, 1999). Rapid expansion within a company, coupled with continued growth in logistics requirements, also encourages logistics users to outsource their logistics activities to 3PL firms (Sum et al.). Logistics outsourcing, through 3PL firms, has become a rapidly expanding source of competitive advantage and logistics cost savings (Rabinovich, Windle, Dresner & Corsi, 1999). Many Fortune-500 companies have now outsourced transportation, warehouse, and inventory management functions (Burnson, 2000). IT in a 3PL firm plays an essential role in synchronizing and coordinating complex supply chain activities between logistics users and their customers and is therefore an important strategic capability for 3PL firms. Although there are limited studies addressing how 3PL firms build high level IT capability, those that have recognize the inherent difficulties (Lewis & Talalayevski, 2000; Stone, 2001). Previous studies of IT have mainly focused on the value of IT, such as cost reduction and productivity improvement. Only limited studies have investigated the relationships that may exist between IT and the strategy that a company pursues. We examine these issues within 3PL firms F. Lai, K. Zantow, D. Li & Q. Wang 2008 Volume 17, Numbers 3/4 248 in China. China’s 3PL firms represent a unique opportunity to address issues related to IT and strategy. The entry of China into the World Trade Organization (WTO) has resulted in tremendous growth in business within China, and the logistics industry there is growing. Increased competition in China’s logistics industry has forced many 3PLs to review their strategies, and the value propositions they represent to their clients. Our paper proceeds as follows. We first present a review of the literature, followed by discussion our research methodology. Next we present results of the analysis, and discuss our findings and limitations. Finally, we present our conclusions. LITERATURE REVIEW AND HYPOTHESE Within the logistics literature, IT plays an important role and has been promoted as a means to enhance logistics competitiveness (Bowersox & Daugherty, 1995; Daugherty, Ellinger & Rogers, 1995), increase capability, decrease cost, and improve service (Closs, Goldsby & Clinton, 1997). Although IT was considered a key component in future logistics systems (Dawe, 1994) and showed growth trends in logistics, many managers still caution the use of IT, due to high technology cost (e.g., DRP systems and satellite communications systems), risk of organizational damage during implementation, and lack of demonstrated effectiveness (Sum et al., 2001). Other reasons for organizations’ hesitancy to invest in IT include the expected obsolescence of hardware and software, application redundancy, and irrelevance of applications to a firm’s particular industry and information needs (Dawe, 1994). Bowersox, Daugherty, Dröge, Rogers and Wardlow (1989) refers to logistics managers’ inability or unwillingness to adopt IT, despite the influence applications may have on firm success/failure, as the “information gap”. We are interested in the relationship of those factors to the strategy that a firm pursues, and to perceived IT advantage. The model is presented in Figure 1. IT Strategic Posture We represent IT strategic posture as being composed of: the importance placed on IT (IT Importance hereafter), the degree of resource effort devoted to IT (IT Effort hereafter), and the managerial involvement in the strategic planning (IT Involvement hereafter) (c.f., Wang, Lai, & Zhao, 2008; Lai, Wang & Zhao, 2006). The relationships between these three components are not the focus of the present study, which had been examined in Wang et al. (2008) and Lai et al. (2006). However, to make the model appropriately specified and avoid potential biases of parameter estimations, we still incorporate the relationships into our model. Therefore, we have: H1a: IT importance has significant and positive influences on IT Efforts H1b: IT importance has significant and positive influences on IT Involvement Linking IT & Competitive Strategy Journal of International Technology and Information Management 249 Figure 1: The Research Model. IT Advantage IT strategy must be integrated with the overall corporate strategy and united with all the organization's other competitive strategies. All levels of management must develop competitive strategies that emphasize the central role of IT (Mattson, Beheshti & Salehi-Sangari, 2000). Strategy alignment theory emphasizes top management participation for aligning IT strategies and business strategies. An alignment between its IT and business strategies enables an organization to acquire, deploy, and leverage its IT investments and capabilities effectively in pursuit of its business strategies and in support of its business activities (Reich & Benbasat, 2000; Hirschheim & Sabherwal, 2001). This alignment might be facilitated through an understanding of the overall organizational objectives by top management. Lederer and Burky (1989) showed that IT executives who participate more in strategic planning believe that they have a better understanding of top management's objectives than do those who participate less. Zmud (1988) argued that structural mechanisms (e.g., steering committees, technology transfer groups) associated with communications and management systems (e.g., planning and control mechanisms) are needed to build IT-line partnerships for the successful introduction of new technologies. Boynton et al. (1994) also suggests that the effective application of IT depends on the interactions and exchanges that bind IT and line managers. Therefore we hypothesize: H2a: IT involvement has significant and positive influences on IT advantage Top management championship, including top management beliefs and participation, may also determinately influence IT adoption and assimilation (Chatterjee et al., 2002). Top management championship is a metastructuring action which defines institutional norms and values regarding how managers should engage in structuring actions related to IT (Chatterjee et al., 2002). F. Lai, K. Zantow, D. Li & Q. Wang 2008 Volume 17, Numbers 3/4 250 Chatterjee et al. (2002) defined top management championship in terms of managerial beliefs about IT initiatives and participation in those initiatives. In firms where top managers believe that IT offers a strategic opportunity and actively participate in the IT initiatives, their beliefs and participation serve as powerful signals to others in the managerial community about the importance placed on IT adoption and assimilation. Through their beliefs, top management can offer vision and guidelines to managers about the opportunities and risks in assimilating IT innovation. Further, top management can legitimize the willingness and energy of managers to commit their effort to IT adoption (Chatterjee et al., 2002). Studies demonstrated that the top management beliefs and participation significantly influence the web assimilation (e.g. Chatterjee et al., 2002). Therefore, we believe that top management beliefs and participation in IT can promote and improve IT advantage and capability. H2b: IT importance has significant and positive influences on IT advantage. Goal theory suggests that effort is the most immediate determinant of performance (Locke et al., 1988). Kren (1990) proposes an extended expectancy theory model that combines expectancy theory and goal theory. In this model, greater effort to reach an objective leads to better performance. Meanwhile, higher levels of resource commitment may help a firm to acquire such competitive IT equipments as hardware, software, network, and databases. Together with appropriate strategic integration with business objectives, the competitive IT equipments may facilitate the achievement of IT advantage over competitors. Therefore, we hypothesize: H2c: IT effort has significant and positive influences on IT advantage. Competitive Strategy In Porter’s (1980, 1985) view, competitive advantage comes from either being able to reduce costs below the industry average or through differentiating products or services in some way that would entice customers to pay above average prices for the product or service. Porter (1980) referred to these as cost leadership and differentiation strategies. IT can be seen as a tangible resource that is commonly available, which may contribute to the success of a company pursuing either a cost leadership or differentiation strategy (Hill, 1988). IT affects the competitive advantage of a company (Lea, 2005; Evans & Neu, 2008) by influencing the cost drivers or uniqueness drivers of value chain activities (Porter, 1985). Internal processes and activities, as resources, are used by companies as methods of increasing communications, quality, and performance (Fedorowicz et al., 2004; Whitten, 2004). Although IT may help firms to achieve differentiation and cost leadership, the logistics literature has shown that firms that Collapse"
103470,1990,public choice,are economic statistics overproduced?,are economic statistics overproduced,No Result.
103471,1990,public choice,"instrument choice, political reform and economic welfare",instrument choice political reform and economic welfare,"The purpose of this paper is to formally examine the effect of placing constraints, such as the line-item veto or a balanced budget amendment, on legislative behavior. There are two basic findings that emerge from the analysis. First, constraints on one type of instrument, such as spending, will in general result in more widespread use of other kinds of instruments, such as regulation. Second, it is naive to conclude that constraints on legislative behavior will promote economic efficiency and/or reduce the growth of government. The primary contribution of the paper is to suggest how changes in the political environment can affect instrument choice, economic welfare, and the size of government. Collapse"
103472,1990,public choice,"an alternative to ""a public choice theory of the great contraction.""",an alternative to a public choice theory of the great contraction,"In the October 1988 edition of this journal Anderson, Shughart and Tollison (1988: 4) argued that ""restrictive monetary policy of the Fed in the 1929-1933 period was ... rational, self-interested behavior."" They contend that what Friedman and Schwartz wrongly interpreted as inept monetary policy was actually the calculated campaign of a bureaucracy bent on consolidating its power base. They imply that the Federal Reserve Board of Governors, in order to purge nonmember banks, purposely disregarded policy options that would have greatly reduced bank failures, increased the money supply, and hastened economic recovery. Anderson et al. (hereafter AST their regressions a meaningless mathematical exercise; and their public choice interpretation of Fed intentions, a mirage. Although the percentage of commercial banks belonging to the Fed steadily increased during the Great Contraction, AS&T's 5 to 1 failure rate ratio is seriously misleading. It is based upon a severely limited historical context and misinterpretations of both hazard rates and rates of change. Collapse"
103473,1990,public choice,the ruthless fed: a critique of the ast hypothesis,the ruthless fed a critique of the ast hypothesis,"ConclusionThis paper examines the hypothesis that the Great Contraction was the result of rational rent-seeking by members of the Federal Reserve System. In contrast to the AST hypothesis, evidence on the share prices of member banks that survived the contraction suggests that the owners of these banks suffered an absolute decline in real wealth and a decline relative to a broad spectrum of other investment alternatives. Furthermore, monetary surprises had no statistically discernible effect on the share prices of these banks. This evidence conflicts with the notion that rational rent-seeking would lead the owners of member banks and their bureaucratic conspirators in the Federal Reserve System to unleash a policy with the goal of contracting the money supply by 35 percent. Collapse"
103474,1990,public choice,a public choice theory of the great contraction: further evidence,a public choice theory of the great contraction further evidence,"In recent issues of this journal, perhaps no other paper has inspired as intense interest as our (Anderson, Shughart and Tollison, 1988) challenge to the Friedman-Schwartz (1963) ""mistake theory"" of the Great Contraction. In that article we advanced an hypothesis of Fed behavior grounded in the political self-interests of the banking system's regulators and their overseers in Congress. Specifically, we argued that the Fed obtained two important benefits from the massive contraction in the money supply over which it presided between late 1929 and the middle of 1930. First, because of the differentially higher failure rates of nonmember banks during this period, the proportion of the commercial banking industry under the Fed's control increased dramatically. The second benefit came in the form of a change in the Fed's method of finance which freed the Board of Governors from the normal congressional budgetary process. Putting the Great Contraction in interest-group terms, we suggested that during 1929-1933, the Fed was acting rationally as the agent of congressmen serving on important oversight committees who, in turn, were representing the interests of the member banks in their states and districts. We tested this hypothesis using bank failure rate data across states and found that, holding other things equal, the failure rates of nonmember banks in the early 1930s were significantly higher in states with representation on the House Banking Collapse"
103475,1990,public choice,the effect of rent-seeking on family income levels: some suggestive empirical evidence,the effect of rent seeking on family income levels  some suggestive empirical evidence,ERR
103498,1990,rand journal of economics,the diffusion of new technologies:  evidence from the electric utility industry,the diffusion of new technologies evidence from the electric utility industry,"This article investigates the effect of firm size and ownership structure on technology adoption decisions using data on the electric utility industry. We argue that traditional models of technology diffusion may be subject to sample selectivity biases that overstate the effect of firm size on adoption probabilities. By extending conventional hazard rate models to use information on both adoption and nonadoption decisions, we differentiate between firms' opportunities for adoption and their underlying adoption propensities. The results suggest that large firms and investor-owned electric utilities are likely to adopt new technologies earlier than are their smaller and publicly owned counterparts. Moreover, the selection biases from conventional statistical models may overstate size effects and understate ownership and factor cost effects by as much as a factor of two. Collapse"
103499,1990,rand journal of economics,auditor reputation:  the impact of critical reports issued by government inspectors,auditor reputation the impact of critical reports issued by government inspectors,"In recent years U.K. Department of Trade investigations into the affairs of a specific company have sometimes been critical of the work done by its auditors and reporting accountants. This article assesses the impact of this criticism on the auditor's reputation. In particular, it examines the stock price performance of the auditor's listed clients. The results show a small wealth loss for the audit clients. The article also examines the impact of the Department of Trade criticism on an audit firm's number of listed clients and its future audit fees. Compared to a control group, the criticized auditors appear to suffer a small loss in market share based on number of listed clients and on audit fees. The evidence suggests that criticized auditors incurred economic losses from the damage to their reputations. This is consistent with models of reputation advanced in the industrial organization literature. Collapse"
103501,1990,rand journal of economics,"trade association disclosure rules, incentives to share information, and welfare",trade association disclosure rules incentives to share information and welfare,"In this article I propose a monopolistic competition framework to analyze the effects of different disclosure rules used by trade associations on the incentives to share information and on the welfare of consumers, firms, and society. This framework is appropriate whenever a single firm cannot influence aggregate market magnitudes, and serves as a benchmark for the analysis of information-pooling agreements abstracting from strategic considerations. I report two main results. First, a policy of nonexclusionary disclosure destroys the incentives to share information, while exclusionary disclosure preserves them. Second, information sharing increases expected total surplus with Cournot competition but decreases it with Bertrand competition in the context of a Quadratic-Normal model with demand uncertainty. Collapse"
103504,1990,rand journal of economics,"information, advertising and health choices:  a study of the cereal market",information advertising and health choices a study of the cereal market,"This article examines the effects of information on consumer and producer behavior by focusing on the ready-to-eat cereal market during a period in which information developed about the health benefits of fiber cereal consumption. Although cereal producers were initially prohibited from advertising these health benefits, the regulatory ban against producer advertising was lifted during the period we study. Our results indicate that consumers changed their behavior once informed of the health benefits and that advertising was an important source of information once the ban was lifted. Producer health claims about fiber also led to significant product innovation and did not cause adverse effects in other health dimensions of cereal consumption. Government and general information sources had limited impact on fiber cereal choices in the years prior to the advertising. Analysis of individual food consumption data indicates that theories of information acquisition are important in explaining who responds most quickly to new information; household and individual characteristics that reflect costs of acquiring information, ability to process information, and valuation of health are all important determinants of fiber cereal choices. Moreover, the evidence suggests that advertising reduced the differences across consumers by lowering the costs of acquiring information for broad segments of the population. In contrast, the information processing advantages due to education were not reduced by advertising. Collapse"
103527,1990,regional studies,nike just did it: international subcontracting and flexibility in athletic footwear production,nike just did it international subcontracting and flexibility in athletic footwear production,"DONAGHU M. T. and BARFF R. (1990) Nike just did it: international subcontracting and flexibility in athletic footwear production, Reg. Studies 24, 537–552. Over its twenty-five year history, Nike has developed a complex set of production alliances with manufacturers in South East Asia. Athletic shoe production now takes place under several different subcontracting arrangements. Based on interviews with company officials, this paper describes in detail Nike's present production partnerships, which include conventional capacity and complementary subcontracting types, as well as newer forms. The study opens with brief histories of the athletic shoe industry and the company. Analysis is directed toward understanding the global production of Nike shoes in terms of the ‘New International Division of Labour’, the nature and form of vertical disintegration and production subcontracting, and the current debate surrounding industrial system flexibility. We show that Nike's subcontracting system allows the company a... Collapse"
103528,1990,regional studies,spatial implications of the national health service white paper,spatial implications of the national health service white paper,"Countries seeking to remake the structure of their societies have placed a strong emphasis on the development of a viable and effective system of local government. The restructuring of local government is especially significant in the context of South Africa's emerging democracy. A key component of this process has been boundary delimitation, which involved a process of spatial organisation and re-organisation. In South Africa the de-racialisation of local government represents a major challenge. Many affluent white local authorities were reluctant to give up the power and privileges of the old order and merge with previously black local authorities. Also, the socio-spatial distortions of the apartheid era need to be addressed through a more equitable distribution of resources, and the re-drawing of geographical boundaries. Attempts at municipal restructuring in South Africa have been fraught with problems and conflicts. The conflicts engendered, the negotiations, compromises, and coalitions generated constitute important areas of research. Examining and elucidating the manner in which these various forces have manifested themselves in the major metropolitan centres is the central theme of this paper. The focus of this paper is on boundary delimitation in the Durban Metropolitan area. Introduction Countries undergoing political and economic transition have placed a strong emphasis on the development of a viable and effective system of local government (Mawhood, 1993; Maharaj, 1997). A critical issue is the “nature of local territorially-based communities and their potential for democratic self-governance within the complex political and economic environment” (Sancton, 1996:277). The restructuring of local government has been primarily “influenced by economic, technological, political and ideological change”, and can “modify spatial patterns of development and disparities” (Razin, 2000:7). In South Africa the de-racialisation of local government represents a major challenge. 1 Paper presented in the IASTE conference “[un]bounding traditions: the tensions of borders and regions” held in Honk-Kong (December 2002) “Rencontres de l’innovation territoriale 2 The restructuring of local government is especially significant in the context of South Africa's emerging democracy, especially since this transformation ""has taken place in a way that is probably unique from an international comparative perspective"" (Swilling, Monteiro and Johnson, 1995:16). A key component of this process has been spatial re-organisation through boundary delimitation. However, boundaries are not neutral geographic lines. Boundary changes are often associated with a redistribution of political power and resources, with some institutions and parties benefiting, and others being disadvantaged (Alexander, 1982; Hasson and Razin, 1990; Keating, 1995; Cameron, 1999). Quite often boundary conflicts are associated with urban municipalities encroaching into rural areas. Unless there are redistributive transfers, urban-rural inequalities are accentuated (Razin, 2000). In a process of territorial restructuring, there are potential areas of dispute, which often reveal local, regional and national geopolitical stakes (Hasson and Razin, 1990). Therefore, local and regional spatial restructuring led to contestations and conflicts over the delimitation of new boundaries in South Africa (Ramutsindela and Simon, 1999; Narsiah and Maharaj, 1997; 1999; Ramutsindela, 2001). The conflicts engendered, the negotiations, compromises, and coalitions generated constitute important areas of research. The aim of this paper is to analyse the effects of the contemporary territorial and administrative restructuring on urban dynamics in South Africa. More specifically, the focus is on how the process of territorial restructuring impacted on metropolitan areas as well as their hinterlands. The political and economic implications associated with attempts to extend urban boundaries into rural areas will also be assessed. It is important to understand how boundary rationalisation and political power intersect. The nature, size, composition and governance of regions are constantly changing depending on the balance of societal forces and the dominant mode of accumulation (Markusen, 1978). Political, territorial and administrative reorganisation therefore needs to be theorised in terms of the articulation of a number of structures including those associated with ideology, bureaucracy, nationalism, gender and regimes of accumulation (Driver, 1991; Murphy, 1991). “Rencontres de l’innovation territoriale 3 Local Government in Transition in South Africa Since the 1990s South Africa embarked on the long journey towards reconstruction, development and planning in the post-apartheid era. Success would depend on a sensitive understanding of the ""geographical legacy of apartheid and the scars it has left behind, and also to the complex local, regional and environmental diversity that characterises the South African whole"" (McCarthy, 1991:23). The contemporary South African city is reflective of a discourse of apartheid urban planning characterised by racially fragmented and discontinuous land use and settlement patterns, haphazard, dysfunctional and inefficient spatial ordering, land use mismatches, low level population density and the concentration of the poor in relatively high density areas on the peripheries and the rich in the core intermediate urban areas (Hindson et al, 1992:6). Local government in the apartheid city was characterised by a ""complex process of functional inclusion, spatial separation, and political exclusion"" (Swilling, et al. 1991:175). In the democratic era the Local Government Transition Act (LGTA) of 1994 provided an institutional base for the disbanding of race-based municipalities, scrapping of apartheid laws relating to local government, and the establishment of transitional local councils (Khan and Maharaj, 1997). About 1 260 local authorities were merged to form 843 municipalities. However, the type of local government that materialised in terms of the LGTA did not ""necessarily support the notion of 'one city' or 'one tax base'"" (Wooldridge, 2002:132). In many areas the spatial inscription which prescribes the separation of areas of abject poverty from areas of affluence persists (Narsiah and Maharaj, 1999). For example, there was a tendency to exclude ""settlements on the periphery of the metropolis which would lower the per capita tax base, and place a strain on service delivery capacity"" (Wooldridge, 2002:132). Furthermore, ""hastily drawn municipal boundaries and negotiated ward boundaries produced a fragmented system, with metropolitan areas divided into impractical substructures, and some small towns and rural areas cut up like a patchwork quilt"" (Cashden, 2002:162). “Rencontres de l’innovation territoriale 4 In an attempt to address these problems the White Paper on Local Government provided the foundation for a new developmental local government (DLG) system (Government Gazette, 13/3/98, p. 15). It introduced the notion of the ‘wall to wall principle’ in order to ensure that all the parts of the country, including the poor rural peripheries, will be integrated in the new demarcation process. DLG exhorts local authorities to focus on achieving developmental outcomes, such as the provision of basic infrastructure and services; the creation of integrated cities and livable environments; the encouragement of local economic development initiatives; and the empowerment of communities. Three inter-linked approaches to help municipalities to effectively play a developmental role were proposed: ""integrated development planning and budgeting; performance management; and working together with local citizens and partners"" (Government Gazette, 13/3/98, p.16). An important strategy to help municipalities to become more developmental is integrated development planning (IDP). Integrated development planning depends on the coordination of a range of services and regulations, including land-use planning, household infrastructure, environmental management, transport, health and education, safety and security and housing (Government Gazette, 13 March 1998, p.39). The type of municipal institutions that would best suit South African conditions was the subject of considerable discussion. The issue of their shape, size, and boundaries was directly linked to the socio-economic dynamics. There were two different perspectives with regard to metropolitan government. For those who were interested in the growth imperative metropolitan government plays a pivotal role in promoting economic development because it provides a more effective regional infrastructure for reproducing labour power as well as promoting production and distribution; offered tax exemptions and subsidies to attract investment; and facilitated the establishment of pro-growth partnerships with the private sector. Those who favoured the equity focus argued that metropolitan government could promote more equitable land use; ensure fairer taxation; improve efficiency, service provision and capacity; reduce socio-spatial inequalities and foster rational planning (PLANACT, 1992; Government Gazette, 13/3/98, pp.81-84). In South Africa the White Paper on Local Government defined metropolitan areas as follows: Metropolitan areas are large urban settlements with high population densities, complex and diversified economies, and a high degree of functional integration across a larger geographic “Rencontres de l’innovation territoriale 5 area than the normal jurisdiction of a municipality. Economic and social activities transcend municipal boundaries, and metropolitan residents may live in one locality, work in another, and utilise recreational facilities across the metropolitan area (Government Gazette, 13/3/98, p.78). Mega cities were established in South Africa's six metropolitan areas (Johannesburg, Pretoria, East Rand, Du Collapse"
103529,1990,regional studies,a house price based regional policy,a house price based regional policy,"Abstract Hansen and Singleton (1982) and Dunn and Singleton (1986) have found supporting evidence for the overidentifying restrictions of two empirical consumption-based asset pricing models, when estimated with a particular set of single asset returns. In this paper, we submit these models to further scrutiny by testing whether they exhibit (structural) stability. A series of tests, recently developed by Ghysels and Hall (1990), are applied and a test for structuralinvariance is introduced based on the likelihood ratio type test procedure of Eichenbaum, Hansen, and Singleton (1988). There are a number of reasons why structural stability tests are particularly appropriate for diagnostic testing of Euler equation models, namely: (1) the Lucas econometric policy evaluation critique; (2) Euler equations are only a partial description of the data-generating process and parameter stability is one of the few assumptions imposed in their estimation via the generalized method of moments; and finally (3) it is demonstrated that Hansen's overidentifying restrictions test has no power against a class of local alternatives characterized by a parameter drift. Collapse"
103536,1990,regional science and urban economics,on welfare theory and urban economics,on welfare theory and urban economics,"The publication of the Club of Rome’s landmark report ‘The Limits to Growth’ in 1972 shook the intellectual foundations of social theory and challenged the very premises on which modern economy and prosperity are based. Once set in motion, it led to a revolutionary re-evaluation of human aspirations and economic activities. Among its many consequences, it has stimulated creative minds to look freshly at the underlying processes governing the wealth and welfare of nations. The article then traces their creative impact on the mind of one of the most original economic theorists of our age – Orio Giarini.* As ‘The Limits to Growth’ alarmed the world by the unsustainability and dire consequences of unbridled economic growth, Giarini offers a correspondingly affirmative vision of economics with unlimited potential for wealth and welfare.† Forty years ago was a crucial turning point in human affairs, though it was poorly understood, disputed and even denied at that time. Three events stand out for their particular significance: the end of the Gold Standard in 1971, the publication of The Limits to Growth in 1972, and the first oil crisis in 1973. They marked the end of an era of rapid economic development for the industrialized countries, unalloyed optimism and unquestioned faith in capitalism, the beginning of a period of increasing doubt and uncertainty, which has now culminated in a multi-dimensional crisis of unparalleled proportions. Since then, the world has been wracked by increasing financial instability demarcated by more than 200 monetary currency crises and 145 large-scale banking crises. The average growth rate in Western Europe declined progressively from 6.1% in the 50s to 4.8% in the 60s and 2.3% in the 70s, then fell to less than 2% since 1990. The initial quadrupling of oil prices, an alarming reminder that non-renewable resources are actually non-renewable, spurred price inflation in the late 70s, which was only kept under control by tight monetary policy, slower growth, and rising levels of unemployment and income inequality in the decades that followed. * The events described in this narrative of the early history of the Club of Rome are based on a chapter in Orio Giarini’s French autobiography, Itinerary Towards Retirement at 80, ( in French : “La retraite à 8o ans “ ), Economica – Paris, 2003. † The attack was on growth as defined quantitatively and measured by GDP. For discussion, see Orio Giarini et al., “Introductory Paper for a Programme on The Wealth of Nations Revisited,” Cadmus 1, no.1 (2010): 9-27. http://cadmusjournal.org/article/issue-1/introductory-paper-programme-wealth-nationsrevisited and Garry Jacobs and Ivo Šlaus, “Indicators of Economic Progress: The Power of Measurement and Human Welfare,” Cadmus 1, no.1 (2010): 53-113. http://cadmusjournal.org/article/issue-1/indicators-economic-progress-power-measurement-and-human-welfare Collapse"
103570,1990,review of economics and statistics,a comparison of the forecasting ability of ecm and var models,a comparison of the forecasting ability of ecm and var models,The results of forecasting experiments based on an error correction mechanism (ECM) model and various types of vector autoregressive (VAR) and Bayesian vector autoregressive (BVAR) models are presented. A Bayesian error correction mechanism (BECM) model is also tested. This model represents a hybrid of the BVAR and ECM models. The results from experiments using fifty industries and monthly Ohio labor market data demonstrate that the ECM model produces forecasts with much lower errors than any of the alternative VAR or BVAR models when the variables used in the model pass the statistical tests for cointegration. The findings confirm many of the beliefs expressed by Granger (1986) and Engle and Yoo (1987) based on theoretical consideration of the ECM model versus the VAR model. A result contradictory to the contentions of Engle and Yoo is that the BECM model performs well at the longer forecast horizons for both cointegrated and non-cointegrated industries. Collapse
103572,1990,review of economics and statistics,"a re-examination of the monetary model of exchange market pressure: canada, 1963-1988",a re examination of the monetary model of exchange market pressure  canada  1963 1988,"In re-evaluating the performance of the Girton and Roper (1977) two-country monetary model of exchange-market pressure (EMP) for Canada, estimates are provided for an extended specification that allows for unrestricted dynamics. Results show the hypothesized negative effect of domestic credit growth on EMP to be supported throughout. The relevance both of the dynamics and of the hypothesized unitary tradeoff between exchange rate and international reserve movements for Bank of Canada policy is demonstrated for the sequence of fixed and floating exchange rate regimes contained in the 1963:1-1988:1 sample. Collapse"
103573,1990,review of economics and statistics,"entry, rivalry and free banking in antebellum america",entry rivalry and free banking in antebellum america,ERR
103592,1990,review of financial studies,asymmetric information and the medium of exchange in takeovers: theory and tests,asymmetric information and the medium of exchange in takeovers theory and tests,"In a model of takeovers under asymmetric information, we identify a separating equilibrium in which the value of the bidder firm is revealed by the mix of cash and securities used as payment for the target. The model predicts that the revealed bidder value is monotonically increasing and convex in the fraction of the total offer that consists of cash. We examine the model restrictions using data from Canada where mixed offers are both relatively frequent and free of the confounding tax-related options characterizing mixed offers in the U.S.. We find that the average announcement-month bidder abnormal return in mixed offers is large and significant. However, maximum likelihood estimates of parameters in both linear and non-linear cross-sectional regressions fail to support the model predictions. Collapse"
103595,1990,review of income and wealth,the economic well-being of the elderly,the economic well being of the elderly,"D ecision-making regarding the care of the mentally incompetent patient is often complicated by legal,’-7 economic,*-” and ethical factor^'^-'^ that affect case management. The incompetent patient requires an advocate, a responsibility that generally falls upon the family. If a conflict over a patient arises within the family, the result may well be inimical to effective patient management. On June 25th, 1990, the United States Supreme Court issued its decision in Cruzan v Director, Missouri Department of Health, the first case in which the high court considered the termination of life-prolonging measures for incompetent patients. In a 5-4 decision, the Court upheld the decision by the Missouri Supreme Court that the feeding tube of Nancy Cruzan, a patient in a persistent vegetative state since 1983 could not be removed at the request of her parents. The reason was that there was no ’clear and convincing’ evidence that the patient when competent had specifically indicated that she would not wish to be tube-fed if she ever became irreversibly unconscious. The narrow import of the decision in Cruzan must be stressed. The Court ruled that it is constitutionally permissible for a state to require that life-prolonging measures for an incompetent patient remain in place unless the patient had left behind a clear directive excluding such measures in the event of a future state of incompetency. The Court did not rule that such a state rule is mandatory but simply that a state may so rule if it chooses. The decision in Cruzan thus leaves intact the long line of cases, beginning with In the Matter of Karen Quinlan, in which state supreme courts have held that decisions to terminate life-prolonging treatment for incompetent patients should rest with the patient’s family and physicians. The courts have further ruled that the courtroom should be the forum of last resort for decision-making involving the tennination of life-prolonging treatment for incompetent patients. In the view of the judges, however, they have a roIe to play when there is in-family conflict that cannot be resolved in the health care setting. In that event, judicial action is called for in which either the court makes the treatment decision or else vests that authority in a guardian appointed to act on the patient’s behalf. (The trend in the more recent cases is to assign the decision to a guardian, not to the court itself.) The “Daughter from California Syndrome” describes one such in-family predicament. In this case an adult daughter, who had not seen her mother for 5 years, appeared on the scene when critical health care options were being considered for her incompetent mother. Upon being confronted with her mother’s condition, the daughter responded with acute denial as well as anger and resentment directed against the medical staff. She refused to come to terms with her mother’s condition, demanded inappropriate aggressive care, and impeded the management of the case. offer a possibIe soIution to these problems by allowing competent individuals to choose their own health care before they become incompetent. We report this case because it illustrates many of the features of this syndrome. Health care Collapse"
103596,1990,review of income and wealth,the welfare state and poverty in finland and sweden from the mid-1960s to the mid-1980s,the welfare state and poverty in finland and sweden from the mid 1960s to the mid 1980s,"Using poverty lines representing the fixed basket of goods and services, the development over time of poverty in Finland and Sweden are compared. In both countries, poverty decreased rapidly between the mid‐1960s and the mid‐1970s, after which changes have been less dramatic. During the first part of the 1980s poverty continued to decrease in Finland, but increased in Sweden. Comparisons for age‐groups showed large reductions in poverty rates among the aged in both countries. Poverty has shifted from the permanent old age poverty towards a more temporary poverty in young adulthood. International comparisons show that in the early 1980s both Finland and Sweden had poverty rates below the average of the affluent Western nations. Furthermore, these comparisons suggest that cross‐national variations in poverty rates are partly explained by the size of the welfare state. Also, time series analysis shows that income transfers have taken an increasing number of people out of poverty in both countries. Collapse"
103597,1990,review of income and wealth,netting out income taxes for different types of income of households,netting out income taxes for different types of income of households,"Income is not a well-defined term either in economic statistics or in economic theory. In the national accounts certain definitions of income have been convened and internationally standardised, but the ongoing debate about the coming revision of the United Nations' SNA has shown that different views continue to exist as regarding the conceptual background and the systemic embedding of those definitions. One issue in this context is the distinction between income paid and income received within a pair of economic transactors. Income is a classical area of government interference in the circuit of economic flows for the purpose of partly correcting the original income distribution, partly of financing its own spending. A view of national accounts shows the difficulties that occur based on the fact that the simple theoretic equality of income paid and income received is not valid in many economic fields. The most conspicuous case is that of indirect taxes, which form part of value added, i.e. income originated in production, but not assigned to either of the production factors. Although this is an old and well-known case and its implications have been accepted in national accounting, they have hardly been reflected in economic theory or political practice. Even less attention has been devoted to the case of direct taxes although in quantitative terms it is equally relevant. The question is simple. A gap between income paid and income received is generated by direct taxes as well as by indirect taxes so that one would like to know the true net result. What amount remains with the households of each type of income after the government has interfered? For economic analysis the opposition of income from labour and income from capital, each being netted from its direct tax component may be of interest. The income politics of trade unions and of employers can hardly result in decent compromises if the different view points from which income paid and income received must be studied are not taken into account. In the Federal Republic of Germany a concept of ""net compensation of employees"" was developed in connection with the early national accounts, although neither of the relevant international systems recommend it. It was later complemented by the concept of ""net entrepreneurial and property income."" Also transfer incomes were balanced against their share of income tax (Hamer, 1964). It seems that similar ideas are being pursued in the United Kingdom (Central Statistical Office, 1988). The fact that such compilations have not become Collapse"
103598,1990,review of income and wealth,units in national accounts and the basic system of economic statistics,units in national accounts and the basic system of economic statistics,"Units which are actually used in legal-administrative reality often are not directly suitable as statistical units. For the purposes of the statistical description of the economic processes a transformation is needed. In this paper I examine the consequences of a changing legal-administrative reality to the definition of statistical units, particularly in the light of the SNA revision now under way. The National Accounts comprise a schematic description of the economic processes that constitute a national economy. This implies that a multi-form actual situation is stylized and reduced to quantifiable simplicity in accordance with a set of definitions and definition equations which in turn are based on underlying economic theories. Traditionally, in the National Accounts two main lines can be distinguished. One is aimed at the description of production processes. The other one focuses on the estimation of macroeconomic data such as national income, etc. or on the description of how these macro-aggregates are generated, distributed and spent. For both approaches it can be said that they lead to a description of reality which is not objective and which, if considered from another point of view, incorporates certain distortions. These are a consequence of the axioms and definitions of the underlying theories and the way in which they are made operational. According to Archipof (1985) National Accounts data are created as a projection of basic data, a projection which involves aggregation and transformation processes. In this view the transformation processes are based on a set of a priori equations which determine the coherence of the system and establish which confrontations must be made. The commodity flow method can be seen as an example of this. It starts out from the equivalence of the sum of production and imports to the sum of consumption, investment, exports and changes in stocks. Depending on the set of equations chosen several National Accounts systems can be realised. It must be stressed though, that National Accounts are meant to be a description of economic reality. The transformations necessary to realise a description of reality relate to a variety of aspects; for example, the description of economic agents and the deeds of these agents. In the National Accounts and the underlying statistics, these Note: The views expressed in this paper are those of the author and do not necessarely reflect the views of the Netherlands Central Bureau of Statistics. Collapse"
103599,1990,review of income and wealth,stock flow consistent income for industrial and commercial companies,stock flow consistent income for industrial and commercial companies,"Although industrial and commercial companies (ICCs) are primarily concerned with the production of goods and services which yield a trading profit there is evidence that they have become increasingly concerned with their portfolios of tangible and financial assets and liabilities. As relative prices, interest rates and exchange rates alter, there are implied changes to the realisable net worth of ICCs. That these changes can be substantial is particularly illustrated by reference to the U.K. experience between 1980 and 1982 and in 1985. However, conventional transactions based income measures are purposely not designed to capture these changes. Measuring income on a stock-flow consistent basis provides a complementary view of the performance of ICCs which is intended to capture these portfolio effects. Collapse"
103601,1990,review of income and wealth,economic burdens of marital disruptions: a comparison of the united states and the federal republic of germany,economic burdens of marital disruptions a comparison of the united states and the federal republic of germany,"Longitudinal data from the Panel Study of Income Dynamics and the German Socio-Economic Panel are used to show that women and children are worse off following a marital split than are men in both the United States and Germany. The size of the difference is sensitive to the equivalence scale used, but despite its far more extensive tax and transfer system the disparate impact of divorce or separation on women and children persisted in Germany at a level at least as high as in the United States. Collapse"
103602,1990,review of income and wealth,major changes proposed for the next sna: an overview,major changes proposed for the next sna an overview,The first draft of the new manual for the System of National Accounts will be circulated during 1990. The major changes to be incorporated in it are highlighted in this article. A full list of all changes is available from the United Nations Statistical Office and will be included as an annex to the final manual. Collapse
103604,1990,review of income and wealth,"gender wage differences in australia, sweden and the united states",gender wage differences in australia sweden and the united states,"In this paper I use microdata from the Luxembourg Income Study t s investigate the contributions of industrial structure, occupational mix and personal and family characteristics to observed gender differences in wages in Australia, Sweden and the United States. A particular effort is made to analyse differences in distribution as well as level of wages. The conclusion reached is that different factors determine the wages of low- and high-wage workers. For higher-wage workers, personal and family characteristics are important explanations for wage variation. For lower-wage workers, occupation plays a more significant role. Collapse"
103605,1990,review of income and wealth,aspects of poverty in greece,aspects of poverty in greece,"Dependence of damage estimates upon assumptions of economic growth and technological development Greater economic growth could, by increasing emissions, lead to greater damages from climate change. On the other hand, by increasing wealth and advancing technological development and human capital, economic growth would also increase a society's adaptive capacity and reduce those damages. Although analyses of the impacts (or damages) of climate change generally incorporate economic growth into the emissions and climate change scenarios that they use as inputs, these analyses do not adequately account for the increase in adaptive capacity resulting from that very growth. Because of this inconsistency, these analyses generally tend to overstate impacts. For instance, the average GDP per capita for developing countries in 2100 is projected to be $11,000 (in 1990 US$, at market exchange rates) under A2, the slowest economic growth scenario, and $66,500 under A1, the scenario with both the greatest economic growth and largest climate change. By comparison, in 1990 the GDP per capita for Greece, for example, was $8,300 while Switzerland, the country with the highest income level at that time, had a GDP per capita of $34,000. Based on historical experience, one should expect that at the high levels of GDP per capita projected by the IPCC scenarios in 2100, wealth-driven increases in adaptive capacity alone should virtually eliminate damages from many climate-sensitive hazards, e.g., malaria and hunger, whether or not these damages are caused by climate change. Current damage estimates are inflated further because they usually do not adequately account for secular (time-dependent) improvements in technology that, if history is any guide, ought to occur in the future unrelated to economic development. A compelling argument for reducing greenhouse gases is that it would help developing countries cope with climate change. It is asserted that they need this help because their adaptive capacity is weak. Although often true today, this assertion becomes increasingly invalid in the future if developing countries become wealthier and more technologically advanced, per the IPCC's scenarios. Damage assessments frequently overlook this. Are scenario storylines internally consistent in light of historical experience? Regardless of whether the economic growth assumptions used in the IPCC scenarios are justified, their specifications regarding the relationship between wealth and technological ability are, in general, inconsistent with the lessons of economic history. They assume that the less wealthy societies depicted by the B1 and B2 scenarios would have greater environmental protection and employ cleaner and more efficient technologies than the wealthier society characterized by the A1F1 scenario. This contradicts general experience in the real world, where richer countries usually have cleaner technologies. Under the IPCC scenarios, the richer A1 world has the same population as the poorer B1 world, but in fact total fertility rates — a key determinant of population growth rates — are, by and large, lower for richer nations and, over time, have dropped for any given level of GDP per capita (Goklany 2001a). Merits of reallocating expenditures from mitigation to international development Halting climate change at its 1990 level would annually cost substantially more than the $165 billion estimated for the minimally-effective Kyoto Protocol. According to DEFRA-sponsored studies, in 2085, which is at the limit of the foreseeable future, such a halt would reduce the total global population at risk (PAR) due to both climate change and non-climate-change-related causes by 3 percent for malaria, 21 percent for hunger, and 86 percent for coastal flooding, although the total PAR for water shortage might well increase. The benefits associated with halting climate change — and more — can be obtained more economically through “focused adaptation”, i.e., activities focused on reducing vulnerabilities to the above noted climate-sensitive hazards, or through broadly advancing sustainable development in developing countries by meeting the Millennium Development Goals (MDGs) by 2015. In fact, such efforts, which together could annually cost donor countries $150 billion according to UN Millennium Project and World Health Organization studies, should reduce global malaria, hunger, poverty, and lack of access to safe water and sanitation by 50 percent (each); reduce child and maternal mortality by at least 66 percent; provide universal primary education; and reverse growth in AIDS/HIV, and other major diseases. These numbers also indicate that no matter how important climate change might be in this century, for the next several decades it would be far more beneficial for human well-being, especially in developing countries, to deal with non-climate change related factors. Not only would either focused adaptation or adherence to the MDGs provide greater benefits at lesser costs through the foreseeable future than would any emission reduction scheme, they would help solve today's urgent problems sooner and more certainly. Equally important, they would also increase the ability to deal with tomorrow's problems, whether they are caused by climate change or other factors. None of these claims can be reasonably made on behalf of any mitigation scheme today. Accordingly, over the next few decades the focus of climate policy should be to: (a) broadly advance sustainable development, particularly in developing countries since that would generally enhance their adaptive capacity to cope with the many urgent problems they currently face, including many that are climate-sensitive, (b) specifically reduce vulnerabilities to climate-sensitive problems that are urgent today and might be exacerbated by future climate change, and (c) implement “no-regret” emission reduction measures, while (d) concurrently striving to expand the universe of no-regret options through research and development to increase the variety and cost-effectiveness of available mitigation options. Ancillary benefits associated with greenhouse gas (GHG) reductions Some GHG emission control options might provide substantial co-benefits by concurrently reducing problems not directly caused by climate change (e.g., air pollution or lack of sustained economic growth, especially in developing countries). However, in both these instances, the same, or greater, level of co-benefits can be obtained more economically by directly attacking the specific (non-climate change related) problems rather than indirectly through greenhouse gas control. On the other hand, a direct assault on the numerous climate-sensitive hurdles to sustainable development (e.g., hunger, malaria, and many natural disasters) would, as indicated, provide greater benefits more cost-effectively than would efforts to mitigate climate change. Collapse"
103606,1990,review of income and wealth,on international trade in banking services,on international trade in banking services,"International financial liberalization may alter saving–investment imbalances and patterns of capital flows across countries. Using a panel of OECD countries for 1990–1996, I examine how the liberalization of capital movements and financial services trade affects net private capital flows. Capital inflows tend to fall (rise) with the liberalization of commercial presence in banking and securities (insurance) services, possibly reflecting an increase (decrease) in saving. I find that capital account liberalization stimulates capital inflows, suggesting that better access to external financing helps sustain larger current account deficits. When cross-border trade is liberalized, capital inflows change insignificantly. Collapse"
103607,1990,review of income and wealth,a note on the revision of the united nations system of national accounts,a note on the revision of the united nations system of national accounts,"This report summarizes the proceedings of a series of meetings called by the Conference on Research in Income and Wealth of the National Bureau of Economic Research in June of 1966. The major conclusions of the conference, as transmitted to the Statistical Office of the United Nations, were as follows: (1) The aim of integrating the various parts of the system of national accounts, including input‐output and financial transactions, is to be welcomed. (2) The more recently developed parts of the system need considerably more work to reach the same level of clarity and usefulness which the national income and product accounts have acquired. (3) Some simplification of the proposed basic system should be considered, involving the identification of a minimum of information that should and could be provided by all countries. (4) In line with the conference's overriding interest in national accounts as an instrument for economic analysis and a means of more informed policy formation, the proposed system needs considerable strengthening in the field of income distribution. Collapse"
103699,1990,scandinavian journal of economics,a further note on union power in the long run,a further note on union power in the long run,"Kemp and Long (1987) showed that, in the long run, non-saving workers may be unable to improve their lot by controlling the effective supply of labour. Their proposition was interesting not least because it offered the germ of an explanation for the recent decline in unionism in some capitalist countries, notably the United States and Japan. However, the Kemp-Long analysis was conducted in terms of a simple model economy of the Solow-Swan type, with just one product, constant returns to scale and a constant savings propensity for capitalists. Since the appearance of their article, the possibility of long-run labour impotence has been reconsidered by several authors and it has emerged that an allembracing labour union might be able to do something for its members, even in the long run, if the production function is convex-concave, cf. Palokangas (1989), or if the union resorts to all-or-nothing offers, cf. Manning and Shea (1988), or if capitalists optimize over time, cf. Kemp and Long (198 9a, b). In all of the papers just mentioned, it is assumed that the wellbeing of workers depends on their consumption of the produced commodity only. However, labour economists typically assume that the wellbeing of workers also depends on the amount of leisure time available to them. The purpose of the present note, then, is to examine the implications of that assumption for the Kemp-Long problem. In all other respects, our specification of the economy is the same as that of Kemp and Long. It will be shown that if the workers' marginal rate of substitution of consumption for leisure is greater than the steady-state wage rate when the Collapse"
103700,1990,scandinavian journal of economics,"bargaining, trust and the role of money",bargaining trust and the role of money,No Result.
103721,1990,social science quarterly,household structure among teenage mothers in the united states,household structure among teenage mothers in the united states,Using indvidual-level data from the 1980 U.S. Census this paper examines household structure among teenage mothers. A typology is developed for the common forms of household arrangements. Only a small percentage of teenage mothers head their own households; the majority live with others. The patterning of household structure is related to age marital status race poverty and school enrollment. The implications of these results for household transitions social policy and future research are discussed. (EXCERPT) Collapse
103722,1990,social science quarterly,the impact of the  bakke decision on black and hispanic enrollment in medical and law schools,the impact of the bakke decision on black and hispanic enrollment in medical and law schools,No Result.
103723,1990,social science quarterly,social services impact on elderly independent living,social services impact on elderly independent living,"An ageing population, government funding cuts and pressures on local authorities to provide services that meet the needs of elderly people, particularly those with complex needs, has resulted in challenges for service providers and service users. This study examined the impact on service users and other stakeholders of rapid response services for elderly and vulnerable residents living in social housing in north-east England. The housing provider has a rapid response team for residents as part of its organisational infrastructure. This includes a 24/7 emergency response service combined with a telecare service, funded by the local authority for vulnerable clients with complex needs and self-funded by others. The study utilised semi-structured qualitative interviews to collect data. Participants included service users, commissioners, service providers, adult social care and family carers. Thematic analysis was used to identify key issues. Service users reported feeling safer and more confident knowing someone would be there at times of need. Family carers reported improvements in their own health and well-being, as they no longer felt on call all the time. The number of requests for ambulances as a consequence of falls was reported to have reduced by the rapid response team. Vulnerable people with electronic monitoring/telecare support were able to remain in their own home for longer, reducing the need for residential funding by the local authority. The partnership between the housing association and the local authority resulted in a service able to respond to changing needs as well as identifying deterioration in residents' health status. In conclusion, the use of a combined rapid response and telecare service resulted in elderly people remaining independent in their homes for longer, which improved their reported quality of life and relieved stress on carers and pressures on other service providers. Collapse"
103724,1990,social science quarterly,the impact of the tax revolt era state fiscal caps,the impact of the tax revolt era state fiscal caps,"Les effets de la taxe sur le capital et de ses avatars aux Etats-Unis, etudies dans trois Etats : Michigan, Ohio, et Caroline du Sud"
103725,1990,social science quarterly,"money and power:  financiers and the electric manufacturing industry, 1878-1896",money and power   financiers and the electric manufacturing industry  1878 1896,No Result.
103726,1990,social science quarterly,"information, attitudes, and elite opinions on the strategic defense initiative",information attitudes and elite opinions on the strategic defense initiative,ERR
103727,1990,social science quarterly,sociopolitical participation and the significance of social context: a model of competing interests and obligations,sociopolitical participation and the significance of social context a model of competing interests and obligations,No Result.
103728,1990,social science quarterly,urban crime control:  violent crimes in new york city,urban crime control violent crimes in new york city,"It was a year packed with unsettling events. The Panic of 1857 closed every bank in New York City, ruined thousands of businesses, and caused widespread unemployment among industrial workers. The Mormons in Utah Territory threatened rebellion when federal troops approached with a non-Mormon governor to replace Brigham Young. The Supreme Court outraged northern Republicans and abolitionists with the Dred Scott decision (""a breathtaking example of judicial activism""). And when a pro-slavery minority in Kansas Territory tried to foist a pro-slavery constitution on a large anti-slavery majority, President Buchanan reneged on a crucial commitment and supported the minority, a disastrous miscalculation which ultimately split the Democratic party in two. In America in 1857, eminent American historian Kenneth Stampp offers a sweeping narrative of this eventful year, covering all the major crises while providing readers with a vivid portrait of America at mid-century. Stampp gives us a fascinating account of the attempt by William Walker and his band of filibusters to conquer Nicaragua and make it a slave state, of crime and corruption, and of street riots by urban gangs such as New York's Dead Rabbits and Bowery Boys and Baltimore's Plug Uglies and Blood Tubs. But the focus continually returns to Kansas. He examines the outrageous political frauds perpetrated by proslavery Kansans, Buchanan's calamitous response and Stephen Douglas's break with the President (a rare event in American politics, a major party leader repudiating the president he helped elect), and the whirl of congressional votes and dramatic debates that led to a settlement humiliating to Buchanan, and devastating to the Democrats. 1857 marked a turning point, at which sectional conflict spun out of control and the country moved rapidly toward the final violent resolution in the Civil War. Stampp's intensely focused look at this pivotal year illuminates the forces at work and the mood of the nation as it plummeted toward disaster. Collapse"
103729,1990,social science quarterly,"specific deterrence, rational choice, and decision heuristics: applications in juvenile justice",specific deterrence rational choice and decision heuristics applications in juvenile justice,"INTRODUCTIONAccording to the traditional rational choice theory of criminal behavior, people choose to commit crimes in a rational manner.1 They weigh the costs and benefits and make informed decisions to maximize their utility.2 Under this framework, the state can deter crime through two main avenues: increasing the probability of detection and increasing the punishment if caught, both of which increase the total cost of committing a crime.3Recently, however, behavioral insights have begun to cast doubt on traditional rationality assumptions.4 Lab experiments and empirical studies using real-world data have shown that people exhibit bounded rationality.5 For example, individuals have limited cognitive capacities and use heuristics-mental shortcuts-to simplify complex decisions.6 A clear understanding about when and how these behavioral biases come into play can open up a new range of policy tools and help inform a more accurate model of criminal decisionmaking and deterrence.One specific behavioral anomaly that is now well recognized in the behavioral economics and psychology literature is ambiguity aversion.7 This phenomenon, which is distinct from risk aversion,8 occurs when an individual must make a cost-benefit analysis using incomplete information regarding the probabilities of future events, such as the likelihood of detection or probability of punishment. Sometimes, rather than relying on an unbiased estimate of the unknown probability, individuals will rely on a pessimistic estimation, overweighting the probability of a negative outcome due to their fear of the unknown. In other words, when there is uncertainty about the probabilities of various outcomes (ambiguity), people may assume that bad outcomes are more likely than they actually are.A few scholars have already suggested using this aversion to increase deterrence in the criminal context.9 They argue that if people are more likely to shy away from decisions that involve uncertain probabilities, making detection and punishment ambiguous-in other words, making the probability of detection and punishment more uncertain-can deter crime.10 However, this application is not as clear-cut as it might seem.11 Rather than finding ambiguity aversion in all circumstances, research has shown that individuals have a switching point at which they may become ambiguity-seeking under the right circumstances.12 Ambiguity-seeking behavior is the exact opposite of ambiguity-averse behavior. When ambiguity-seeking individuals make a cost-benefit analysis using incomplete information regarding the probabilities of future events, they rely on an optimistic estimation rather than an unbiased estimation, overweighting the probability of a positive outcome due to their preference for the unknown. This lack of uniform ambiguity attitudes implies that the imposition of ambiguity to increase deterrence will work only in some circumstances. In other circumstances, ambiguous punishment and detection may actually increase crime. When that is the case, policies that decrease ambiguity can better serve deterrence purposes.This Note is the first to apply these insights about ambiguity attitudes to the unique goals and structure of the juvenile justice system. As is the case in the adult criminal justice system, policies that increase the ambiguity of detection or decrease the ambiguity of punishment are promising means for deterring juvenile crime. Therefore, focusing on the circumstances under which juveniles are likely to be ambiguity-seeking, this Note argues that providing juvenile defendants with the right to counsel at intake proceedings, a practice currently employed only by some states,13 has the potential to reduce the ambiguity of punishment and deter juvenile crime. Part I presents the typical model of deterrence under traditional rationality assumptions and the impact of considering ambiguity attitudes in this context. Part II applies these behavioral insights to the specific context of the juvenile justice system, starting with an examination of the evidence of ambiguity-averse and ambiguity-seeking behavior among juveniles and progressing to an analysis of the ambiguity that exists in the juvenile justice system and how it can be manipulated to deter juvenile crime. … Collapse"
103730,1990,social science quarterly,the importance of specific resources to community actions:  the case of foreign trade zones,the importance of specific resources to community actions the case of foreign trade zones,No Result.
103731,1990,social science quarterly,"the impact of hyper media coverage on suicide:  new york city, 1910-1920",the impact of hyper media coverage on suicide   new york city  1910 1920,No Result.
103732,1990,social science quarterly,"gender-related differences in paraguayan migration to buenos aires, argentina",gender related differences in paraguayan migration to buenos aires  argentina,This study empirically examines gender-related differences in the decision to migrate internationally. A single-equation logit model is specified to distinguish movers from nonmovers where the explanatory variables include demographic and occupationally related measures. The model is tested with survey data collected on 1303 Paraguayan families half of whom reside in Buenos Aires Argentina. One of the important results suggests that occupationally related variables are more important in explaining male migration decisions than those made by females. (EXCERPT) Collapse
103805,1990,urban studies,urban economic development in a period of local initiative: competition among towns in israel's southern coastal plain,urban economic development in a period of local initiative competition among towns in israels southern coastal plain,"A case-study of six towns in Israel's southern coastal plain demonstrates how a shift from nationally directed industrial dispersal efforts to one in which there is a greater role for locally initiated development strategies leads to the restructuring of the urban system. Semi-peripheral regions attain a growing advantage over the remote periphery, though within the semi-periphery intra-regional competition has intensified. Whereas previously there was a degree of homogeneity in the inter-urban patterning of economic investment, the correct identification of advantages at the micro-level has gained importance in promoting local economic development. These advantages can be utilised by effective local leadership for developing 'niches of success' in the face of general economic stagnation. The local strategy achieving the clearest impact on long-term development paths has been to exploit advantages associated with proximity to a metropolis, particularly by intervening in the housing market and by promoting economic integration with the metropolis. Collapse"
103811,1990,urban studies,controlling and assisting privately rented housing,controlling and assisting privately rented housing,"In this article we examine the extent to which three minority groups were able to achieve selected neighborhood social and physical outcomes in the San Francisco mnetropolitan area. Ecological regressions were estimated to generate elasticities that measure the relative abilities of blacks, Hispanics, and Asians to convert education and income into desirable neighborhood environments. These regressions were interpreted in light of substantial differences between the three groups in levels of residential segregation. Results generally indicated a black disadvantage in the process of residential achievement, but it was not as dramatic as that found in earlier studies or as great as the levels of segregation would suggest. As in prior research, education was found to be the critical variable in explaining spatial differentiation and class stratification among blacks. In the United States, residences are allocated to persons and families through private housing markets. Since public housing comprises less than 2% of the nation's housing stock, and less than 1% of its housing starts (Adams 1987), virtually all households seeking a new residence enter the rental or sale market. Each household has a set of housing needs and desires based on its size, composition, life-cycle stage, and tastes; it also has a set of economic resources with which to achieve these desires, principally capital assets and income. Markets allocate households to specific residences through the mechanism of price; households purchase or rent the home that best suits their needs at the price they can afford (Berry & Rees 1969). According to neoclassic economic theory, the price of housing reflects the balance between aggregate demand and supply within local markets (Alonso 1964; Mills 1972). Researchers have pointed out, however, that housing is different from other commodities, and that these differences structure housing markets in distinct ways. First, residences are immobile; they are tied to a particular piece of land and cannot easily be consumed elsewhere (Logan & Molotch 1987). Second, they represent a very significant investment for most families, and for homeowners are the primary means of capital accumulation *Direct correspondence to the authors at the Population Research Center, University of Chicago, 1155 E. 60th Street, Chicago, IL 60637. i) The University of North Carolina Press Social Forces, September 1990, 69(1):15-32 This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 16 / Social Forces 69:1, September 1990 (U.S. Bureau of the Census 1986). Third, each residence is not only tied to a particular plot of land, it is also bound to a specific neighborhood (Massey et al. 1987) and, in turn, to a larger municipality (Logan 1978). Moreover, each geographic unit is associated with a social environment defined by the behaviors of residents and the service benefits provided by local government (Schneider & Logan 1982, 1985; Massey et al. 1987). Finally, houses, and the communities in which they are found, are more than mere commodities; they are also objects of powerful sentiments that influence judgments and condition decisions (Logan & Molotch 1987). In recognition of the fact that housing is immobile and linked to particular places, Charles Tiebout (1956) proposed a ""pure theory of local expenditures"" to account for the differentiated residential structure of cities. The theory rests on two key assumptions that households are free to move, and that places compete to attract them. Local governments offer different packages of tax costs, service benefits, and zoned environments from which home seekers choose. Over time, market competition produces a variety of service-tax-environment mixes and distributes households among them according to income, preferences, and wealth, yielding a residential structure differentiated by socioeconomic status, family life-cycle stage, race, and ethnicity. Although Tiebout's model recognizes some unique features of housing markets, it does not incorporate the fact that housing costs, and particularly home-ownership costs, reflect a substantial investment for families, or that homes, neighborhoods, and communities are the focus of strong emotional attachments. These traits give rise to what Stinchcombe (1965) calls ""communities of fate,"" where residents and institutions have large stakes not only in their own property, but in the property and characteristics of people in surrounding areas, making collective action highly likely. This propensity for collective action segments housing markets along social lines, so that individual choices are constrained by institutional practices and the collective behavior of others (Logan & Molotch 1987). The segmentation of housing markets, and the constraints it imposes on individual home seekers, are well illustrated by the case of U.S. blacks. The Tiebout model and its successors (Bish 1971; Peterson 1981) assume that households are free to move wherever their tastes and economic resources take them. Given this assumption, racial segregation is interpreted simply as the coincidental by-product of sorting based on income, wealth, and tastes (viz. Clark 1986). Considerable evidence, however, indicates that segregation does not stem from black preferences or low black-income levels. Public opinion polls show that blacks strongly endorse the principle of residential integration (Schuman et al. 1985) and express a clear preference for living in integrated neighborhoods, other things being equal (Farley et al. 1978). In spite of these tastes for integration, however, desegregation does not follow from the simple acquisition of socioeconomic resources sufficient to support spatial mobility. As education, income, and occupation rise, black-white segregation does not decline, but persists at a very high level (MIassey 1979, 1981; Denton & Massey 1988). Rather than reflecting tastes or socioeconomic status, black segregation appears to stem from constraints to black residential mobility imposed by the This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms Segregation and Neighborhood Quality / 17 collective behavior and institutional actions of whites. Studies of the real estate industry, for example, indicate that discrimination and prejudice are still widespread. Real estate agents systematically steer black home seekers away from white neighborhoods, and provide them less favorable treatment than whites (Molotch 1972; Pearce 1976; Wienk et al. 1979; Yinger 1986), actions which have been linked analytically to high levels of residential segregation (Galster 1986). At the same time, lending institutions have been found to finance a disproportionately small number of loans in black neighborhoods, even after objective social and economic factors are controlled (Taggart & Smith 1981; Pol et al. 1982; Leahy 1985). Moreover, in the few cases where blacks succeed in entering a white neighborhood, they are frequently met by organized white resistance and hostility (Hirsch 1983; Cass 1986; Bauman 1987), especially in working-class areas (Logan & Stearns 1981; Stearns & Logan 1986a); and if blacks succeed in establishing themselves, the neighborhood is quite likely to be avoided by subsequent white home seekers, resulting in eventual resegregation (Massey & Mullan 1984). Thus, the collective action of white residents and the institutional practices in the real estate and banking industries bifurcate urban housing markets along racial lines, fostering high levels of segregation despite a strong demand by blacks for integration. Some studies suggest that Hispanics face similar barriers, but to a lesser degree (Hakken 1979; James & Tynan 1986). The effect of a racially segmented housing market in creating and sustaining segregation is crucial to understanding the socioeconomic position of blacks in the United States. Barriers to spatial mobility are, in a very real way, barriers to social mobility. As Logan and Molotch (1987:49) point out, socioeconomic inequality among households and geographic inequality among places are not independent; the two systems of hierarchy reinforce one another: ""High status within the social hierarchy can bring access to the most desirable places ... and a guarantee of a rewarding future for whatever place one controls. At the same time a high status for one's geographical place means the availability of resources . . . that enhance life chances generally."" In a similar vein, Giddens (1980:107-12) argues that segregation is a core mechanism of class structuration, since it concentrates people of low status in space and ensures the maintenance of behaviors and orientations detrimental to success in the larger society. In this article we demonstrate how segregation structures the neighborhood environment achieved by three minority groups in one large urban area. We estimate the degree to which blacks, Hispanics, and Asians are able to convert socioeconomic achievements into selected neighborhood outcomes within the San Francisco-Oakland Standard Metropolitan Statistical Area (SMSA) in 1980. The analysis proceeds in three phases. First, we establish the degree of segregation experienced by blacks, Hispanics, and Asians in the San Francisco SMSA. We then consider the extent to which the degree of segregation faced by each group affects its ability to convert status attainments into neighborhood social outcomes. Finally, we evaluate the degree to which each group is able to convert status attainments into different physical characteristics of the neighborhood environment. This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 18 / Social Forces 69:1, September 1990 Collapse"
103812,1990,urban studies,income-related assistance with housing costs:  a cross-national comparison,income related assistance with housing costs   a cross national comparison,"Importance Understanding if the association of social programs with health care access and utilization, especially among older adults with costly chronic medical conditions, can help in improving strategies for self-management of disease. Objective To examine whether participation in the Supplemental Nutrition Assistance Program (SNAP) is associated with a reduced likelihood of low-income older adults with diabetes (aged ≥65 years) needing to forgo medications because of cost. Design, Setting, and Participants This repeated cross-sectional, population-based study included 1302 seniors who participated in the National Health Interview Survey from 2013 through 2016. Individuals in the study were diagnosed with diabetes or borderline diabetes, were eligible to receive SNAP benefits, were prescribed medications, and incurred more than zero US dollars in out-of-pocket medical expenses in the past year. The data analysis was performed from October 2017 to April 2018. Exposures Self-reported participation in SNAP. Main Outcomes and Measures Cost-related medication nonadherence derived from responses to whether in the past year, older adults with diabetes delayed refilling a prescription, took less medication, and skipped medication doses because of cost. To estimate the association between participation in SNAP and cost-related medication nonadherence, we used 2-stage, regression-adjusted propensity score matching, conditional on sociodemographic and health and health care–related characteristics of individuals. Estimated propensity scores were used to create matched groups of participants in SNAP and eligible nonparticipants. After matching, a fully adjusted weighted model that included all covariates plus food security status was used to estimate the association between SNAP and cost-related medication nonadherence in the matched sample. Results The final analytic sample before matching included 1385 older adults (448 [32.3%] men, 769 [55.5%] non-Hispanic white, and 628 [45.3%] aged ≥75 years), with 503 of them participating in SNAP (36.3%) and 178 reporting cost-related medication nonadherence (12.9%) in the past year. After matching, 1302 older adults were retained (434 [33.3%] men, 716 [55.0%] non-Hispanic white, and 581 [44.6%] aged ≥75 years); treatment and comparison groups were similar for all characteristics. Participants in SNAP had a moderate decrease in cost-related medication nonadherence compared with eligible nonparticipants (5.3 percentage point reduction; 95% CI, 0.5-10.0 percentage point reduction; P = .03). Similar reductions were observed for subgroups that had prescription drug coverage (5.8 percentage point reduction; 95% CI, 0.6-11.0) and less than $500 in out-of-pocket medical costs in the previous year (6.4 percentage point reduction; 95% CI, 0.8-11.9), but not for older adults lacking prescription coverage or those with higher medical costs. Results remained robust to several sensitivity analyses. Conclusions and Relevance The findings suggest that participation in SNAP may help improve adherence to treatment regimens among older adults with diabetes. Connecting these individuals with SNAP may be a feasible strategy for improving health outcomes. Collapse"
103813,1990,urban studies,the tax treatment of housing:  economic issues and reform measures,the tax treatment of housing economic issues and reform measures,"Housing taxation expenditures are viewed by many as an important source of housing subsidies, although they have frequently emerged as taxation systems have evolved, rather than being designed specifically to aid housing consumption or production . The increasing severity of constraints on government budget deficits has elevated the issue of housing tax expenditures beyond that of mere academic curiosity . Some governments have sought to curb the growth of housing tax expenditures by reforming the tax treatment of housing . Is there a case for reform measures? Are there coherent and operational reform measures worthy of serious consideration? These questions raise a host of issues concerned with macro-economic performance, allocative efficiency and equity : macro-economic performance, because housing tax expenditures can erode the tax base, and thereby affect governments' budget deficits; allocative efficiency, because housing taxation expenditures will influence relative rates of return on housing assets and lead to changes in the economic behaviour of households and firms ; and equity, because housing taxation expenditures alter the relative tax burdens among individual taxpayers, and between owneroccupiers and tenants . In this paper each of these issues is addressed and the prospects for reform assessed in an international context . In doing so, I draw on the papers presented at the Joseph Rowntree International Conference on Housing Finance held in York in June 1989 . These provide relevant material on the existing array of housing tax expenditures employed by a representative sample of Organisation for Economic Cooperation and Development (OECD) countries. Firstly, I discuss the relevance of the tax expenditure concept and briefly describe the housing taxation practices of these countries . Next, I explain why housing tax expenditures have attracted critical attention in recent years . This is followed by an examination of the attributes of tax expenditures as a means of supporting housing consumption and production, and the tenure-neutral reform proposals that are increasingly advocated by commentators on housing policy . Lastly, the recent reform measures introduced by OECD governments are explained and evaluated, and are followed by some concluding remarks . Collapse"
103814,1990,urban studies,housing finance and subsidies in the united states,housing finance and subsidies in the united states,No Result.
103815,1990,urban studies,housing finance and subsidy systems in australia,housing finance and subsidy systems in australia,No Result.
103816,1990,urban studies,housing finance and housing subsidies in canada,housing finance and housing subsidies in canada,"ing from the policy differences outlined earlier, four aspects of the organization and outputs of the U.S. welfare state stand out as unusual: the reliance on private provision; the degree of decentralized discretion; the persistent logic of deservingness; and the bias toward the elderly. Private provision. The American welfare state relies to an exceptional extent on the market, rather than the state, to provide social goods. In many countries, both public and private entities provide health care, occupational pensions, child care, housing, and higher education. But only in the United States is the private sector the predominant provider of so many of them. Such an extensive system of private social provision has not emerged organically; it has been engendered by an ideology of the superiority of markets solutions, and by an extensive (and expensive) set of direct and indirect public subsidies to promote the provision and consumption of private benefits and services (Stevens 1988; Howard 1997; Hacker 2002). Decentralized discretion. Even in nominally unitary states, subnational units may have important responsibilities for raising revenue, planning service delivery, and carrying out central mandates. Across welfare states, there is wide variety in the responsibilities delegated to subnational units, in the extent of redistribution of tax revenues across these units, and the extent of subnational variation in welfare outcomes. However, in most countries, standards for key welfare state attributes like population coverage, benefit packages, and eligibility criteria are agreed upon and enforceable by the national government. U.S. states and localities, on the other hand, have significant discretion in establishing the content of even those welfare policies that the federal government mandates and finances. Furthermore, the fact that so much of the U.S. federal government’s welfare activity comes in the form of low-visibility tax benefits and subsidies for private activity means that stateand local-level policies often appear to be where most of the welfare (p. 120) A Cross-National Perspective on the American Welfare State Page 10 of 22 PRINTED FROM OXFORD HANDBOOKS ONLINE (www.oxfordhandbooks.com). (c) Oxford University Press, 2015. All Rights Reserved. Under the terms of the licence agreement, an individual user may print out a PDF of a single chapter of a title in Oxford Handbooks Online for personal use (for details see Privacy Policy). Subscriber: University of Pennsylvania; date: 09 January 2018 state “action” is in the United States The sheer size and diversity of the United States in cultural and economic terms also means that sub-national-level control and financing of welfare state institutions can lead to substantial geographic differentiation in programs and outcomes. Logic of deservingness. If welfare state decentralization varies in degrees across the advanced democracies, the persistent and often explicit motivation of U.S. welfare policy by a logic of deservingness is really a difference in kind. Much of the expansion of the welfare state in Europe and elsewhere in the post-WWII period was justified politically by a rhetoric of social inclusion and solidarity that resonated nearly as strongly with the Christian social tradition as with the social democratic one (Kersbergen 1995; Berman 2006). Christian democratic and social democratic actors may have privileged different aspects of social inclusion, but welfare policies across Western Europe, Canada, and the Antipodes reflected the principle of inclusion with elements of universal entitlement based on need and adequacy. America, on the other hand, has maintained a welfare state logic that instead prioritizes personal responsibility, help for the deserving only, and the principle of “less eligibility.” This feature is particularly noticeable in the field of social assistance, as “welfare” beneficiaries are particularly strongly stigmatized (see e.g. Soss 2000; Schneider and Ingram 1993). Even in other program areas—for example, Social Security, Medicaid, primary and secondary education, public housing, programs for the long-term unemployed—the U.S. welfare state has maintained eligibility criteria and benefit levels that are quite explicit in their intent of excluding the undeserving (the idle and shiftless, noncitizens, those without lengthy contribution records, those who live in poor areas, drug users, convicted felons, etc.) from social solidarity, rather than reintegrating or rehabilitating them. Elderly orientation. Click to view larger Fig. 7.3 Ratio of social spending on elderly/nonelderly populations. (p. 121) A Cross-National Perspective on the American Welfare State Page 11 of 22 PRINTED FROM OXFORD HANDBOOKS ONLINE (www.oxfordhandbooks.com). (c) Oxford University Press, 2015. All Rights Reserved. Under the terms of the licence agreement, an individual user may print out a PDF of a single chapter of a title in Oxford Handbooks Online for personal use (for details see Privacy Policy). Subscriber: University of Pennsylvania; date: 09 January 2018 The population deemed by many to be most deserving of access to social support in the United States is consistently the elderly (Cook 1992). In addition to this cultural support, seniors have a powerful and effective lobby group, the AARP, and an activated electorate (Campbell 2003). The combination of the elderly’s social and electoral desirability has merged with the particularistic political strategies of American politicians to make the United States one of the world’s most elderly oriented welfare states (Lynch 2006, Ozawa and Lee 2013). Older Americans’ access to social benefits has been consistently defended and expanded, whereas supports for workingaged adults and children are less emphasized, and less well funded. Aggregate spending data and analysis of individual income from social transfers show a distinct elderly orientation in the United States (Figure 7.3). Lynch (2006) further found a distinct skew toward the elderly in the United States in both tax expenditures for social purposes, and in health care spending. Pontusson reports that only “[t]‌ransfer spending that is not targeted on the elderly has a strong positive effect on redistribution among working-age households [...]” (Pontusson 2005, 158). As a result of this elderly orientation of social spending in the United States, poverty, child poverty, and income inequality among the nonelderly population in the United States are all well above the OECD average (see chapters on poverty and inequality in this volume). Unfortunately, the same processes that drive elderly oriented social policy also tend to produce stratification of pension benefits, with the paradoxical result that elderly oriented welfare states like the United States also tend to have higherthan-average poverty among the elderly (Lynch 2006, 182). 5 Comparative Welfare State Development Why does the U.S. welfare state not more closely resemble those of other rich countries? In ordinary public conversation, theories abound, including our Protestant heritage, our large and heterogeneous nation, the legacy of slavery, and the absence of a socialist party. Although each of these explanations contains a kernel of truth, none fully accounts for cross-national similarities and cross-national variation in welfare state institutions and outcomes. Comparative welfare state research has centered on the trinity of interests, ideas, and institutions to explain this variation. Source: Ozawa and Lee (2012, Table 2) A Cross-National Perspective on the American Welfare State Page 12 of 22 PRINTED FROM OXFORD HANDBOOKS ONLINE (www.oxfordhandbooks.com). (c) Oxford University Press, 2015. All Rights Reserved. Under the terms of the licence agreement, an individual user may print out a PDF of a single chapter of a title in Oxford Handbooks Online for personal use (for details see Privacy Policy). Subscriber: University of Pennsylvania; date: 09 January 2018 Interests. Who was for and against the development of social policies in the United States? How do these coalitions compare to those in other countries? The configuration of interest groups in the United States did not augur well for the development of comprehensive, solidaristic welfare states. The strong sectoral split between the industrializing North and agricultural South, whose plantation economy relied on laborrepressive agriculture until well after emancipation, did nothing to encourage solidaristic social insurance programs, which more typically developed when smallholding landowners formed coalitions with nascent urban working-class movements (Baldwin 1990; Esping-Andersen 1990). America did have a politically assertive smallholding class, centered in the country’s Midwest and West, but despite their early joint success in establishing an income tax (Morgan and Prasad 2009), working-class partners for agrarian mobilization were notably weak. At the dawn of the American welfare state, unions were fragmented and lacking in the political “power resources” that have been hypothesized to drive welfare state expansion (Korpi 1983; Esping-Andersen 1985). Similarly, and in contrast to the strong social democratic parties in many parts of Europe, the United States has from the beginning lacked a labor-based party. As a result, “redgreen” alliances of the type that drove welfare state expansion elsewhere were impossible in the United States. Although there is much evidence for the argument that weak power resources on the left determined the relatively small size and private nature of the U.S. welfare state, it is not the whole story. Abundant comparative research has shown labor’s power alone has not built welfare states—employers have played an important role as well (Mares 2006; Swank and Martin 2001; Swenson 1989). The United States’ weak and fragmented organizations of employers, too, discouraged the formation of neocorporatis Collapse"
103817,1990,urban studies,housing finance and subsidies in britain after a decade of 'thatcherism.',housing finance and subsidies in britain after a decade of thatcherism,ERR
103818,1990,urban studies,"the housing market, housing finance and housing policy in west germany:  prospects for the 1990s",the housing market housing finance and housing policy in west germany prospects for the 1990s,ERR
103819,1990,urban studies,trends in housing markets and finance and subsidy systems in the 1980s:  the case of greece,trends in housing markets and finance and subsidy systems in the 1980s the case of greece,ERR
103820,1990,urban studies,housing finance in finland,housing finance in finland,"In Finland the private sector borrowing started to rise rapidly in conjunction with the liberalization of capital movements and deregulation of the domestic financial sector during the second half of the 1980s. The financial deregulation coincided with and amplified an economic boom marked by favourable income expectations, loose fiscal policy associated with improved terms of trade and anticipated reduction in income tax rates. All these factors contributed to the overheating of the Finnish economy that finally turned into a severe recession in the beginning of 1990s. The reaction of households to financial deregulation in Finland was similar to that in the other Nordic countries. As in Norway and Sweden, household indebtedness started to rise in the mid-1980s, after the abolishment of lending rate regulations and prior savings requirements for housing loans. Measured by the ratio of household debt to annual disposable income, household indebtedness peaked in 1990 at more than 80 per cent of annual disposable income. Since then, it has fallen slightly. Debt financing in the corporate sector started to increase rapidly in conjunction with the liberalization of capital movements, which enabled firms also in the domestic sector to raise loans in foreign currencies. During the 1980s debt financing grew most in the real estate business, construction and services. Despite increased borrowing, the debt with respect to equity of Finnish firms did not rise significantly until 1990–91, because a large part of the debt growth was matched by increases in corporate earnings and equity values during the late 1980s. Recession turned the situation for the worse as corporate earnings and the market value of assets plummeted. High indebtedness and overcapacity especially in the domestic sector will require several years of adjusting. Collapse"
103821,1990,urban studies,urban housing and financial markets:  some international comparisons,urban housing and financial markets some international comparisons,"The housing sector is the last major sector of the national economy for which analytical foundations for economic and financial policy evaluation have developed. However, the application of this analytical framework to international comparisons of housing policies or the evaluation of housing finance systems still lags considerably behind the strong foundations laid over the years for trade, finance or industry policy analyses. None the less, the era when improvisation and fragmented policy-making in housing and housing finance could be excused is well past. Today, there is a deep and expanding body of basic research and policy analysis that can be shared for the design of housing policies and housing finance institutions whether it is in the European Union or developing countries. Obviously, social and economic contexts as well as wealth levels differ greatly. Common analytical foundations can lead to very different prescriptions in countries where macroeconomic and financial policies, financial infrastructure, and urban laws policies and practices differ. In some countries, the institutions of the markets are so deep that we may no longer be fully aware of them. In others, they barely exist. Everywhere, the division of labour between public and private actions in both housing policy and housing finance is shifting. Like European countries, but at much lower levels of income and wealth, developing countries continue to seek growth with equity. However, those are countries where the majority of people are poor, cities are growing rapidly, public as well as private institutions are often weak and fiscal resources are severely constrained. In such environments, it seems to stand to reason that the financing of social housing should only be a subset of public concerns for the development of a safe and sound housing finance system that will be able to expand in step with urbanisation. The question addressed here is where the financing of social housing fits within the new world of housing finance. The search for alternative forms of housing finance for low-income groups cannot disregard overall trends affecting the financial sector and hope to be sustainable. It has become clear during the past two decades that there is no such thing as a homogeneous 'Third World' across which identical policies and instruments could be conveniently applied. This simplistic concept resulted from the Cold War, ignorance of distant foreign places and, in particular, of their specific human capital, laws, institutions and practices. The reality in housing finance, as well as in other areas of public policy, is that there are profound differences among the more than 180 advanced and developing countries that are now members of the World Bank. It is therefore important to think of housing finance as 'path-dependent' in the sense that any innovation will be shaped by past and current conditions. One can distinguish six broad types of housing finance systems. Their main features are briefly outlined here. In spite of such international diversity, the separation of subsidies from finance and the redesign of social housing finance programmes are everywhere a critical reference for the design and development of better housing finance systems. Collapse"
103873,1990,world bank economic review,"industrial sickness, primary and secondary: the effects of exit constraints on industrial performance",industrial sickness primary and secondary the effects of exit constraints on industrial performance,"In an attempt to protect workers from unemployment, governments in some countries subsidize firms which would otherwise fail. This article examines the effects of such subsidies on output, prices, and welfare. It also briefly reviews the effects for varying price elasticities of supply and demand, for nontraded and fully traded goods, and for firms in which wages exceed their free market level. If the firm produces a nontraded good, the ""sickness"" of one firm that is subsidized to maintain production can spread to other firms that would be viable in the absence of the subsidies. The analysis shows that the exit constraints have the most detrimental effect when the elasticity of demand is low and supply elasticity is high, and when the government weighs the welfare of the firm's workers and consumers less than that of the general taxpayers and the firm owners. When labor costs are higher than their free market levels and output is thus less than the optimal levels, the subsidies could have an effect similar to that of an optimal production subsidy. In practice, bowever, this possibility is likely to be outweighed by associated rent-seeking costs and other distortions. Collapse"
103874,1990,world bank economic review,toward equitable and sustainable rural water supplies:  a contingent valuation study in brazil,toward equitable and sustainable rural water supplies a contingent valuation study in brazil,"Because many rural people are poor, it is usually assumed that rural water supplies must be financed by government agencies. It is now widely recognized, however, that many rural people can and will pay for improved water supplies, and that sustaining and extending services depends on mobilizing this willingness to pay. This article describes a study of willingness to pay for water in Brazil. The study shows that surveys of actual and hypothetical water-use practices can provide policy-relevant information on willingness to pay, which is shown to vary according to household socioeconomic characteristics and the characteristics of the existing and new supplies of water. In rural Brazil, tariffs for yard taps can be increased substantially before significant numbers of housebolds would choose not to connect to an improved system, whereas provision of free water at public taps can protect the poor without jeopardizing the financial viability of the scheme. Billions of people in developing countries face daily problems in obtaining water for drinking, cooking, bathing, and washing. More than 1,500 million people-30 percent of the world's population-are estimated to be without access to uncontaminated water; and an unknown but large proportion have to spend hours daily to collect water (Briscoe and de Ferranti 1988, Churchill 1987). Because the adverse consequences for productivity, health, and quality Collapse"
103878,1990,world bank economic review,import dependency and structural adjustment in sub-saharan africa,import dependency and structural adjustment in sub saharan africa,"One of the effects of structural adjustment programs in sub - Saharan Africa has been the reduction of imports in the face of scarce foreign exchange. This article takes the analysis of import demand beyond the traditional income and price determinants to account for factors likely to be important to sub - Saharan African countries in the 1990s. First, the effect of demand on imports is reflected by the level of absorption rather than the less direct income variable. Second, because adjustment programs may cut government consumption and, through increases in interest rates, reduce investment, these components of absorption are also considered independently to assess their differential effect on imports. Third, import barriers are often set in dollar terms to limit the use of foreign exchange. Because reliable and complete data for import restrictions are not available, the ratio of exports to debt is included as an indicator of foreign exchange availability to reflect its effect on trade barriers and thus imports. The findings suggest that this more comprehensive assessment of import demand will be needed if the size and even direction of changes in import demand in response to policy reform is to be understood and anticipated. Collapse"
103883,1990,world bank economic review,interactions between institutional and informal credit agencies in rural india,interactions between institutional and informal credit agencies in rural india,"In an attempt to expand rural credit and displace the village moneylender, India created a system of rural cooperatives in the 1950s and expanded branch banking into rural areas in the 1970s. This article examines how these measures affected the rural market. It begins with the question of how large the expansion of institutional credit has been and the extent to which it has dislodged the village and nonresident moneylenders. A detailed comparison of three major surveys of the Indian rural credit market suggests that in various guises, the moneylender is still a major source of loans. The article also examines the (weak) evidence on intermediation between the formal and informal sectors. A formal model of the interaction between the informal moneylender and institutional lender is constructed under a variety of assumptions about the exclusivity of loan contracts and the competitive structure of the informal sector. The conclusions are drawn together in the form of five proposals for public policy. Collapse"
103888,1990,world development,growth of the capital goods sector in india after the mid-1960s: an alternative view,growth of the capital goods sector in india after the mid 1960s  an alternative view,"Abstract This paper attempts to trace the growth of the capital goods sector in India between 1965 and 1985. It debates the prevalent view that the industrial sector experienced decelerated growth during this period. Our search for a growth pattern in the capital goods industries does not confirm the deceleration hypothesis. To the contrary, our results suggest impressive rates of growth for the capital goods sector. Collapse"
103890,1990,world development,how economic institutions affect economic performance in industrialized countries:  lessons for development,how economic institutions affect economic performance in industrialized countries lessons for development,"The study of comparative public policy reveals the intensely political nature of policy choices. While policy analysts often look to policy successes and failures outside their borders to draw valuable lessons and insights, cultural, economic, political, and institutional conditions vary from country to country and strongly affect how policy analysis is ultimately used. By combining a conceptual discussion of policy making with an examination of seven specific policy areas, Jessica Adolino and Charles Blake show how politics-in the realm of the environment, education, taxation, economics, immigration, health care, and social welfare-shapes policy choices. The second edition of Comparing Public Policies has been revised and updated to reflect the most recent political and policy developments. This new edition expands coverage of the internationalization of domestic policy making by including a European Union case study in each issue area, along with further discussion of the role of international interest groups in the policy process. The seven policy chapters have been revised and updated to examine current issues in the United States, Japan, Germany, France, the United Kingdom, Italy, and the European Union, such as: - the heightened calls for immigration policy reform - the return to higher budget deficits in several countries - the efforts to lower tax rates in countries with falling expenditures and in countries with rising spending levels - the often unsuccessful attempts to control increasing health care costs in countries with aging populations - the spirited debate over the future role of the welfare state in an increasingly globalized economy - the, at times, divergent education reform debates regarding the role of assessment and calls for decentralization - the uneven environmental performance in the reduction of carbon dioxide emissions Chapters include analyses of crossnational trends-past and present-and a final chapter reexamines the internationalization of public policy in industrialized countries. Useful pedagogical features have been incorporated throughout the text, including ""In Depth"" boxes that offer detailed discussion of the political process or analytical techniques, and ""Country At-a-Glance"" boxes that provide quick reference to political institutions. A wealth of recent data is displayed in numerous tables and a glossary gives students a practical guide to terminology. Collapse"
103894,1990,world development,"how beautiful is small?  scale, control and success in kenyan irrigation",how beautiful is small scale control and success in kenyan irrigation,"Abstract This paper examines the question of scale as a factor in the success of irrigation projects. Experience with government-managed irrigation development in Kenya is discussed, focusing in particular on the Bura Irrigation Settlement Project and on small-scale irrigation in Turkana. It is argued that the performance of both large and small schemes has been poor, and it is suggested that the scale of scheme operation is of less importance in determining the success of development than is the extent to which government irrigation is bureaucratically controlled. Most schemes have been neither initiated nor controlled by farmers. It is bureaucratic management, rather than scale itself, which seems to be the key cause of poor performance in Kenyan irrigation. Therefore, small is by no means always beautiful in the context of irrigation, and enthusiasm for small-scale irrigation as a target of development funding needs to be tempered with caution. The most important feature of small-scale irrigation is its informal nature. The implications of the poor performance of bureaucratically-controlled irrigation are discussed in the context of widespread and growing interest in small-scale approaches to African irrigation. Collapse"
103909,1990,world development,canadian technology transfer to developing countries through small and medium-size enterprises,canadian technology transfer to developing countries through small and medium size enterprises,"Abstract This paper is based on a two-sample study of Canadian small and medium-size manufacturing enterprises (SMEs) that transfer their technology abroad. One sample covers those SMEs transferring technology to the largest or more active Third World countries; the other concerns Canadian SMEs transferring technology to industrialized market economies. Both samples are representative of their respective populations. The paper tests the usefulness of current theories of multinational firms (based on the study of large corporations), and reviews the appropriate technology debate. It also characterizes the manufacturing SMEs that are currently selling technology in the global market. It confirms much of the received theory of multinational corporations. The SMEs studied are very active in research and development, combine technology transfer and foreign direct investment whenever possible, and occupy large portions of the domestic (and often international) market. Their technology (centered on niches) is only complementary to that of large MNCs, not an alternative. This finding is contrary to some aspects of appropriate technology theories. Collapse"
103913,1990,world development,software industry: an opportunity for latin america?,software industry an opportunity for latin america,"The work of the Ibero-American Science & Technology Education Consortium (ISTEC) in developing information technology and digital library manpower will be discussed, focusing on the organizational arrangements and processes that proved most effective in bringing researchers, engineers, computer scientists and librarians together to act on this vision. The Library Linkages (LibLink) Initiative of ISTEC will be analyzed in detail, especially the best practices to ensure regional cooperation in information delivery and in sharing expertise in digital initiatives. Brief descriptions of other IT projects and partnerships in the region will be provided. Overview of the Ibero-American Science & Technology Education Consortium (ISTEC) ISTEC is a non-profit organization comprised of educational, research, and industrial institutions throughout the Americas and the Iberian Peninsula. The Consortium was established in September 1990, to foster scientific, engineering, and technology education, joint international research and development efforts among its members, and to provide a cost-effective vehicle for the application of technology. The idea evolved out of a needs-analysis study conducted by the University of New Mexico Electrical and Computer Engineering Department in Latin America. This study revealed the following obstacles to science and technology information (S&T) sharing and information technology (IT) developments in the region: • Lack of current information for planning and developing technology • Lack of expertise in the use of information • Lack of international cooperation in developing the critical mass needed for projects and joint efforts • Lack of interaction (lack of confidence and sometimes lack of information) between universities and industries This situation has been improving steadily but at the time it was clear that a unifying organization was needed to bring S&T workers together across borders; national, social and economic. With start-up funding from the State of New Mexico and selected IT companies, especially Nortel and Motorola, the ISTEC board created four initiatives to address the above obstacles: 1. The ACE Initiative champions continuing engineering and computer sciences education projects. The most important goals are to upgrade human resources and curriculum development through on-site training, distance learning, and non-traditional exchange programs. This involves not only on-site training, but web-based education, video courses, satellite delivery, and ""sandwich"" graduate programs. The latter brings graduate students from Ibero-America together with experts from ISTEC member organizations to ensure excellence. Examples of outcomes so far include: 6 satellite courses to 250 institutions with ATEI (an Iberian continuing education television society), numerous short courses for Motorola, and over 200 scientists trained in DIP with support from the Organization of American States (OAS). Over 30,000 ftp grabs of the web-based DIP course have been documented. ""Sandwich"" programs have been conducted with PhD students in Spain, Brazil, Argentina, Colombia, Uruguay, Ecuador, Costa Rica and Mexico. These programs bring doctoral students from Latin America to the University of New Mexico to obtain additional experience in areas not offered at their own organizations, or alternately, professors from selected US universities spend a month on site at a Latin American university and works intensively with a cluster of students from the region. 2. The Research and Development (R&D) Initiative focusses on the development and enhancement of laboratory infrastructure at member organizations. The major goal is the design and installation of modular, flexible, and expandable laboratory facilities for education, training, and R&D with links to the productive/private sector. Successful implementations include the deployment of Motorola microprocessors (680XX), microcontrollers (68HC11) and DSPs (56XXX, 96000) as well as equipment, software and expertise from companies such as Nortel Networks, Fluke, and VeriBest. To date 29 Motorola facilities are in place with planned expansion to 58. Approximately 20,000 users have been trained since 1991. There are 9 facilities with laboratory equipment from Nortel Networks, 2 with Fluke and 1 with VeriBest. The latter are planning to expand to 12 facilities. 3. The Los Libertadores Initiative champions networks of excellence in the region. The main goal is to network such Centers of Excellence equipped with the latest telecommunications and computer technology to provide real-time access to a worldwide system of expertise and knowledge. This requires partnerships among industries and governments to create a robust Ibero-American academic and R&D Internet backbone. Towards this goal technical assistance in telecommunications and S&T legislation has been provided to Ecuador and Bolivia. Participation in regional policy conferences such as the IADB’s Informatics 2000 Conference is a part of ISTEC’s strategy. We also assist national, regional and international organizations such as the OAS and UNESCO to develop IT&T strategies for Ibero-America, but in particular for Latin America. 4. The Library Linkages Initiative (LibLINK) is ISTEC’s information sharing, knowledge management, and connectivity project. The next section will focus on this initiative and its efforts in developing digital library projects in Latin America. Suffice to say that the digital libraries component of LibLINK is currently the centerpiece of all the initiatives as it brings people and projects together through sharing information freely and rapidly. Overview of the Library Linkages (LibLINK) project of ISTEC The major goal of LibLINK is to design and implement innovative, international Science and Technology (S&T) information sharing and management services. Thus the Internet and connectivity is of primary importance. The seed funding for LibLINK was provided by Nortel Networks and it is currently supported by membership dues. The annual compound growth rate of the Rapid Document Delivery (RDD) project was around 200% between 1995 and 1999. Since 2000 this is evening out due to successful regional sharing of local resources. Over 27 libraries in 19 countries are connected in real-time and documents are provided using the Ariel® software from the Research Libraries Group. The Centennial Science & Engineering Library (CSEL) at the University of New Mexico, USA, is the headquarters for this initiative and provides document delivery resources free from local collections. Expanded services are also provided at cost from the Canada Institute for Science and Technology Information (CISTI). The RDD project, although the most popular service is a foundation for the more important digital library initiatives which were started in 1998. The projects within LibLINK can be categorized as follows: • Connecting libraries for Information Transfer. This is accomplished through opening Science and Technology library collections especially Latin American collections for scholars through regional networks created to compliment the LibLINK document delivery services. Currently these include LigDoc in Brazil, PrEBi in Argentina, REBIDIMEX in Mexico, and most recently, cooperative groups of libraries in Colombia and Venezuela. • Training librarians and researchers in the efficient and cost-effective search and retrieval of information, document delivery software and processes, and digital library concepts. LibLINK volunteers plan and carry out workshops and miniconferences to facilitate the above. Funding generally come from successful grants from organizations such as the US National Science Foundation (NSF) and other national science councils such as CONACyT in Mexico, and regional organizations such as the Organization of American States (OAS) and UNESCO. • Continually expanding services to more S&T libraries, especially in Latin American countries. The intention is to also expand to other library types (especially Health Sciences) and services. • Developing software for information sharing. One of our member organizations, the University of Vigo in Spain, is developing a document sharing and collaborative workspace technology, called RANDEX. All such developments are tested by members and provided free to members once proven useful. • Developing “push and search” engines for information delivery in conjunction with the ISTEC Portal. • Working with the Networked Dissertation/Thesis Library (NDTL) initiative at Virginia Tech to expand the concept in Ibero-America. • Providing the main interaction method for the ACE and R&D initiatives and participation in the development of a database on S&T people, projects, policies, interests, publications, and opportunities in Latin America. • Advancing and piloting new types of scholarly communication. An electronic journal in computer engineering was established at the Universidad National de la Plata (Argentina) to develop experience in this area. We are actively supporting new publishing efforts such as the NDTL mentioned above and the Open Archives initiative. • Writing grants to further our goals. Grants have been written to IDB, UNESCO, World Bank, NSF and various science councils in the region, OAS, UNESCO, and to other national organizations and industrial partners. To accomplish the above, the LibLink program has developed four sub-initiatives. These are listed below with their current major goals. Initiative 1: Rapid Electronic Document Delivery and general services Connect more libraries and collections in the region Expand services to related multidisciplinary subject areas, e.g. Health care Continued creation, improvement, and/or expansion of regional LibLink Offices (LigDoc, PrEBi, Red de Colombia, REBIDIMEX, etc.) Initiative 2: Consortial Electronic Content Negotiate with vendors for E-journal packages Regional database contracts for citation as well as full-t Collapse"
103917,1990,world development,the secondary market and the international debt problem,the secondary market and the international debt problem,No Result.
103924,1990,yale law journal,on property:  an essay,on property an essay,No Result.
103925,1990,yale law journal,firrea:  controlling savings and loan association credit risk through capital standards and asset restrictions,firrea controlling savings and loan association credit risk through capital standards and asset restrictions,"On August 9, 1989, President Bush signed the Financial Institutions Reform, Recovery, and Enforcement Act of 1989 (FIRREA)' in an attempt both to resolve the current savings and loan association (S&L) crisis and to insure the future health of the S&L industry. F1RREA seeks to prevent a recurrence of the present crisis by both encouraging and forcing S&L's to reduce the riskiness of their asset portfolios.2 Through stricter capital requirements, FIRREA tries to create incentives for S&L owners and managers to adopt lower risk asset portfolio strategies. To supplement these new incentives, FIRREA attempts directly to limit the riskiness of S&L asset portfolios by imposing new limitations on the amount of certain high credit risk assets that an S&L may hold. Collapse"
103927,1990,yale law journal,federal enclaves and local law:  carving out a domestic violence exception to exclusive legislative jurisdiction,federal enclaves and local law carving out a domestic violence exception to exclusive legislative jurisdiction,"Diane Cobb, a member of the United States Armed Forces, resides and works in a federal enclave.' Solely because of this, questions of law arose as to whether a Massachusetts state court could issue a restraining order to protect her and her infant child from an abusive and dangerous civilian husband,2 and whether such a state-generated order even would be enforceable in a federal enclave.3 The jurisdiction of federal courts does not reach into domestic relations.4 Collapse"
103928,1990,yale law journal,twisting the president's arm:  the impoundment control act as a tool for enforcing the principle of appropriation expenditure,twisting the presidents arm the impoundment control act as a tool for enforcing the principle of appropriation expenditure,"In recent years, in response to ever-increasing budget deficits, numerous politicians and commentators have argued that Congress should give the President greater powers to reduce federal spending through a variety of means, including the impoundment of funds.1 While such arguments may initially seem attractive to those who wish to control or eliminate the deficit, they also have significant implications for the principle of separation of powers and the constitutional requirement that ""No Money shall be drawn from the Treasury, but in Consequence of Appropriations made by Law.""2 In light of the continuing pressures to respond to the budget deficit, proposals for expanded executive impoundment are likely to continue. This Note examines the current state of the law relating to the presidential impoundment of funds and will argue that Congress should resist proposals to expand presidential impoundment powers. Instead, Congress should create additional tools to insure that the President does not unconstitutionally impound funds in the future. Professor Kate Stith argues that there are two governing principles relating to the federal budget: (1) the Principle of the Public Fisc, which ""assert[s] that all monies received from whatever source by any part of the government are public funds""3 and (2) the Principle of Appropriations Control, which ""prohibit[s] expenditure of any public money without legislative authorization.""4 There is also arguably a third principle, which is the inverse of the Principle of Appropriations Control. This principle, which this Note calls the Principle of Appropriation Expenditure, requires the expenditure of all money that is Collapse"
103930,1990,yale law journal,the myth that promises prefer supracompensatory remedies:  an analysis of contracting for damage measures,the myth that promises prefer supracompensatory remedies an analysis of contracting for damage measures,"Courts will not enforce liquidated damage clauses when a stipulated sum exceeds (i) the harm that the promisee could reasonably expect to suffer from breach or (ii) the actual harm that breach turned out to cause.' Courts traditionally have not awarded punitive damages ""for a breach of contract unless the conduct constituting the breach is also a tort for which punitive damages are recoverable.""2 Courts also will not grant specific performance ""if damages would be adequate to protect the expectation interest of the injured party,"" nor will courts enforce contracts that accord promisees a right to specific relief.3 These three rules share the goal of limiting a promisee's recovery to his lost expectation. The liquidated damage rule prevents the promisee from contracting for a supracompensatory remedy, and the punitive damage rule prevents courts from awarding such a remedy. The specific performance rule achieves the law's goal indirectly. A promisee who has a right to specific performance can compel the promisor to perform even when the promisor's loss from performance would exceed the promisee's gain. A promisor can purchase her freedom, but sophisticated promisees sometimes will demand more than their expectation as the price. Permitting specific performance only when damages could not protect the expectation interest limits the ability of promisees to obtain supracompensatory payments by threatening to seek specific relief. The ban on ""specific performance clauses"" prevents a promisee from obtaining by contract he power that the general specific performance rule aims to abolish. Collapse"
103932,1990,yale law journal,city of richmond v. j. a. croson co.:  a federal legislative answer,city of richmond v j a croson co a federal legislative answer,"Last year, in City of Richmond v. J.A. Croson Co.,' the Supreme Court once again struggled with the constitutionality of affirmative action. The struggle revolves around the tension inherent in the Fourteenth Amendment, which mandates the elimination of discrimination while simultaneously compelling the equal protection of all people. This tension is apparent where the elimination of discrimination requires that particular groups be treated disparately. Where the effects of discrimination continue to disadvantage a group even after the explicit discriminatory practice has been prohibited, preferential treatment may be necessary to remedy the continued discriminatory impact. In order to restore a disadvantaged minority to real equality, therefore, a government may have to employ a program that bestows remedial benefits on the basis of race. Absent such means, the group may continue to suffer the effects of past discrimination perpetuated by the application of facially neutral laws to the status quo. Unfortunately, the Croson decision does little to resolve the conflicting mandates of the Fourteenth Amendment.' If anything, the decision perpetuates the tension. The broad directives of the decision recognize the continued need to redress past specific discrimination, implying that the Fourteenth Amendment Collapse"
103933,1990,yale law journal,court of appeals review of agency action:  the problem of en banc ties,court of appeals review of agency action the problem of en banc ties,ERR
103934,1990,yale law journal,making shelter work:  placing conditions on an employable person's right to shelter,making shelter work placing conditions on an employable persons right to shelter,"Homelessness in America is a problem of significant dimensions. Current estimates of the number of homeless' vary from 250,0002 to 3,000,000, 3 with a recent count giving a figure of 500,000-600,000. 4 This national tragedy has sparked many efforts to help. One of the major responses by members of the legal community has been to argue for a right to shelter that would obligate the government (generally state or local) to provide housing to all homeless persons who request it.5 Legal scholars have attempted to define the grounds for such a right,6 and litigators have brought lawsuits seeking to establish it.7 Generally, these advocates have argued for an unqualified guarantee of shelter to any person without a home.' For example, in an early article Frank Michelman described the right to shelter as Collapse"
103935,1990,yale law journal,the proper scope of pendent appellate jurisdiction in the collateral order context,the proper scope of pendent appellate jurisdiction in the collateral order context,ERR
103936,1990,yale law journal,immigration law after a century of plenary power: phantom constitutional norms and statutory interpretation,immigration law after a century of plenary power phantom constitutional norms and statutory interpretation,"With its plenary power doctrine, the Supreme Court erred by rejecting the universal in favor of the particular. Liberal immigration theorists, on the other hand, make the opposite error by rejecting the particular in favor of the universal. Drawing on classic international law publicists and the Catholic philosophical tradition, this essay argues that the two concepts— the state’s greater duty toward its own citizens (the particular) and equal dignity and worth of all human beings (the universal)—go hand in hand: complementing each other and giving the state a qualified right to limit immigration along with a qualified duty to admit vulnerable emigres. Footnote 1 of Justice Scalia’s opinion in Arizona v. United States serves as my point of departure. Scalia says: Many of the 17th-, 18th-, and 19th-century commentators maintained that states should exclude foreigners only for good reason. Pufendorf, for example, maintained that states are generally expected to grant “permanent settlement to strangers who have been driven from their former home,” though acknowledging that, when faced with the prospect of mass immigration, “every state may decide after its own custom what privilege should be granted in such a situation.” Scalia mentions what he characterizes as “prudential limitations” on the power to exclude aliens in the context of his argument that Arizona, like the United States, has a right inherent in sovereignty to exclude aliens from its territory “subject only to those limitations expressed in the Constitution or constitutionally imposed by Congress.” * Gene and Elaine Edwards Chair in Law, University of Oklahoma College of Law. I would like to thank the Oklahoma Law Review for hosting this symposium and my fellow panelists for their insightful comments and engaging dialogue. 1. Arizona v. United States, 132 S. Ct. 2492, 2511 (2012) (emphasis added) (quoting 2 SAMUEL PUFENDORF, OF THE LAW OF NATURE AND NATIONS bk. III, ch. III, § 10, at 366 (C.H. Oldfather & W.A. Oldfather trans., 1934) (1688)) (Scalia, J., concurring in part and dissenting in part). 2. Id. 3. Id. 120 OKLAHOMA LAW REVIEW [Vol. 68:119 I won’t focus on the merits of Scalia’s argument that individual states within the United States retain sovereign powers to exclude. My interest lies, instead, in his recognition that historically the sovereign power over immigration was limited—not plenary. This narrative, although presented in threadbare fashion in the footnote of a dissenting opinion, offers a corrective to 125 years of misguided sovereign absolutism. The dominant narrative—the one that is the subject of this symposium— is succinctly summed up by the Supreme Court in the 1972 case of Kleindienst v. Mandel: In accord with ancient principles of the international law of nation-states, the Court in The Chinese Exclusion Case [Chae Chan Ping] held broadly . . . that the power to exclude aliens is “inherent in sovereignty, necessary for maintaining normal international relations and defending the country against foreign encroachments and dangers-a power to be exercised exclusively by the political branches of government . . . .” 4. David Martin’s symposium contribution provides an implicit reply. See David A. Martin, Why Immigration’s Plenary Power Doctrine Endures, 68 OKLA. L. REV. 29, 31 (2015) (Chae Chan Ping answered “a federalism question,” establishing “that the federal government in fact does possess the authority to regulate migration”). 5. 408 U.S. 753, 765 (1972) (citations omitted). In his contribution to this symposium, Kevin Johnson makes a forceful argument that, as a legal matter, the dominant narrative is muzzled by statutory and regulatory interpretation, lessening the harshness of plenary power’s full potential. See Kevin R. Johnson, Immigration in the Supreme Court, 2009-13: A New Era of Immigration Law Unexceptionalism, 68 OKLA. L. REV. 57, 63-64 (2015) (“[T]he Court has, to a large extent, continued to bring U.S. immigration law into the jurisprudential mainstream.”); see also Martin, supra note 4, at 32 (“The litigation picture is not so bleak as often portrayed. Subconstitutional litigation is plentiful, with a significant success rate . . . .”); Hiroshi Motomura, Immigration Law After a Century of Plenary Power: Phantom Constitutional Norms and Statutory Interpretation, 100 YALE L. J. 545, 560 (1990) (“The principal decisions that have contributed to this expansion of judicial review in immigration cases have not been decisions of constitutional immigration law. Instead, they reached results favorable to aliens by interpreting statutes, regulations, or other forms of subconstitutional immigration law.”). My interest lies in the political/cultural effect of this narrative rather than its strictly legal effect. Margaret Taylor and Kit Johnson’s contribution to the symposium, describing the conditions of confinement at Artesia, suggests that the dominant narrative has much purchasing power in shaping the cultural/political debate. Margaret H. Taylor & Kit Johnson, “Vast Hordes . . . Crowding in Upon Us”: The Executive Branch’s Response to Mass Migration and the Legacy of Chae Chan Ping, 68 OKLA. L. REV. 185, 186 (2015) (moving beyond formal legal doctrine to assess the plenary power doctrine’s influence on “the Executive Branch’s policy response to mass migration”). 2015] SCALIA’S REPLY TO 125 YEARS OF PLENARY POWER 121 Since Chae Chan Ping, “[t]he Court without exception has sustained Congress' plenary power” over immigration. Since that time, this misinterpretation of the “ancient principles of international law of the nation-states” has led the Court to “repeatedly emphasize[] that ‘over no conceivable subject is the legislative power of Congress more complete than it is over’ the admission of aliens.” The plenary power doctrine was born in 1889 in the Chinese Exclusion Case, Chae Chan Ping v. United States. In concluding that a long-term United States resident, Chae Chan Ping, was excludable from the United States on his return home after visiting China, the Court, quoting Chief Justice Marshall in Schooner Exchange v. McFadden, said: The jurisdiction of the nation within its own territory is necessarily exclusive and absolute. It is susceptible of no limitation not imposed by itself. Any restriction upon it, deriving validity from an external source, would imply a diminution of its sovereignty to the extent of the restriction, and an investment of that sovereignty to the same extent in that power which could impose such restriction. The Court, in Chae Chan Ping, failed to quote or explore the Schooner Court’s limitation of this supposedly “exclusive and absolute” right—“all sovereigns have consented to a relaxation in practice, in cases under certain peculiar circumstances, of that absolute and complete jurisdiction within their respective territories which sovereignty confers. This consent may, in 6. Kleindienst v. Mandel, 408 U.S. 753, 765 (1972). 7. Fiallo v. Bell, 430 U.S. 787, 797 (1977) (quoting Oceanic Navigation Co. v. Stranahan, 214 U.S. 320, 339 (1909)). 8. Chae Chan Ping v. United States, 130 U.S. 581 (1889). 9. 11 U.S. (7 Cranch) 116, 136 (1812). 10. Chae Chan Ping, 130 U.S. at 604 (emphasis added). My focus is on the Supreme Court’s initial error in concluding that nation-states have an absolute right to exclude under international law. There are, however, at least two other bases for limiting the sovereign power to exclude: one that would have been available to the Court in 1889 and the other of more recent origin. First, the Court did not analyze whether the Constitution itself placed internal as opposed to external restraints on the ability to exclude. For example, does the Constitution’s equal protection norm forbid exclusion based upon race? See Michael Scaperlanda, Polishing the Tarnished Golden Door, 1993 WIS. L. REV. 965, 976-77. Second, more recent cases, including Kleindienst v. Mandel, fail to address the rights revolution in international law over the past several decades, which limits the sovereignty of nation-states in ways that might be relevant to immigration. See id. at 1009-15. 122 OKLAHOMA LAW REVIEW [Vol. 68:119 some instances, be tested by common usage, and by common opinion, growing out of that usage.” Instead, the Chae Chan Ping Court continued: The power of exclusion of foreigners being an incident of sovereignty belonging to the government of the United States as a part of those sovereign powers delegated by the constitution, the right to its exercise at any time when, in the judgment of the government, the interests of the country require it, cannot be granted away or restrained on behalf of any one. Collapse"
103937,1990,yale law journal,strategic bargaining and the economic theory of contract default rules,strategic bargaining and the economic theory of contract default rules,ERR
103938,1990,yale law journal,what is positive law?,what is positive law,"The parent of a young adult son affliaed with schizophrenia describes his family's experience with schizophrenia and the mental health system. As chairman of the California Task Force for the Seriously Mentally I1l, he outlines the planning process by which a bill was drafted that ultimately may revamp the entire mental health system. Passed by the California legislature and signed into law by the governor, an integrated service system conceived by the determined father of a mentally ill person will now be tested. I am tempted to write, from a family's perspective, about supported work as therapy for persons afflicted with schizophrenia. An idea whose time keeps coming around, work offers a positive alternative to mindless empty days filled with cigarettes, coffee, and TV. It is clear to me that the mentally ill, just as much as any of us, need success and satisfaction that comes only from meeting real and meaningful challenges. I have also been tempted to use this space to question why so few support programs for the mentally ill deal with the assets of the persons they serve, the abilities that are still intact. Why do they drone on mercilessly, focused on symptoms and deficits, invariably arriving at outcomes of little consequence for vulnerable individuals whose brains are terrorized and disorganized by profound disease? But instead of that, I intend to use this communication opportunity to offer a few hard-to-handle observations in the hope that you will find yourself challenging what I say or adding some of your own ideas. It has been 10 years since my son David called me from Harvard University, where he was on the president's list as an outstanding scholar, and said I can handle it, Dad. I don't want you to worry. I've checked into the psychiatric clinic, and they've given me some medicine. It's the strangest thing. I hear voices, hundreds of them, telling me that everyone wants me dead. It's like all the radios of the world blaring all the stations at once, and it doesn't stop. It jams my brain. God, I hope they can help me. I remember those words as clearly as if it were yesterday. Scared, yet confident, needful yet capable, the oldest of my children had embarked on a voyage into what is arguably the most terrifying and painful realm of human experience--the uncharted, unpredictable depths ofbiological brain disease, schizophrenia. How quickly Harvard, perhaps the best and certainly the most self-congratulatory institution of higher learning in our country, wanted him out of there. Knowing what I now know about the disease that afflicts David, I can almost forgive Harvard their haste in abandoning this young man whom they had so aggressively courted because his grade point average and Scholastic Achievement Test scores were nearly perfect and his self-assured entrance essay showed such promise. I won't burden you with a litany of complaints about doctors and medications, hospitals and clinics, treatment programs, residences, governmental bureaucracies, and the like. The insensitive, often abusive behavior of some who provide ""service"" is thankfully balanced by the dedicated, resourceful, and resilient behavior of others. You learn how to be grateful for the good moments and the caring professionals. I found, after a couple of years in this unwanted and demanding experience, that I was so angry and so depleted emotionally and financially that I had to make a conscious change in my own behavior in order that I might survive. As I attempted to locate help for my son, I found, instead of a mental health system, a bewildering, Kafkaesque maze of difficult-to-access programs for which nobody had continuing responsibility or sufficient authority and where standards of quality were nonexistent. Because I work in the media as an observer of the human condition and a storyteller, I went public with my feelings about all of the issues surrounding serious mental illness and especially about the experience of living with my ill son and searching for appropriate care. My wife had insisted that our ill son remain at home to be nurtured in the bosom of the family. Instinctively, she refused to cast out her ill son, to abandon a child in need to "" them"" out there in a less-than-caring world that gave little evidence of concern for his best interests. I admired that loving grit and determination but disagreed totally. Somehow, somewhere, there had to be people and programs and residences capable of offering David more of what he now needed than we in our ignorance could provide. His illness and the behaviors it evoked dumbfounded and frightened us. And, of course, there were our two younger children, a daughter and a son, both in high school, who were suffering psychologically and physically from our ill son's unpredictable antics. Even when he was out of the home during an acute hospitalization, or when he had run off somewhere in a fit November 1990 • American Psychologist Copyright 1990 by the American Psyeholosical Association, Inc. 0003-066X/90/$00.75 Vol. 45, No. 11, 1245-1248 1245 of schizophrenic wanderlust and was living by his wits in the streets of God-knows-where, it was always David's needs and ""poor David's condition"" that seemed to dominate everything and hover everywhere. But the psychiatrists, psychologists, and day-treatment programs we turned to offered processes that had very little relevance to his raging illness and our confusing circumstance--at least in our judgment. We sensed that something was wrong; something was missing. At first, naturally, we suspected our own judgment. We had no prior experience to fall back on, so we listened attentively to professional talk about structure and lowering of expectations and about how to communicate with an ill person who was experiencing terrifying and distorted perceptions. We were game, we were willing to try almost everything. What we found was a paucity of ideas that worked. A lot of mystification of process shrouded in polysyllabic argot--appropriately annotated and attributed to source, of course--but when decoded there was a scant yield of a few tangible suggestions about what we might do to make life more bearable for him and, frankly, for the rest of us, especially if David were to continue living in the family home. Our home, incidentally, was rapidly becoming an institution in which David wasn't getting better and we were all getting worse. Meanwhile, outside of our home, my father was diagnosed as having the most virulent form of leukemia, with a prognosis of six months to live. My mother's cardiologist confided that her heart disease, complicated by diabetes and stressed by my father's sudden grave illness, could snuff out her life at any time. My brother's cocaine abuse, which had destroyed his very successful fashion photography career, was about to take him from the scene. And my mother-in-law was on the verge of the first of three devastating strokes that would leave her bedridden, a veritable vegetable, despite miraculously successful brain surgery. As the family decision maker, I found myself coming to grips with how American medicine delivers benefits to those in need, insured, and capable of paying for the best that can be purchased. It was a soap opera wrapped in a nightmare. By far the worst of circumstances were to be found in the bizarre, nonsystematic way in which our society pretends to support the needs of the seriously mentally ill. In political rhetoric lawmakers give the impression of attention and concern; our bureaucracies display the illusion of care provided. And, as a people, we seem to accept this outrage as if it were the way things had to be. Hundreds of families have thought that perhaps it was their ill family member's unique symptoms that made the existing housing and the day treatment and the medication less than effective, less than adequate. Dan E. Weisburd is President of the California Alliance for the Mentally I11 (CAMI), the largest affiliate of the National AMI. He publishes and edits The Journal of the California Alliance for the Mentally I11, which is now in its second year of publication. Correspondence concerning this article should be addressed to Dan E. Weisburd, 10260 Moorpark Street, Toluca Lake, CA 91602. What came as no small surprise was to find parallel areas of neglect in the other profound illnesses that beset my various family members. Housing was one such area. Go into expensive acute hospital care and almost anything you could ask for is available, paid for in large measure by insurance, or out of pocket if you can afford it. But, try to maintain a home for a stroke victim; a heart patient; or a person gravely ill with leukemia, a drug problem, or a brain disease like schizophrenia, and see what you come up against. Far lower cost? Definitely true. But who will pay for the ""professional care"" needed? The state? No. The federal government? No. The private insurer? No way. Not in most cases. Premium costs would be staggering. The vice president of Blue Cross told me that on the ""Today"" show. So, without a reasonable flow of capital for home care, no trained labor force develops; none exists. Move your seriously ill family member to a community residence, is what society tells you. ""Professionalize"" their care in that way. You want to do the right thing. You turn the patient over to a system of care-a nursing home, a convalescent hospital, a skilled nursing facility. And most are a far cry from the squalor of the past. But few, if any, treat people as individuals or deal with infirmed persons' assets as well as their deficits. Fewer still give them private moments of dignity and recognition and some meaningful way to fill the hours of another precious day of life. That seems to have been ruled out some time in the past as not appropriate to the program, or too costly, or too impractical, or too whatever. Watch as I have watched very ill people strain to communicate through an endless proces Collapse"
103941,1990,yale law journal,"the supreme court and ""civil rights,"" 1886-1908",the supreme court and  civil rights   1886 1908,ERR
103942,1990,yale law journal,constrained individualism in group litigation: requiring class members to make a good cause showing before opting out of a federal class action,constrained individualism in group litigation requiring class members to make a good cause showing before opting out of a federal class action,"Conventional wisdom instructs that class actions, which vest control of litigation involving many class members in the hands of a few class representatives, interfere with the exercise of an individual's right to direct her own lawsuit. Class action proponents and opponents alike accept this dogma when they assume that the only defense of group litigation is that its varied benefits outweigh the harms attendant to a loss of individual control over litigation.1 In contrast, this Note argues that the traditional characterization of individualism is inappropriate in the class action context. The important individual interests implicated by group litigation recommend a revision of Rule 23's ""opt out"" provision so as to make it available to all class members contingent upon a good cause showing.2 Collapse"
103943,1990,yale law journal,altering the course of the constitutional convention: the role of the committee of detail in establishing the balance of state and federal powers,altering the course of the constitutional convention the role of the committee of detail in establishing the balance of state and federal powers,"The Committee of Detail, comprised of five delegates from the Constitutional Convention, produced the first draft of the Constitution. Yet most constitutional scholars have relegated any mention of the Committee to an historical footnote.1 Though several have recognized individual contributions by the Committee,2 none has identified either the full scope of the Committee's work regarding state or federal powers or its significance for the eventual balance of these powers in the Constitution. The Constitutional Convention itself was a response to general dissatisfaction with the weakness of the central government.' In a series of letters shortly before the Convention, James Madison wrote that all ""men of reflection,"" even ""the most orthodox republicans,"" were disturbed by ""[tihe existing Collapse"
103944,1990,yale law journal,accomplices in federal court: a case for increased evidentiary standards,accomplices in federal court a case for increased evidentiary standards,"Only reliable evidence should support criminal convictions.1 The federal courts2 and Congress3 have, accordingly, voiced their condemnation of perjury in criminal trials when suborned by prosecutorial misconduct and have developed sanctions for perjury.4 Yet existing sanctions and strictures have proved inadequate in detecting and curtailing the numerous instances of perjury5 that result when the government, via the prosecutor, entices a witness to testify in return for some type of consideration. 6 Whether in the form of a more lenient sentence, a dropped charge, or immunity, in practical terms these promises are really rewards offered by the prosecution.' Because the prosecution actually induces the testimony, the government should make a forthright attempt to ensure that the ensuing testimony is void of falsity. Perjury8 not only threatens the accuracy of the factfinding process but insidiously undermines the entire Collapse"
103945,1990,yale law journal,a proposed scheme of municipal waste-generator liability,a proposed scheme of municipal waste generator liability,"The Environmental Protection Agency (""EPA"") traditionally has exercised its administrative discretion to allocate liability for the cleanup costs of the nation's privately owned hazardous waste sites by shifting most of the liability to such private-sector entities as generators and transporters of toxic wastes.1 Studies increasingly reveal, however, that the municipal solid waste (""MSW"") present at many of these waste sites2 has considerable hazardous potential,3 thereby making municipalities and their citizens liable for cleanup costs as waste generators.4 The EPA typically makes generators of hazardous waste liable for cleanup costs through enforcement actions pursuant to its statutory mandate: the Comprehensive Environmental Response, Compensation, and Liability Act (""CERCLA"" or ""Superfund""). 5 Superfund liability is but one problem potentially associated with municipal waste generation. Superfund liability stems from harms associated with previous generation and disposal of waste. There are many indications that trends in current municipal waste generation will pose greater problems in the near future.6 Most citizens probably are not aware that MSW is toxic. The American population is producing an unprecedented volume of waste,7 and most states lack comprehensive plans for its safe disposal.' Collapse"
104004,1990,american journal of agricultural economics,exchange rate effects on inputs and outputs in canadian agriculture,exchange rate effects on inputs and outputs in canadian agriculture,"In a seminal piece of work, Schuh demonstrates, using a single market partial equilibrium model, that exchange rates have a large impact on the price received for farm products. He argued that the overvalued U.S. dollar reduced export demand and the domestic price of U.S. grains. Since then, much of the work on estimating the impact of exchange rate changes on agriculture has implicitly assumed that inputs to the agricultural sector come from the nontraded sector.' We argue this assumption can result in an error in the estimation of the effects of exchange rates. In an examination of the passthrough of changes in the $US/CDN exchange rate, we find the data from 1975 to 1989 are consistent with the hypothesis of a complete passthrough of exchange rates for many important variable inputs to Canadian agricultural production. This analysis indicates that, while exchange rates may have had significant impacts on the return to fixed factors and farm income, they have had little effect on grain production. In the case of livestock feeding in Canada, exchange rate devaluations may actually reduce production and reduce the return to fixed factors because inputs (especially grain) are more tradeable than outputs. Finally, we examine the potential welfare impact on nontradeables (such as land) from changes in the exchange rate. Theoretical Model Collapse"
104005,1990,american journal of agricultural economics,performance of mexican agriculture: the effects of economic and agricultural policies,performance of mexican agriculture the effects of economic and agricultural policies,"agriculture to a profitable activity. In this paper I will analyze the relationship between macroeconomic factors, mainly foreign exchange, investment, and prices, and the performance of the farm sector. Three time periods are considered: (a) from 1910, when the Mexican Revolution started, to 1940 when agriculture began to exist in a peaceful environment; (b) from 1940 to 1970, when the agricultural growth rate surpassed the population increase, exports were in progress, and the farm sector was contributing effectively to the whole economy, although the conditions for a poor sector performance were developing; and (c) from 1970 to the present, when, as a result of past and current policy conditions, the rural sector moved to a point where it was unable to contribute as effectively to the economy. Food imports increased, trade was nearly balanced, and the strength of agriculture generally deteriorated. Collapse"
104009,1990,american journal of agricultural economics,imperfect information and the theory of government intervention in farm credit markets,imperfect information and the theory of government intervention in farm credit markets,"This paper describes some possible economic motivations for government intervention in farm credit markets. These motivations are made possible by explicit departures from the perfect information and complete market assumptions that underlie both the neoclassical competitive paradigm and the fundamental theorems of welfare economics. To be more precise, the analysis focuses on three types of asymmetric information between farmers and prospective outside investors, both of whom are assumed to be risk neutral. Arguably, the three informational asymmetries examined here represent natural departures from ""perfect markets"" in that, unlike the unencumbered neoclassical model, each elicits farmers' optimal use of debt contracts (rather than equity instruments or other contractual forms) to raise investment capital. Despite their consistency with observed contractual choices, all three of the models analyzed below yield free-market equilibria that diverge from first best/perfect information outcomes. Given the inefficiency of these market equilibria, the key question raised and discussed here is: Can government improve upon the derived competitive outcomes and, if so, how? Despite the enormity of the credit rationing literature, only a few extant studies address this question in models that are consistent with debt contracts. Among them are DeMeza and Webb (1987, 1988, 1989) and Innes (1988), which focus on the third type of informational asymmetry considered below. The other two information structures posited in this paper have been studied in several analyses of credit markets, but with attention focused on positive attributes of the unfettered competitive equilibrium rather than on prospective implications for welfare-enhancing government intervention. In what follows, th s literature is surveyed and, for the first two information structures, some new policy results a e presented. A critical discussion of the paper's limitations concludes the article. Collapse"
104010,1990,american journal of agricultural economics,the effects of federal credit programs on farm output,the effects of federal credit programs on farm output,"Institutions with the direct or implied support of the federal government have supplied on average in the 1980s 45% of total credit received by the agricultural sector of the U.S. economy. The Farm Credit System (FCS), a borrower-owned cooperative whose bonds carry an implied federal guarantee, held about 27% of total outstanding farm debt in 1987. The Farmers Home Administration (FmHA), a federal agency that makes loans to farmers who are attempting to cope with emergency situations, held an additional 17%. Collapse"
104011,1990,american journal of agricultural economics,moral hazard in federal farm lending,moral hazard in federal farm lending,"In financial markets, moral hazard occurs at both the institutional and borrower level.' It is currently an issue relative to commercial banks and, particularly, savings and loans, where the insurance provided by FDIC and FSLIC has resulted in high-risk activities by institutions under financial stress (Barth, Bartholomew, and Labich). Moral hazard at the individual level results whenever the borrower's economic environment contains incentives which encourage actions that reduce the probability of repaying a loan. Collapse"
104016,1990,american journal of agricultural economics,forecasting multiple time series with little prior information,forecasting multiple time series with little prior information,"Cereal Chem. 67(4):338-342 After a preliminary testing of 10 commercial Southeast Asian noodle affected firmness, and sodium chloride had little influence on the noodle flours, brands A and B were used to determine optimum formulation quality parameters examined. Among the interaction terms, the only for Chinese wet noodles. The response surface study consisted of 27 significant (P < 0.01) contribution to any noodle quality parameter was combinations of the following independent variables: flour protein level that of sodium carbonate and protein content on color. The results suggest (10.5-12.7%), sodium chloride (0.0-3.34%), and sodium carbonate that a Chinese wet noodle of excellent quality can be made from a wheat (0.0-2.0%). Noodle quality attributes measured for each formulation flour in the 10.0-11.5% protein range, using 1.4-1.7% sodium chloride combination were color, pH, firmness, mold growth, and odor. Sodium and 0.7-1.2% sodium carbonate. carbonate affected all properties of partially cooked noodles; protein level Chinese wet noodles, also known as ""boiled noodles"" or ""Hokkien mee"" are possibly the most popular kind of noodles in Southeast Asia (Moss 1971). Essentially, this kind of noodle is obtained by partially cooking raw noodles for a short period of time (1-2 min) resulting in a fine core of dough in the center, surrounded by cooked or gelatinized dough. The noodles are made available to the consumer in this partially cooked form and are then reboiled prior to consumption. Noodle quality has two distinct aspects: appearance and eating quality. Color and brightness are a part of the appearance aspect. Color preference varies with the region, although most consumers prefer a clear pale yellow product free from specks and discoloration. A certain degree of translucency and glossiness adds to product appeal. Chinese wet noodles are unique with regard to their eating quality requirements: a firm ""al dente"" product is considered desirable. Currently there is a need for scientific information to show how wheat flour and other components can best be utilized in the production of Oriental types of noodles. Chinese wet noodles were selected for this study because of their simple formulation and relative popularity. The purpose of this study was to find an optimum formulation for Chinese wet noodles. This involved a detailed analysis of selected Southeast Asian noodle flours and the effect of selected ingredients on noodle quality. A response surface study was done using a fractional factorial design consisting of three variables at three levels. MATERIALS AND METHODS Flour Samples of flour typically used for wet noodle manufacture in Southeast Asia were received from Thailand, Malaysia, Indonesia, and Singapore. No speculation is made regarding the origin of the wheats or their class. The samples were tested (data not shown) for proximate composition and rheological performance. Two of these flours, labeled A (10.5% protein and 0.52% ash, 14.0% mb) and B (12.7% protein and 0.56% ash, 14.0% mb), were selected for use in this investigation because they possessed 'Published with the approval of the Director of the Agricultural Experiment Station, North Dakota State University, as Journal Series no. 1831. 2 Graduate research assistant, associate professor, and food technologist, respectively, Department of Cereal Science and Food Technology, North Dakota State University, and consultant, U.S. Wheat Associates, Singapore. 'Present address: Department of Grain Science and Industry, Kansas State University, Shellenberger Hall, Manhattan, KS, 66506. This article is in the public domain and not copyrightable. It may be freely reprinted with customary crediting of the source. American Association of Cereal Chemists, Inc., 1990. 338 CEREAL CHEMISTRY different levels of protein and ash content. With a maltose content of 19 mg/ 10 g of flour, brand A was brighter and had a higher amylograph peak viscosity (3,310 BU) than brand B (2,980 BU) and was free from sprout damage (falling number 375). Brand B was known to have fungal amylase supplementation (falling number 250) and a maltose content of 23 mg/ 10 g flour. Starch damage contents were low for both brands and the flours showed no oxidizing agents present as indicated by the extensigraph. Flour brand A was known to be a raw material in demand for making Chinese wet noodles in Southeast Asia. Equal parts of flour brands A and B were blended together to give a third flour with an intermediate protein level for the response surface study designed to determine optimum Chinese wet noodle formulation. Flours A and B were obtained from a commercial flour mill in Malaysia. Chemicals Food grade sodium chloride and sodium carbonate (Fisher Scientific Co., Minneapolis, MN) were used. Experimental Design A response surface design described by Cochran and Cox (1971) was used to study the relative contribution of a variable to noodle quality and to determine the optimum noodle formulation. Following preliminary trials, three independent variables were selected: flour protein level (10.5, 11.6, and 12.7, 14.0% moisture basis), sodium chloride (0, 1.67, and 3.34%) and sodium carbonate (0, 1.0, and 2.0%) concentrations, the latter two expressed on flour weight basis. The experimental design, which consisted of three variables at three levels, required 27 formula combinations. The combinations included a formulation having intermediate levels of the three variables replicated 10 times and tested for the purpose of measuring inherent variance in the technique. The design was randomized to increase precision. Five dependent variables were measured for each treatment. They were color, pH, firmness, mold growth, and odor. The data obtained from this study were treated by multiple regression analysis for a second-order response surface equation which contained linear, quadratic, and interaction terms for the three independent variables. The best final equation was found using the stepwise regression procedure described by Draper and Smith (1981). To determine the effects of the variables on the quality of the noodles, contour plots for each quality parameter were generated as a function of two variables while the other variable was held constant. The optimum formulation for Chinese wet noodles was obtained by superimposing the contour plots. Moisture Flour moisture contents were determined in triplicate using the Brabender moisture oven according to the AACC approved air-oven method 44-15 (1983). The semiautomatic moisture tester (Brabender Corporation, South Hackensack, NJ) was used to heat 10 g of flour at 13'C for 1 hr. Protein Flour protein contents were determined in triplicate by the standard Kjeldahl method according to method 46-1 lA (AACC 1983). The results were reported on a 14.0% moisture basis. Noodle Preparation The standard formula for this study consisted of 100 parts of flour, 30-38 parts added water, 1.0 part sodium carbonate, and 1.5 parts salt (Dick et al 1986). The optimum water absorption was determined by the appearance and handling properties of the dough sheet. While insufficient water gave a sheet with jagged edges and nonuniform surface, excess water rendered the dough sheet too extensible and difficult to handle. A dough sheet with a smooth uniform surface and sharp edges was considered optimum. The dough was mixed at high speed in a Hobart model 950 mixer (The Hobart Manufacturing Co., Ltd., Troy, OH). Water, brine, or solution in use was added over 30 sec; the mixing was interrupted at 2 min after start to scrape down the sides of bowl and beater with a spatula. The total mixing time was 5 min. At the end of the mixing stage, the mixture appeared crumbly with nodules of dough having an average diameter less than 3 mm. The mixture was transferred to the Ohtake Noodle Machine (Ohtake Noodle Machine Manufacturing Co., Ltd., Tokyo, Japan) to press into an initial dough sheet by passage between smooth rolls set at a gap setting of 3.8 mm. The noodle machine was set at low speed (setting 1) to facilitate handling. The sheet was folded in half and passed between two rollers perpendicular to the fold at a roll gap setting of 5.68 mm. This cycle was repeated six times. The dough sheet was then gradually reduced in thickness without folding, by passing it between the rolls twice at each of the following gap settings: 3.8, 2.7, and 1.75 mm. A reduction (verified using calipers) in dough-sheet thickness ranging from 29 to 33% was obtained between each subsequent setting. The sheet was dusted lightly on both sides with commercial cornstarch and then passed through the cutter rolls to slit into strands. The noodle so formed was approximately 2.3 mm thick and 2.5 mm wide. The noodles were weighed and placed in polyethylene bags to rest for 30 min at room temperature (250 C). Partial Noodle Cooking Noodles (100 g) were cooked in 1,000 ml of boiling distilled water in a Pyrex beaker according to the procedure developed by Dick et al (1986). The water was maintained at a gentle boil (98 + 0.50C), the noodles were dropped into the water and stirred with bamboo sticks. The noodles were cooked for a standard time (1 min, 30 sec). They were then rinsed for 1 min under cold running tap water, drained for 1 min, weighed, placed in polyethylene bags, sealed and allowed to rest for 30 min at room temperature (25 C). Each trial was done in triplicate. Quality Evaluation of Noodles Five quality parameters were examined: color, pH, firmness, mold growth, and odor. Noodle color was measured immediately after cutting into strands, following a 30 min rest period just before cooking, immediately after cooking, and 30 min after cooking. Noodle color was determined again after 24and 48hr holding at 250C. Noodle pH was determined before and after cooking, and again at 24and 48-hr intervals. Firmness was determined after the cooked noodles had rested 30 min, whereas examination for mold growth Collapse"
104017,1990,american journal of agricultural economics,forecasts from a state space multivariate time-series model,forecasts from a state space multivariate time series model,"specify the lag structure of the model based on the rank of an estimated Hankel correlation ma­ trix. This Hankel matrix of autocorrelations is obtained as the expectation of the outer product of the observed series with itself lagged. The usual carriers of past information, lagged ob­ servations, and moving-average errors are com­ bined in a set of state variables, which are min­ imum sufficient statistics for the past history of the series. Robustness with regard to model specification is obtained by forming these states as specific linear combinations of the observed series. Because of the way in which the states are formed, a largely nonsubjective search can be conducted over possible models while assur­ ing consistency of included coefficients even if model order is misspecified. [For a development of these attributes, see Aoki and Havenner (1989b), Havenner & Aoki (1988b, c), as well as the original Aoki reference.] The state space model consists of two matrix equations: Collapse"
104018,1990,american journal of agricultural economics,forecasts from a nonparametric approach:  ace,forecasts from a nonparametric approach ace,"selection algorithm like the state space models do. ACE does internally make use of the idea of cross validation, which is closely related to prediction error. We have carried that empirical philosophy over to the area of model selection and combined strongly held economic priors with a search for models with low values of the final prediction error criterion. Our model selection strategy was to consider only those models with homogeneity and per capita quantities. Because our interest is in prediction, we considered only those models in which the regressors would be predetermined. Collapse"
104019,1990,american journal of agricultural economics,results of a price forecasting competition,results of a price forecasting competition,"Abstract The transition to competition in the U.S. telecommunications industry has been fostered under an asymmetric regulatory structure. As a result, the local exchange carriers are required to levy access charges on the inter-exchange carriers for the local distribution of long distance messages at rates significantly in excess of underlying economic costs. This rate-cost disparity has incented the inter-exchange carriers to selectively circumvent the local exchange carriers local distribution facilities and interconnect directly with their large and medium-sized business customers. This phenomenon is commonly known as bypass. This study estimates the timing of bypass adoption by establishing a relationship between bypass adoption and competitive entry in the long distance market. Specifically, the study maintains that, from the end-users' perspective, bypass appears as discounted long distance service. Hence, it is possible to analyze AT&T's loss of marketshare to competitors over time and infer a similar market response with regard to bypass. In other words, if AT&T market share erosion can be explained principally by the difference between AT&T and its competitor's toll rates, it should be possible to explain bypass adoption by the difference between what customers pay for switched access and what they would pay for bypass. Utilizing this approach, a model is developed that can be used to gauge the adoption rate of bypass. The model forecasts that bypass adoption would reach nearly 19% of switched access revenues by 1990. The evidence presented in this study clearly attests to the need to move to cost-based pricing of LEC services as rapidly as possible. Collapse"
104020,1990,american journal of agricultural economics,policies for economic development,policies for economic development,"This report is the thirteenth in the annual series addressing major development issues. This report is about the poor. It is thus about the fundamental issue in economic development the eradication of poverty from the world. The report defines poverty in broad terms, to include literacy, nutrition, and health, as well as income. The evidence suggests that rapid and politically sustainable progress on poverty has been achieved by pursuing a strategy with two equally important elements. The first is to promote the efficient use of the poor's most abundant asset labor. It calls for policies that harness market incentives, social and political institutions, infrastructure and technology. The second element is the provision of basic social services to the poor (e.g. primary health care, family planning, nutrition, and primary education). The report concludes that eliminating poverty altogether is not a realistic goal for the 1990s, but that reducing it greatly is entirely possible. Using plausible assumptions about the global economic environment, and with some policy improvements, the report projects a fall of one third in the number of people in poverty by the year 2000. Collapse"
104021,1990,american journal of agricultural economics,adjustment policies and economic development,adjustment policies and economic development,"The principal objective of the International Monetary Fund (IMF), as defined by its articles of agreement, is the correction of balance of payments disequilibrium in member countries. The policies the IMF recommends to reduce the degree and duration of external and internal imbalances that give rise to balance-of-payments difficulties must, however, be set within the context of achieving and maintaining price stability and satisfactory rates of economic growth. In analyzing IMF policies designed to achieve these multiple objectives, it is useful first to consider the circumstances in which the IMF is Collapse"
104022,1990,american journal of agricultural economics,a new paradigm for policy reform and economic development,a new paradigm for policy reform and economic development,"Throughout much of the world, significant institutional changes are in the air, wind, and water. Evidence is accumulating that democracy has won the political battle and that markets have won the economic battle. Some serious scholars have even argued that the history of thought about first principles, including those governing political and social organizations, has come to an end (Fukuyama). For those who hold this view, Hegel was simply too early in forecasting the end of the evolution of thought about such first principles nearly two centuries ago. The entire Marxian experiment was nothing more than a 150-year detour which corroborated Hegel's view that the ""end of history"" coincided with the emergence of liberal-democratic states following the French and American revolutions. Regardless of whether the history of ideology is over, a new consensus on economic, political, and civil freedoms has emerged. This consensus means more than simply adjusting macroeconomic policy, achieving stability, and selling off a few government-owned enterprises in an attempt to set developing countries on a path toward broad-based economic growth. Instead, it means creating a vision of an open economy underpinned by an open polity; identifying and removing the obstacles to economic participation, obstacles that lock the ordinary citizen out of the game; enhancing the availability and utility of information resources by shaping incentives and helping to establish the basic rules of social transactions; encouraging more efficient organization of economic activity whether by market, hierarchy, or hybrid modes (Williamson) in ways that may lead to fundamental restructuring of an economy; and fostering institutional frameworks that expand the role of human choice and promote the full panoply of entrepreneurial energies. Collapse"
104092,1990,brookings papers on economic activity,the stock market and investment: is the market a sideshow?,the stock market and investment is the market a sideshow,"Recent events and research findings increasingly suggest that the stock market is not driven solely by news about fundamentals. There seem to be good theoretical as well as empirical reasons to believe that investor sentiment, also referred to as fads and fashions, affects stock prices. By investor sentiment we mean beliefs held by some investors that cannot be rationally justified. Such investors are sometimes referred to as noise traders. To affect prices, these less-than-rational beliefs have to be correlated across noise traders, otherwise trades based on mistaken judgments would cancel out. When investor sentiment affects the demand of enough investors, security prices diverge from fundamental values. The debates over market efficiency, exciting as they are, would not be important if the stock market did not affect real economic activity. If the stock market were a sideshow, market inefficiencies would merely redistribute wealth between smart investors and noise traders. But if the stock market influences real economic activity, then the investor sentiment that affects stock prices could also indirectly affect real activity. Collapse"
104095,1990,brookings papers on economic activity,european monetary reform: progress and prospects,european monetary reform progress and prospects,"IN THE PAST TWO YEARS, a new plan for monetary union in Europe has gained widespread popularity. The plan has also invigorated the initiative to build a single currency area among European Community (EC) countries-an initiative that has been a recurrent feature of the debate on European monetary policy throughout the postwar period. Indeed, many observers now believe that the achievement of a monetary union is highly likely: C. Fred Bergsten states that Western Europe is ""almost certain [emphasis added] to go beyond 'completion of the internal market' to an Economic and Monetary Union, or EMU."" ' The policy problems related to monetary reform are determined by the approach taken to reform. In the late 1960s, two alternative strategies were much debated; surprisingly, they have received little attention recently. The first program, known as the gradualist strategy (the supporters of which have been labeled ""economists""), relies on progressive removal of trade barriers, convergence of inflation rates, progressive stability of exchange rates, and parallel modification of monetary policies and institutions. The second strategy involves a sudden currency reform (its supporters have been labeled ""monetarists""). This strategy amounts to either the irrevocable locking of exchange rates, with the elimination of target zones, or the replacement of national currencies with a single currency. Both possibilities would require a common central bank to manage the system. Collapse"
104098,1990,brookings papers on economic activity,privatization in eastern europe: the case of poland,privatization in eastern europe the case of poland,"The transformation of the Eastern European economies into market economies requires comprehensive action on three fronts: macroeconomic stabilization, liberalization of economic activity, and privatization of state-owned enterprises.1 Each of these is a monumental task. Nonetheless, privatization stands out as the most difficult and novel of the three, both conceptually and politically. There are enormous challenges in transferring state-owned property — which constitutes around 90 per cent of industrial capital in Eastern Europe — to private hands in a manner that is rapid, equitable, and fiscally sound, and that accomplishes two fundamental goals: the efficient operation of the resulting private enterprises and the development of efficient capital markets. Collapse"
104102,1990,brookings papers on economic activity,hostile takeovers in the 1980s: the return to corporate specialization,hostile takeovers in the 1980s the return to corporate specialization,"HOSTILE TAKEOVERS invite strong reactions, both positive and negative, from academics as well as the general public. Yet fairly little is known about what drives these takeovers, which characteristically involve significant wealth gains to target firms' shareholders. The question is where these wealth gains come from. We examine the sample of all 62 hostile takeover contests between 1984 and 1986 that involved a purchase price of $50 million or more. In these contests, 50 targets were acquired and 12 remained independent. We use a sample of hostile takeovers exclusively to avoid using evidence from friendly acquisitions to judge hostile ones, as many studies have done. We examine such post-takeover operational changes as divestitures, layoffs, tax savings, and investment cuts to understand how the bidding firm could justify paying the takeover premium. We also examine the possibility of wealth losses by bidding firms' stockholders as the explanation for target shareholder gains. The analysis of post-takeover changes is complicated because once the target and the bidding firms are merged, it becomes impossible to attribute to the target the changes recorded in joint accounting data. As a consequence, we do not use such data, but rather focus on discussion in annual reports, 1OK forms, newspapers, magazines, Moody's and Collapse"
104105,1990,brookings papers on economic activity,the impact of corporate restructuring on industrial research and development,the impact of corporate restructuring on industrial research and development,"This paper investigates whether the recent wave of corporate restructuring in the United States has had a negative impact on research arid development investment by industrial firms. Using a newly constructed sample of about 2500 manufacturing firms from 1974 to 1987, I examine three major classes of restructuring events: leveraged buyouts and other ""going private"" transactions, mergers and acquisitions in general, and substantial increases in leverage. The major conclusions are first, that leveraged buyouts do not occur in RD rather, the evidence seems consistent with an agency cost and cash flow-driven model of buyouts. Second, major increases in leverage are followed by substantial declines in the R&D intensity of the firms in question, and the effect takes at least three years to work through. Finally, although the evidence on acquisitions by publicly traded firms is mixed, the basic conclusion is that any declines in the R&D intensity of acquiring firms relative to their past history appear to be associated with the leverage structure of the transaction rather than the acquisition itself. Collapse"
104111,1990,brookings papers on economic activity,vertical integration and market foreclosure,vertical integration and market foreclosure,"Supported by the MIT Energy Lab, the Guggenheim Foundation, the Olin Foundation, the National Science Foundation, the Taussig Visiting Professorship at Harvard and the Marvin Bower Fellowship at the Harvard Business School. Collapse"
104114,1990,brookings papers on economic activity,the concentration-margins relationship reconsidered,the concentration margins relationship reconsidered,"THIS PAPER DISCUSSES an old and much-maligned topic: the cross-sectional relationship between the concentration of firms in the marketplace and price-cost margins. 1 Because it is hard to imagine a literature for which modern graduate students in economics are taught to have more contempt, some immediate justifications are in order. I have two. First, despite the well-known problems with this literature, it continues to affect antitrust policy. The inappropriate inferences used to justify an active antitrust policy have given way to equally incorrect inferences that have been used to justify a relaxed merger policy. Second, the alternative to cross-industry studies is to study specific industries. Indeed, the econometric analysis of individual industries has been labeled the ""new empirical industrial organization. ""2 Although this development is a healthy one, it is important to recall that it was the failure of studies of individual industries to yield general insights that made crossindustry studies popular. One might argue that the primary lesson from three decades of cross-sectional studies is that general principles based on simple indicators are not to be had. Nevertheless, the imprecisions Collapse"
104117,1990,brookings papers on economic activity,"capital, labor, and productivity",capital labor and productivity,"In anticipating future rates of adoption of new technology in forest products, several uniquely important factors come into play. In this respect, the role of innovations imported from other industries, the effect of raw material shortages, the importance of economic factors in adoption of innovations, and the problems presented by the heterogeneity of wood raw material and finished products are discussed. The nature of the adoption process and reasons for long lags between innovation and adoption are also addressed. Certain observations carry. implications for how research and information gathering should be conducted and what priorities should be accorded activities related to technology development and research in forest products. Many times in this century, serious timber shortages have been forecast for the forest products industry. Although the economic scarcity of some wood materials is apparently increasing (the real price of sawlogs has been rising for a long time (17)), other wood materials seem unaffected (pulpwood prices have remained relatively stable over the last four decades (28)). Thus, although numerous wood-saving technological improvements are reportedly “on the shelf ” and others are adopted rapidly by the industry, slow adoption rates for some major innovations undoubtedly reflect an appropriate response to economic conditions rather than conservatism. This paper addresses the following questions: What are some of the principal and unique influences on technological change in the forest products industry that must be understood to anticipate future rates of adoption of new technology? Do these influences currently elicit appropriate rates of technology adoption? The paper has five major sections: 1) the importance of innovations imported from other industries (interindustry flow) and other countries; 2) the effect of raw material shortages; 3) the effect of the economic performance of innovations; 4) problems presented by the heterogeneous nature of wood raw material; and 5) problems presented by the heterogeneity of finished products. It is taken as axiomatic that the impact of technological change is not felt at the stage of invention or innovation, but when improved technologies are actually used in production. For this reason, we pay particular attention to the determinants of the adoption of new technologies. Interindustry technology flow Prospects for technological change in forest products are heavily shaped by 1) commitment of resources to research and development (RD and 2) developments in industries that are remote from forest products. For example, the forest products sector has made considerable use of sophisticated electronics components, including computers, lasers, and computerized axial tomography scanners (on an experimental basis). Many industries depend upon other sectors of the economy for the expansion of their technological capabilities. In the United States, five sectors account for more than 75 percent of total RD and Research Forester, Research Forester, and Forester, USDA Forest Serv., Forest Prod. Lab., One Gifford Pinchot Dr., Madison, WI 53705-2398. Research for this paper was funded by the Forest Prod. Lab. under cooperative agreement No. USDAFP-86-0877. This paper was received for publication in November 1989. © Forest Products Research Society 1990. Forest Prod. J. 40(10):15-22. FOREST PRODUCTS JOURNAL Vol. 40, No. 10 15 outside. For example, the aircraft and missiles sector accounts for the largest amount of total RD and 2) Wood products are not high technology and, therefore, are not likely to be subject to revolutionary technological breakthroughs in their manufacture and use. Such reasoning is parochial and unconvincing. The world is full of old. “mature” industries and products that have been completely revitalized by “revolutionary technological breakthroughs,” as suggested by the adoption of robots, synthetic fibers, and lasers in the textile industry — surely a mature industry. Agriculture and medicine are also mature industries, and yet they have both been transformed by revolutionary technological breakthroughs within the past 50 years. Improving the monitoring and searching activities at the interfaces between forest products and high technology industries and between domestic and foreign industries may facilitate forecasting as well as the transfer of valuable technologies in the years ahead. As we will discuss, the problem in the forest products industry is not maturity. Rather, many of the industry’s difficulties in achieving technological improvements stem from the heterogeneity of raw materials and the wide variety of requirements for finished wood products. Moreover, the adoption of new technologies is often precluded by economic considerations. Response to raw material scarcity The direction of technological change in the forest products industry, as elsewhere, is not a purely random or exogenous phenomenon, even though the industry may be affected by events that originate entirely outside the industry. Rather, technological change is influenced by the changing structure of costs of manufacturing (labor, capital, and raw materials) and the prices of competing products (plastics, steel, and concrete). Technological innovations in the forest products industry tend to have a strong labor-saving bias (11,25), suggesting that the industry tends to increase its competitiveness through improvements in labor productivity. We acknowledge that labor and capital scarcity must be considered because they strongly influence technological change. However, many innovations unique to forest products have been triggered by raw material shortages. In this section, we focus on Collapse"
104118,1990,brookings papers on economic activity,the productivity of capital in a period of slower growth,the productivity of capital in a period of slower growth,"THE UNITED STATES has invested a smaller fraction of its gross national product in capital goods than almost any of its major international competitors in the 40 years since 1948. Over this same period, average labor productivity growth in the United States has also been among the slowest. For the first 25 years of the period there was little cause for dissatisfaction. U.S. productivity growth was higher than it was in the prewar years, and the still higher rates in Europe could easily be explained as a catch-up phenomenon. But after 1973 U.S. labor productivity growth fell to only a little more than 1 percent a year, and in the past five years net investment has dropped substantially. Many people have argued that increasing the level of physical investment in the U.S. economy would have a large payout in higher productivity growth. We agree that investment should be increased, but we suspect that the potential productivity improvements are being exaggerated. The issue is important both for economic analysis and for economic policy. If the growth payoff from increasing the capital stock is large, economic policy can concentrate on raising national saving and investment. If the payoff is small, the nation would be wise to bend some of its efforts toward other means of improving productivity growth. From the perspective of economic analysis, we want to know how to Collapse"
104164,1990,canadian journal of economics,time and space in economic analysis,time and space in economic analysis,"In this article we examine the extent to which three minority groups were able to achieve selected neighborhood social and physical outcomes in the San Francisco mnetropolitan area. Ecological regressions were estimated to generate elasticities that measure the relative abilities of blacks, Hispanics, and Asians to convert education and income into desirable neighborhood environments. These regressions were interpreted in light of substantial differences between the three groups in levels of residential segregation. Results generally indicated a black disadvantage in the process of residential achievement, but it was not as dramatic as that found in earlier studies or as great as the levels of segregation would suggest. As in prior research, education was found to be the critical variable in explaining spatial differentiation and class stratification among blacks. In the United States, residences are allocated to persons and families through private housing markets. Since public housing comprises less than 2% of the nation's housing stock, and less than 1% of its housing starts (Adams 1987), virtually all households seeking a new residence enter the rental or sale market. Each household has a set of housing needs and desires based on its size, composition, life-cycle stage, and tastes; it also has a set of economic resources with which to achieve these desires, principally capital assets and income. Markets allocate households to specific residences through the mechanism of price; households purchase or rent the home that best suits their needs at the price they can afford (Berry & Rees 1969). According to neoclassic economic theory, the price of housing reflects the balance between aggregate demand and supply within local markets (Alonso 1964; Mills 1972). Researchers have pointed out, however, that housing is different from other commodities, and that these differences structure housing markets in distinct ways. First, residences are immobile; they are tied to a particular piece of land and cannot easily be consumed elsewhere (Logan & Molotch 1987). Second, they represent a very significant investment for most families, and for homeowners are the primary means of capital accumulation *Direct correspondence to the authors at the Population Research Center, University of Chicago, 1155 E. 60th Street, Chicago, IL 60637. i) The University of North Carolina Press Social Forces, September 1990, 69(1):15-32 This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 16 / Social Forces 69:1, September 1990 (U.S. Bureau of the Census 1986). Third, each residence is not only tied to a particular plot of land, it is also bound to a specific neighborhood (Massey et al. 1987) and, in turn, to a larger municipality (Logan 1978). Moreover, each geographic unit is associated with a social environment defined by the behaviors of residents and the service benefits provided by local government (Schneider & Logan 1982, 1985; Massey et al. 1987). Finally, houses, and the communities in which they are found, are more than mere commodities; they are also objects of powerful sentiments that influence judgments and condition decisions (Logan & Molotch 1987). In recognition of the fact that housing is immobile and linked to particular places, Charles Tiebout (1956) proposed a ""pure theory of local expenditures"" to account for the differentiated residential structure of cities. The theory rests on two key assumptions that households are free to move, and that places compete to attract them. Local governments offer different packages of tax costs, service benefits, and zoned environments from which home seekers choose. Over time, market competition produces a variety of service-tax-environment mixes and distributes households among them according to income, preferences, and wealth, yielding a residential structure differentiated by socioeconomic status, family life-cycle stage, race, and ethnicity. Although Tiebout's model recognizes some unique features of housing markets, it does not incorporate the fact that housing costs, and particularly home-ownership costs, reflect a substantial investment for families, or that homes, neighborhoods, and communities are the focus of strong emotional attachments. These traits give rise to what Stinchcombe (1965) calls ""communities of fate,"" where residents and institutions have large stakes not only in their own property, but in the property and characteristics of people in surrounding areas, making collective action highly likely. This propensity for collective action segments housing markets along social lines, so that individual choices are constrained by institutional practices and the collective behavior of others (Logan & Molotch 1987). The segmentation of housing markets, and the constraints it imposes on individual home seekers, are well illustrated by the case of U.S. blacks. The Tiebout model and its successors (Bish 1971; Peterson 1981) assume that households are free to move wherever their tastes and economic resources take them. Given this assumption, racial segregation is interpreted simply as the coincidental by-product of sorting based on income, wealth, and tastes (viz. Clark 1986). Considerable evidence, however, indicates that segregation does not stem from black preferences or low black-income levels. Public opinion polls show that blacks strongly endorse the principle of residential integration (Schuman et al. 1985) and express a clear preference for living in integrated neighborhoods, other things being equal (Farley et al. 1978). In spite of these tastes for integration, however, desegregation does not follow from the simple acquisition of socioeconomic resources sufficient to support spatial mobility. As education, income, and occupation rise, black-white segregation does not decline, but persists at a very high level (MIassey 1979, 1981; Denton & Massey 1988). Rather than reflecting tastes or socioeconomic status, black segregation appears to stem from constraints to black residential mobility imposed by the This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms Segregation and Neighborhood Quality / 17 collective behavior and institutional actions of whites. Studies of the real estate industry, for example, indicate that discrimination and prejudice are still widespread. Real estate agents systematically steer black home seekers away from white neighborhoods, and provide them less favorable treatment than whites (Molotch 1972; Pearce 1976; Wienk et al. 1979; Yinger 1986), actions which have been linked analytically to high levels of residential segregation (Galster 1986). At the same time, lending institutions have been found to finance a disproportionately small number of loans in black neighborhoods, even after objective social and economic factors are controlled (Taggart & Smith 1981; Pol et al. 1982; Leahy 1985). Moreover, in the few cases where blacks succeed in entering a white neighborhood, they are frequently met by organized white resistance and hostility (Hirsch 1983; Cass 1986; Bauman 1987), especially in working-class areas (Logan & Stearns 1981; Stearns & Logan 1986a); and if blacks succeed in establishing themselves, the neighborhood is quite likely to be avoided by subsequent white home seekers, resulting in eventual resegregation (Massey & Mullan 1984). Thus, the collective action of white residents and the institutional practices in the real estate and banking industries bifurcate urban housing markets along racial lines, fostering high levels of segregation despite a strong demand by blacks for integration. Some studies suggest that Hispanics face similar barriers, but to a lesser degree (Hakken 1979; James & Tynan 1986). The effect of a racially segmented housing market in creating and sustaining segregation is crucial to understanding the socioeconomic position of blacks in the United States. Barriers to spatial mobility are, in a very real way, barriers to social mobility. As Logan and Molotch (1987:49) point out, socioeconomic inequality among households and geographic inequality among places are not independent; the two systems of hierarchy reinforce one another: ""High status within the social hierarchy can bring access to the most desirable places ... and a guarantee of a rewarding future for whatever place one controls. At the same time a high status for one's geographical place means the availability of resources . . . that enhance life chances generally."" In a similar vein, Giddens (1980:107-12) argues that segregation is a core mechanism of class structuration, since it concentrates people of low status in space and ensures the maintenance of behaviors and orientations detrimental to success in the larger society. In this article we demonstrate how segregation structures the neighborhood environment achieved by three minority groups in one large urban area. We estimate the degree to which blacks, Hispanics, and Asians are able to convert socioeconomic achievements into selected neighborhood outcomes within the San Francisco-Oakland Standard Metropolitan Statistical Area (SMSA) in 1980. The analysis proceeds in three phases. First, we establish the degree of segregation experienced by blacks, Hispanics, and Asians in the San Francisco SMSA. We then consider the extent to which the degree of segregation faced by each group affects its ability to convert status attainments into neighborhood social outcomes. Finally, we evaluate the degree to which each group is able to convert status attainments into different physical characteristics of the neighborhood environment. This content downloaded from 157.55.39.104 on Mon, 20 Jun 2016 06:09:10 UTC All use subject to http://about.jstor.org/terms 18 / Social Forces 69:1, September 1990 Collapse"
104431,1990,international journal of forecasting,experience curve models in the electricity supply industry,experience curve models in the electricity supply industry,"This review is a supplement to the paper by Sharp and Price (1990) and should be regarded as an alternative engineering approach to the modelling and forecasting of experience, or learning, curves. It highlights the problems associated with accurately defining a model to time series that show a combination of a continuous trend and a cyclical component, as detected by the authors in the Sharp and Price data. The authors give a number of alternative perspectives of the same time series, in this case average thermal efficiency data from the U.K. electricity supply industry, with the corresponding conclusions associated with each approach. Particular attention is drawn to the use of the “time constant learning curve” quoted by Sharp and Price which the authors show is a reasonable predictor of the average thermal efficiency. However, a tremendous improvement results from selecting the “ripple” model as a thermal efficiency predictor. Collapse"
104452,1990,journal of accounting research,determinants of auditor expertise,determinants of auditor expertise,"This study sought to determine the factors that influence auditor changes by unquoted companies operating in Nairobi. The study covered the period 1981-1990 and to achieve the objective of the study , a sample of 100 firms out of 25463 companies was selected • The Primary data was collected using a questionnaire . Fifteen factors were obtained from pertinent literature . They included audit fees, quality of audit service, industry expertise , management change, reporting disputes, qualified opinion, auditor reputation, lacked confidentiality, influence of head office , need for non-audit services , size, organisation complexity , demand by shareholders and tax services. The preliminary analysis showed that 32 out the 65 firms changed auditors during the period of study. More auditor changes occurred between 1986 and 1990 inclusive than the period between 1981 and 1985 inclusive. Quality of audit service had the highest sample mean whereas the dependent variable (AC) and tax services had wide variations when compared with the other variables. It was also shown that auditor change was common with small firms. The conditional logit model and correlation analysis were used for data analyses and and the results indicated that: (1) Quality of audit service was very significant in explaining auditor switching. (2) Organisation complexity , size and financial distress were also significant in explaining Collapse"
104454,1990,journal of accounting research,the effect of forecast redundancy on judgments of a consensus forecast's expected accuracy,the effect of forecast redundancy on judgments of a consensus forecasts expected accuracy,"Maines explores the impact of data characteristics on judgments of consensus forecasts' accuracy. The data features studied are the accuracy and redundancy of the individual forecasts being combined. Although Maines' experimental work focuses on sales and EPS forecasting tasks, the impact of data characteristics on the use of data in judgment and decision making is an important issue in many accounting and managerial contexts. Thus, the implications of this study go beyond the particular contexts chosen for illustration. Conference discussion revolved primarily around the issues of hypothesis development, external validity, research design, and interpretation of results. My discussion is also organized around these issues. Collapse"
104455,1990,journal of accounting research,the effect of forecast redundancy on judgments of a consensus forecast's expected accuracy:  discussion,the effect of forecast redundancy on judgments of a consensus forecasts expected accuracy discussion,"Maines explores the impact of data characteristics on judgments of consensus forecasts' accuracy. The data features studied are the accuracy and redundancy of the individual forecasts being combined. Although Maines' experimental work focuses on sales and EPS forecasting tasks, the impact of data characteristics on the use of data in judgment and decision making is an important issue in many accounting and managerial contexts. Thus, the implications of this study go beyond the particular contexts chosen for illustration. Conference discussion revolved primarily around the issues of hypothesis development, external validity, research design, and interpretation of results. My discussion is also organized around these issues. Collapse"
104456,1990,journal of accounting research,analyst following and institutional ownership,analyst following and institutional ownership,"Beginning with Beaver [1968] and Ball and Brown [1968], researchers have commented on the notion that firms differ in how much information is available to their investors. Firm size was first conjectured and later shown in several studies to be correlated with the availability of predisclosure information.1 O'Brien and Bhushan extend this literature by examining whether changes in firm size cause changes in the number of analysts following a firm, or whether the association between analyst following and firm size is due to the correlation between firm size and other variables. They do so by simultaneously modeling analysts' decisions concerning which firms to follow and institutions' decisions concerning which firms to hold in their portfolios. In her presentation, O'Brien emphasized that they are not testing an equilibrium model of analysts' and institutions' behavior, but rather conducting an exploratory investigation into the endogenous and exogenous factors influencing how much information is available about a firm. The conference discussion focused primarily on two areas: the incentives of analysts and institutions, and the empirical procedures and results of the paper. My discussion is structured around these two areas as well. Collapse"
104458,1990,journal of accounting research,a cognitive computational model of risk hypothesis generation,a cognitive computational model of risk hypothesis generation,"The purpose of this paper is to describe and evaluate a cognitive, computational model of risk hypothesis generation during audit planning. A cognitive, computational model is a computer program that mimics human judgment processes and decisions. I chose the risk hypothesis generation task for study because it has not been extensively examined, although research in both auditing and medical decision making has emphasized the importance of hypothesis generation because of the framing effect hypotheses have on information search and evaluation.1 The risk hypothesis generation model presented here specifies the knowledge experienced auditors use to generate risk hypotheses, the processes they use to apply that knowledge, the form risk hypotheses take, and the content of risk hypotheses given case data. The model's accuracy and completeness are evaluated based on auditors' critiques of the model's performance and tests of the model's Collapse"
104460,1990,journal of accounting research,assessing audit risk from errors and irregularities,assessing audit risk from errors and irregularities,"This paper uses the three-part model which identifies audit risk as IR CR . DR, where IR is the inherent risk that a material error or irregularity exists in an account, CR is the risk that the control system does not catch an error given that it exists, and DR is the risk that the auditor's procedures do not identify an error given that it exists and has been missed by the internal control system. The paper also takes as given that the auditor will use a hypothesis-testing framework for decision making. These assumptions are inspired by generally accepted auditing standards. As such, the paper takes an institutional setting as given and derives the behavior implied by the institution, rather than deriving an optimal institution. Analytically, the author's methods resemble those of Fellingham and Newman [1985] (hereafter FN) and Newman and Noel [1989] (hereafter NN).1 Similar to FN, the current paper shows how the equilibrium auditee strategies may be used to determine the analogue of the prior probability distribution used in decision-theoretic analyses of audit risk. Similar to NN, the author develops comparative statics on the equilibrium strategies with respect to auditor and auditee payoffs. A key feature distinguishing this paper from the other analyses is the addition of substantive testing to the audit setting; this allows the determination of DR in a strategic setting. Consequently, the three-part Collapse"
104462,1990,journal of accounting research,"pressure and performance in accounting decision settings: paradoxical effects of incentives, feedback, and justification",pressure and performance in accounting decision settings paradoxical effects of incentives feedback and justification,"This paper shows that the positive effects on decision making of financial incentives, performance feedback, and the requirement to justify one's decisions to others can be undermined or even reversed by the availability of a decision aid. More specifically, in the absence of a decision aid, subjects achieved greater classification accuracy in a repetitive decision task when a monetary incentive was offered, or when feedback about past performance was provided, or when they were required to justify their choices, relative to the absence of these three variables. In contrast, when a statistically valid decision aid was available, the same incentive, feedback, and justification requirements resulted in lower classification accuracy, again relative to the absence of these three variables. These results are interpreted within a framework having two basic tenets. First, financial incentives, performance feedback, and a justifi- Collapse"
104620,1990,journal of policy modeling,devaluations and credibility in structural adjustment policy,devaluations and credibility in structural adjustment policy,"Exchange-rate regimes in transition economies over the last decade have spanned the entire spectrum of possibilities, going from freely floating to permanently fixed (currency boards, DM-ization) through managed floats, preannounced crawling rates, and bands with or without intermittent adjustments. Such extreme diversity is due to differences in available foreign reserves and in initial macroeconomic imbalances (especially the presence of a monetary overhang in some transition economies) and to differences in government preferences between inflation and unemployment. Performance of alternative exchange-rate regimes is difficult to assess, (i) because performance can be mixed, (ii) because all exchange regimes if sustained have a tendency to validate themselves via their impact on inflation, and above all, (iii) because performance depends on the entire package of public policy instruments (fiscal, monetary, and structural) and on exogenous factors, as well as the exchange-rate regime itself. Poland (1990-1999) bears out these considerations. nitially (starting 1 January 1990) a fixed exchange rate was selected, pegged to the U.S. dollar at the then-prevailing free-market rate; it was backed by a $1,000 million stabilization fund provided by the G-24 which was not used but enhanced regime credibility. Monetary overhang had been virtually eliminated by earlier rounds of price increases and of zloty devaluations in 1989. Government preferences for disinflation assigned to the exchange rate the role of nominal anchor for the entire stabilization program. The fixed rate, expected to last no more than 3-4 months, in spite of massive inflation (249 percent in 1990 alone) was maintained until May 1991, when a 17-percent devaluation occurred and, the peg switched to a currency basket (full details are given in Table 1). Subsequently, concerns about externlc balance and the maintenance of trade competitiveness led to a crawling-peg regime, with intermittent additional devaluations, then to an increasingly broader band around a central parity crawling at rates progressively decreasing over time (see Table 1 and Figs. 1 and 2). As in every transition economy, the real exchange rate has been steadily revalued from an initial gross undervaluation. Unlike most transition economies, however, Poland gradually has been able to consolidate the exchange rate (see Fig. 1), stabilize the crawling regime, and even experience occasional nominal revaluations (see Fig. 2). Collapse"
104621,1990,journal of policy modeling,energy and environmental constraints on growth: a cge modeling approach,energy and environmental constraints on growth a cge modeling approach,"This study simulates a CO2 permit market in Romania using a dynamic general equilibrium model. The carbon constraint is set at 20.7% below the reference emissions level for sectors eligible according to the EU-ETS (European Union Emission Trading Scheme). Free permit distribution enhances growth despite a severe emissions cap, because environmental regulation stimulates structural changes (Porter, 1991). That is, grandfathering allows sectors additional resources to invest in developing technologies, but it also raises the CO2 abatement costs because of energy rebound effects from enhanced growth. Results under endogenous growth (Romer, 1990) are very similar to those obtained under an exogenous growth scenario (Ramsey, 1928), as the substitution effects are responsible for the majority of variations; in addition, Romanian research activities are too modest to significantly impact this system. The abatement cost per unit of GDP is higher under endogenous growth, as spillover effects reduce incentives to invest. Technological diffusion continues to have a positive impact on economic growth, which counterbalances the free-riding attitude adopted by some energy-intensive sectors, such as glass and cement. Collapse"
104624,1990,journal of policy modeling,a general equilibrium approach to public utility pricing: determining prices for a water authority,a general equilibrium approach to public utility pricing determining prices for a water authority,ERR
104708,1990,monthly labor review,earnings inequality accelerates in the 1980's,earnings inequality accelerates in the 1980s,"Introduction Though the question of how income is distributed among individuals is one that has long puzzled social scientists, it is only recently that scholars have refocused their attention on the dynamics of inequality in industrialized economies (Atkinson 1997; Conceicao and Galbraith 2001). In large part, the resurgence of interest in the subject has been driven by a series of empirical studies seeking to identify and explain changing patterns in the earnings and employment structure of the United States and the United Kingdom during the late 1970s through the 1980s. Most notable among these trends are well-documented cases of increases in the inequality of wages, for both men and women, as well as significant increases in earnings differentials by skill (Burtless 1990; Bound and Johnson 1992; Katz and Murphy 1992; Levy and Murnane 1992; Freeman and Katz 1995; Jenkins 1995, 1996; Goodman et al. 1997; Gottschalk et al. 1997). In Canada, similar studies of income distribution find that inequality increased throughout the 1980s, albeit at a less pronounced pace than in the United States or United Kingdom (Blackburn and Bloom 1991; Freeman and Needels 1991; Card and Freeman 1994). More recent empirical evidence, however, suggests that the pace of inequality has picked up during the 1990s. Data at the national level show that growth in total income inequality (as measured by the Gini coefficient) among Canadian households accelerated from a 2.9 percent increase over the 1981-1989 period to a 6.5 percent increase from 1990 to 1999. A look at provincial-level data reveals even more pronounced variations. Across the provinces, family income inequality increased modestly and even decreased in a few cases during the 1980s. However, from 1990 to 1999, it increased significantly: for all provinces except Saskatchewan the growth in inequality ranged from about 2 percent in Manitoba to as much as 9.7 percent in Ontario and 11.4 percent in Newfoundland. Despite the evidence that inequality is on the rise in Canada, there remains little agreement as to the causes of such an increase (MacPhail 2000; see also Bluestone 1996 and Storper 2000 for more general perspectives). Several competing explanations have been offered. Factors tied to globalization (trade, immigration, FDI) figure prominently among them and given Canada's greater integration into the global economy there may be some credence to this line of explanation. (1) Other explanations focus more closely on deeper structural changes occurring within economies, namely, those related to technological change, deindustrialization and demographic shifts. Yet others focus on changes to various institutional factors such as declining unionization and real minimum wages or cutbacks to social programs. To examine the potential contribution of these different sources of inequality, most studies use decomposition techniques of one sort or another. While these certainly have merit, trying to quantitatively determine the precise portion of the rise in inequality explained by each possible cause can be quite problematic (Freeman and Katz 1994; Bluestone 1996; U.S. Congress Joint Committee 1997). Above all, the downside of this approach is that it tends to force researchers into limiting the scope of their investigation on only a few specific explanations at a time (MacPhail 2000). In reality, it is most likely that the observed rise in inequality stems from a 'mixed cocktail' of several possible factors (Bluestone 1996; Cline 1997; Gottschalk and Smeeding 1997). The goal of this article is to explore the relationship between rising income inequality across Canadian provinces and the various causal explanations offered in the literature. This question has been examined previously: MacPhail (2000), in particular, offers an excellent entry point into the investigation of the various possible causes of rising inequality in Canada during the 1980s (see also Morissette 1996). … Collapse"
104710,1990,monthly labor review,helping poland cope with unemployment,helping poland cope with unemployment,"This article discusses an approach to working with large groups of students traumatized by a common crisis. Initially, identified as Classroom Crisis Counseling (Brock, 1996; Brock, Sandoval, & Lewis, 1996), this model was influenced by Terr's (1992) discussion of mini-marathon groups. Recently, the author's training in critical incident stress management (Mitchell & Everly, 1996b) led to several significant modifications that are included in the current discussion. The most obvious modification was re-labeling the model as Classroom Crisis Intervention (CCI). This was done to help clarify that crisis counselors are not required to implement this type of intervention (Galante & Foa, 1986). While, mental health professionals will typically play an important role in CCI, it is not crisis therapy. It is best thought of as psychological first aid. As such, it is appropriate for almost any school professional to be a CCI facilitator. As a coordinator of a school crisis intervention team for the past 7 years, the author has had numerous opportunities to use this model. Although designed to be employed following traumatic events (i.e., a school shooting and the witnessing of fatal traffic accidents), CCI has also been used to assist in the management of grief reactions. The author has found CCI to be effective in dealing with these events, and it is hoped that the present article will stimulate further study. General CCI Issues and Procedures Who should participate in CCI? Any group of students having difficulty coping with a common trauma could be brought together in a CCI. As a rule CCI groupings are relatively homogeneous (Mitchell & Everly, 1996a; Weinberg, 1990; Wollman, 1993). Groups can be identified according to several logical categories, including classroom membership, age, grade, familiarity with victims, and / or degree of exposure to a traumatic event (Pitcher & Poland, 1992). However, following major school-wide disasters, CCI is perhaps best employed with naturally occurring classroom groupings. The author has found that the connections already developed by classmates facilitate group sharing and support. It would, however, be appropriate to hold a separate session for those students from different classrooms who were most directly involved with the trauma. Also, it may be necessary to exclude from CCI students who are displaying severe stress reactions (Johnson, 1993). These students, whose crisis reactions may interfere with CCI, will require one-on-one crisis intervention. What is the optimal size of the CCI group? CCI is effective with classrooms ranging in size from 15 to 30 students. Combining classrooms and creating groups of more than 40 students is not recommended (Bell, 1995; Mitchell & Everly, 1996a). Large groups are likely to limit group sharing and interfere with the expression of feelings (Schonfeld, 1989). Where and When Should CCI Be Offered? Whenever possible CCI should take place in the students' regular classroom (Schonfeld, 1989). Traditional counseling settings should be avoided (Farrington, 1995). In this way the unnecessary labeling of students as patients is prevented (Klingman, 1987), and the normality of crisis reactions reinforced. This setting also provides structure and routine that are not only reassuring but also facilitate CCI's group processes. A classroom's rules will make it easier to progress through CCI steps. Also, it is suggested that the normal desk arrangement be used. In general, the physical environment should be as normal and natural as possible. Mitchell and Everly (1996a) have observed most adult emergency service personnel (e.g., firefighters, paramedics and police) must be well defended to perform their jobs. Thus, they recommend not initiating group crisis interventions until trauma survivors are psychologically ready to talk about the crisis. Typically, this means waiting at least 24 hours before debriefing a traumatic event. … Collapse"
104712,1990,monthly labor review,"white-collar pay in goods-producing industries, march 1990",white collar pay in goods producing industries  march 1990,ERR
104713,1990,monthly labor review,testing a census approach to compiling data on fatal work injuries,testing a census approach to compiling data on fatal work injuries,ERR
104714,1990,monthly labor review,employment programs for disabled youth:  an international view,employment programs for disabled youth an international view,"Excerpt] The transition of young disabled people from school to work is an issued of increasing concern to U.S. government policymakers. Currently, only one-third of all disabled Americans with disabilities work, although the remaining two-thirds who are not working would like to have a job, but may or may not be looking for one. However, data show that more than 15 percent of individuals with disabilities are unemployed compared with approximately 5 percent for the general population. For young disabled people, the employment situation has been even worse. The Disability Advisory Council, a commission created by Congress to study the effectiveness of the current Federal employment/disability policy and programs, noted in its 1988 Report to Congress that few high school graduates with developmental disabilities make a successful transition from school to sustained, gainful employment. It is estimated that in 1986, more than 90 percent of these special education graduates became dependent in some way after high school. What are other countries doing to ease the transition process? This report, which is based upon the findings of two cooperative U.S.-Organization for Economic Cooperation Development (OECD) activities, describes employment policies and programs to aid disabled young jobseekers in Japan, Sweden, Italy, Denmark, and the United States. Collapse"
104718,1990,national tax journal,"a normative inquiry into the base of a retail sales tax:  casual sales, used goods, and trade ins",a normative inquiry into the base of a retail sales tax casual sales used goods and trade ins,Develops three propositions that a normative sales tax should reimburse the transferor for taxes paid on unused consumption at a time a used consumer durable is sold.
104722,1990,national tax journal,capital market evidence of windfalls from the acquisition of tax carryovers,capital market evidence of windfalls from the acquisition of tax carryovers,Examines the relation between tax carryovers for acquired corporations and their excess stock returns at the time of announcement of a pending acquisition. Uses a matched pair design and limits the sample to taxable acquisitions which may qualify for a step up in tax basis. Collapse
104723,1990,national tax journal,taxes and firms' dividend policies:  survey results,taxes and firms dividend policies survey results,Attempts to get direct evidence of the importance of tax factors in firms' dividend payout decisions and to distinguish between competing theories of how taxes affect dividends.
104724,1990,national tax journal,rent-seeking and peak-load pricing of public services,rent seeking and peak load pricing of public services,Examines the pricing of usage in a government-provided facility in the presence of rent seeking over the distribution of the revenues raised. Distinguishes government prices and market prices in terms of the control over the revenues raised. Collapse
104744,1990,oxford bulletin of economics and statistics,productivity in the uk services sector:  historical trends 1856-1985 and comparisons with the usa 1950-85,productivity in the uk services sector   historical trends 1856 1985 and comparisons with the usa 1950 85,ERR
104903,1990,review of financial studies,stock market structure and volatility,stock market structure and volatility,"The procedure for opening stocks on the NYSE appears to affect price volatility. An analytical framework for assessing the magnitude of the structurally induced volatility is presented. The ratio of variance of open-to-open returns to closeto-close returns is shown to be consistently greater than one for NYSE common stocks during the period 1982 through 1986. Tbe greater volatility at the open is not attributable to the way in which public information is released since both the opento-open return and the close-to-close return span the same period of time. Instead, the greater volatility appears to be attributable to private information revealed in trading and to temporaryprice deviations induced by specialist and other traders. The implied cost of immediacy at the open is significantly higher than at the close. Other empirical evidence in this article documents the volume of trading at the open, the time delays between the exchange opening and the first transaction in a stock, the difference in daytime volatility versus overnight volatility, and the extent to which volatility is related to trading volume. Collapse"
104985,1990,social science quarterly,phenotypic discrimination and income differences among mexican americans,phenotypic discrimination and income differences among mexican americans,"Objective. We reexamine the issue of phenotypic discrimination against Mexicans in the U.S. labor market, originally studied by Telles and Murguia (1990) and later by Bohara and Davila (1992). We also seek to explain this topic with respect to the Puerto Rican and Cuban populations in the United States. Methods. Instead of using household income as a dependent variable, we use occupational ranking scores computed by Hauser and Warren (1996) in combination with data from the 1990 Latino National Political Survey (LNPS). The occupational rankings more accurately reflect the level of labor market discrimination faced by individuals. Furthermore, the use of the more recent LNPS allows us to update the work of previous scholars and extend the analysis to two previously unexamined Latino groups—Puerto Ricans and Cubans. Results. Our findings indicate that darker‐skinned Mexicans and Cubans face significantly lower occupational prestige scores than their lighter‐skinned counterparts even when controlling for factors that influence performance in the labor market. However, we find no conclusive evidence that skin‐color differences impact occupational prestige scores for Puerto Ricans. Conclusions. Using earlier data, some scholars found evidence for difference in labor market performance among Mexican Americans as a function of phenotypic variations among Mexican Americans. Today, dark‐skinned Mexican Americans and Cuban Americans continue to face higher levels of discrimination in the labor market, whereas dark‐skinned Puerto Ricans do not, which may indicate regional differences across the three groups that need to be controlled for. Collapse"
104987,1990,social science quarterly,the racial convergence thesis in women's intergenerational occupational mobility,the racial convergence thesis in womens intergenerational occupational mobility,No Result.
104988,1990,social science quarterly,"bringing community ses back in:  reanalyzing black suburbanization patterns, 1960-1980",bringing community ses back in   reanalyzing black suburbanization patterns  1960 1980,No Result.
104989,1990,social science quarterly,military substitution effects from foreign economic aid:  buying guns with foreign butter?,military substitution effects from foreign economic aid buying guns with foreign butter,No Result.
104990,1990,social science quarterly,the federal context of regulation: the spatial allocation of federal enforcement,the federal context of regulation the spatial allocation of federal enforcement,No Result.
104991,1990,social science quarterly,patterns of voter participation in the american states,patterns of voter participation in the american states,No Result.
104992,1990,social science quarterly,the county mental institution:  did it do what it was designed to do?,the county mental institution did it do what it was designed to do,No Result.
104993,1990,social science quarterly,an empirical note on salaries in major league baseball,an empirical note on salaries in major league baseball,No Result.
105284,1990,journal of financial economics,political and legal restraints on ownership and control of public companies,political and legal restraints on ownership and control of public companies,"Law and politics affect the financial structure of the public corporation, perhaps as much as economics. Law restricts financial institutions from holding large equity blocks and from networking the small blocks they do own. Impetus for these restrictions came from several sources: a public-spirited belief that financial stability would be fostered by financial fragmentation, American federalism (each state created its own insular set of financial institutions), rivalries between groups of financial institutions, and popular mistrust of powerful private financial institutions. The stability of many of these rules also derives from the political resistance that one would expect corporate managers and benefited financial institutions to offer to any change. Collapse"
105361,1990,population research and policy review,child care and public policy:  introduction,child care and public policy introduction,"One of the objectives of the U.S. Department of Education's National Goals for Education is that ""Children will receive the nutrition and health care needed to arrive at school with healthy minds and bodies, and the number of low birthweight babies will be significantly reduced through enhanced prenatal health systems."" This paper provides background information on the current state of child health, nutrition, and health care in the United States, and on medical conditions and public health problems relevant to academic achievement and the functioning of schools. The paper begins with observations about the relationship between child health and learning. Several ways in which ill health can interfere with the learning process are outlined. The paper touches on the influence of economic disparities in child health status, health limitations, the infant mortality rate, frequency of medical care, place of care, health insurance coverage, nutritional status, and birth weight. The paper concludes by noting that there is a link between child health and educational outcomes, but even substantial progress in improving children's health status cannot be relied on to dramatically alter group differences in academic achievement. Contains 26 references. (CW) ******************************************************************************** Reproductions supplied by EDRS are the best that can be made from the original document. ******************************************************************************** U.S. DEPARTMENT OF EDUCATION Office of Educational Research and Improvement EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) This document has been reproduced as received from the person or organization originating it. Minor changes have been made to improve reproduction quality. Points of view or opinions stated in this document do not necessarily represent official OERI position or policy. CHILD HEALTH AND SCHOOL READINESS: BACKGROUND PAPER ON A NATIONAL EDUCATION GOAL Nicholas Zill, Ph.D. Child Trends, Inc. 2100 M Street, NW, Suite 610 Washington, DC 20037 October 1990 PERMISSION TO REPRODUCE AND DISSEMINATE THIS MATERIAL HAS BEEN GRANTED BY eickx-ol R .Gsrx TO THE EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) Prepared for: Planning and Evaluation Service Office of Planning, Budget and Evaluation U.S. Department of Education Collapse"
105364,1990,population research and policy review,the potential of child care tax credits to reduce poverty and welfare recipiency,the potential of child care tax credits to reduce poverty and welfare recipiency,"As more and more mothers of young children enter the work force, interest in government financing of child care grows. The chief government subsidy for child care is the child care credit in the federal Internal Revenue Code. This is a nonrefundable credit and therefore provides benefits only to those with incomes high enough to require them to pay income tax. Yet of the $ 5.5 billion spent by the federal government on child care in 1986, this program accounted for $ 3.5 billion.This paper simulates the effects of expanding the child care tax credit by (1) doubling the reimbursement rates of the current credit; (2) making the credit refundable; and (3) both making the credit refundable and increasing its value for all families with income below $ 32,000.Results suggest that these changes will have modest effects on the income and earnings of mothers, and on the poverty gap and welfare recipiency. Costs, however, differ substantially. Doubling the value of the credit is far more expensive than either making the credit refundable or making it both refundable and more generous at the bottom of the income distribution. Making the credit refundable may cost taxpayers very little by leading to increases in hours worked and concomitant reductions in welfare payments. Collapse"
105367,1990,population research and policy review,"black business transformation, black well-being, and public policy",black business transformation  black well being  and public policy,"This article examines public policy and social welfare issues related to a recent trend in black business ownership: the decline of black-owned businesses in ‘traditional’ personal services serving a predominantly black clientele, and the corresponding increase of black-owned businesses in ‘emerging’ capital- and knowledge-intensive fields. It is argued that, while the growth of black business ownership in emerging fields is a sign of black economic progress, overall trends in black business ownership are not entirely positive. For one thing, the divergent trends in traditional and emerging black-owned firms reflect widening socioeconomic disparities within black communities. Moreover, the decline of traditional black-owned firms bodes ill for disadvantaged blacks in inner-cities. After reviewing the development of black business enterprise in the United States, trends in black business ownership since the 1970s are examined. Patterns of change in traditional personal services and emerging business services are then linked to social and economic transformations that have enabled many blacks to participate in the larger national economy. The article concludes by discussing the implications of declines in traditional black businesses for black well-being and for public policy. Collapse"
105368,1990,population research and policy review,labor market structure and fertility differences among puerto rican women:  the effects of economic and social policies on opportunity costs,labor market structure and fertility differences among puerto rican women the effects of economic and social policies on opportunity costs,"The oft-observed inverse relationship between economic activity in the formal or informal sector and levels of fertility is attributed to the opportunity costs of reproduction. The economic and social policies that initiate and maintain the substantial flow of federal transfer payments to the Puerto Rican population is likely to reduce the opportunity costs among women participating in the informal economy; therefore, informal labor market participants will have fertility levels more like women who have never worked than like women active in the formal labor market. Using data from the 1982 Puerto Rican Fertility and Family Planning Assessment, this paper compares fertility differentials among ever-married women who have never worked, who have ever worked in the informal economy, and who have only worked in the formal economy. Contrary to expectations, the fertility levels of informal labor market participants are more like those of formal labor market participants; economic activity in either sector is associated with bearing fewer children. Federal transfer payments do not appear to reduce the opportunity costs of reproduction among women employed in the informal economy. An earlier version of this paper was presented at the 1989 meeting of the Population Association of America. Collapse"
105370,1990,population research and policy review,structural changes in high performing minority and nonminority banks in the deregulated banking environment,structural changes in high performing minority and nonminority banks in the deregulated banking environment,"This study considers the changes that have occurred in the balance sheets of high-performance minority-owned banks during the period 1981–1988 and compares them to changes in the balance sheets of high-performance nonminority-owned banks. The changes are analyzed using two methodologies: decomposition analysis and a t-test of the mean differences of the balance sheet and of its component parts. The study examines changes in total assets, total liabilities, and the balance sheets as a whole, and then disaggregates the assets and liabilities to determine where the changes occurred in each group of banks. The findings show statistically significant differences in the average proportions of assets and liabilities between the two groups: minority banks held more cash than nonminority banks, fewer loans, more premises, a higher volume of other assets, more common stock and lower undivided profits. Although the wide differences in the structural changes of assets and liabilities began to narrow in 1985–1987, the analysis indicates the differences may again be increasing between the two groups. Collapse"
105373,1990,population research and policy review,methodological problems and policy implications in sexual harassment research,methodological problems and policy implications in sexual harassment research,"This paper argues that the ability of social research to influence legal arguments and policy decisions on sexual harassment in the workplace has been stymied by several methodological problems which are shared by most major studies on the topic. Determination of the incidence of harassment and its major sub-types is difficult because of problems with sampling (e.g., response rate, sample size) and instrument construction (e.g., number or variety of harassment categories). Additionally, severity of harassment is rarely treated as a variable.Several resolutions to these problems are presented. First, estimates of the proportion of women who have experienced harassment, as well as the proportion having experienced the major sub-types of harassment, are derived. Second, a mutually exclusive and exhaustive set of sexual harassment categories, which includes harassment types that have evolved recently from legal decisions and policy developments, is discussed. Finally, an outline of factors which might be used to assess harassment severity is presented. Resolving these issues will provide social scientists and non-scientists alike with clearer answers to the ‘How much?’, ‘Which types?’ and ‘How serious?’ questions about harassment. Collapse"
105603,1990,journal of policy modeling,"wages, money, and exchange rates with endogenous unions and governments",wages money and exchange rates with endogenous unions and governments,This paper analyzes the role and macroeconomic impact of monetary and exchange rate policy as well as of wage formation in open economies where wages are primarily determined through collective bar ... Collapse
105604,1990,journal of policy modeling,energy prices and co2 emissions in the 1990s,energy prices and co2 emissions in the 1990s,No Result.
105605,1990,journal of policy modeling,computation of a world general equilibrium under bilateral quotas and an application to the analysis of textile trade restrictions,computation of a world general equilibrium under bilateral quotas and an application to the analysis of textile trade restrictions,"Abstract This article describes general equilibrium computational techniques for analyzing models of world trade in which bilateral quota restrictions are imposed on sales by specified sellers to specified buyers. The use of these techniques is motivated by the need to analyze voluntary restraint and ather measures that now restrict a sizeable fraction of world trade. The bilateralism in such arrangements generates the added efficiency cost that buyers will typically not purchase from their least-cost source of supply. From a computational point of view, the presence of bilateral quotas increases the dimensionality of the equilibrium fixed-point problem by a factor equal to the number of trading countries and commodities for which quota restrictions apply. Bilateral quotas are first analyzed in pure exchange economy for which the Gale-Nikaido mapping must be modified for existence and computation of equilibrium. This is followed by a discussion of the extensions needed to incorporate production. An application of the computational techniques to the analysis of global restrictions on trade in textiles and apparel under the Multi Fiber Arrangement is presented, and, in a final section, further areas of potential policy application are discussed. Collapse"
105606,1990,journal of policy modeling,computable general equilibrium analysis of agricultural liberalization:  factor mobility and macro closure,computable general equilibrium analysis of agricultural liberalization factor mobility and macro closure,"Abstract Although agriculture is a relatively small part of the U.S. economy, changes in agricultural policies may have significant economywide impacts. The nature of the impacts will depend on the degree of factor mobility and disposition of the saved farm program expenditures; and will be conditioned by existing sector-specific distortions. We present the application of a 10-sector CGE model under various factor mobility and microeconomic closure assumptions to analyze unilateral and multilateral agricultural liberalization. We demonstrate the implications of the various assumptions both for linking single country to multicountry models and for overall policy analysis. Collapse"
105607,1990,journal of policy modeling,an empirical assessment of macroeconometric and cge approaches in policy modeling,an empirical assessment of macroeconometric and cge approaches in policy modeling,"Abstract The macroeconometric (ME) and computable general equilibrium (CGE) models can be considered the cornerstones of the spectrum of quantitative models used today for macroeconomic policy analysis. In the paper we design two small-scale models (an ME and a CGE model) in such a manner that they are representative of their large-scale counterparts, estimate them on a common database, and attempt a systematic comparative assessment of their simulation properties. We suggest various possibilities for use in policy analysis that explore and combine the results of both models. Collapse"
105821,1990,journal of banking and finance,"statistical study of foreign exchange rates, empirical evidence of a price change scaling law, and intraday analysis",statistical study of foreign exchange rates empirical evidence of a price change scaling law and intraday analysis,Abstract In this paper we present a statistical analysis of four foreign exchange spot rates against the U.S. Dollar with several million intra-day prices over 3 years. The analysis also includes gold prices and samples of daily foreign exchange prices over 15 years. The mean absolute changes of logarithmic prices are found to follow a scaling law against the time interval on which they are measured. This empirical law holds although the distributions of the price changes strongly differ for different interval sizes. Systematic variations of the volatility are found even during business hours by an intra-day analysis of price changes. Seasonal heteroskedasticity is observed with a period of one day as well as one week as the result of an analogous intra-week analysis; taking this into account is necessary for any future study of intra-day price change distributions and their generating process. The same type of analysis is also made for the bid-ask spreads. Collapse
105970,1990,conflict management and peace science,consequential damage and nuclear deterrence,consequential damage and nuclear deterrence,"We present a geometric generalization of the Intriligator-Brito deterrence/attack model that can accomodate a variety of assumptions about the effectiveness of weapons. Our analysis implies that empirical consideration of weapons effectiveness is crucial to strategic application of the model. We also incorporate consequential damage into the model, i.e., destruction that is not relatively immediate, but is realized over long periods of time after war has broken out. We find that consequential damage significantly alters the deterrence/attack interpretations applied to relative and absolute weapons stocks in a nuclear deterrence relationship. Collapse"
105971,1990,conflict management and peace science,expected utility and peace science:  an assessment of trade and conflict,expected utility and peace science an assessment of trade and conflict,"In this paper, I attempt to tease out the implications of expected utility models on conflict resolution and peace science. I also consider the trade-conflict model of Polachek (1980) and its extensions, as one example of expected utility in international relations. I raise three questions concerning the impact of expected utility: (1) when does trade mitigate conflict? (2) can trade provoke conflict? (3) can trade induce cooperation? From a review of empirical findings, I consider the political content of the trade-conflict model in particular, and of expected utility models, in general. Subsequently, I discuss two additional questions: (4) how do exchange norms influence trade and conflict? and (5) what incentives exist to foster cooperation from trade under expected utility? I advance the critique that while expected utility underrepresents political actions and motives, the implicit content of politics in the trade-conflict model is predominantly realist. Given this tendency, expected utility in conflict resolution and peace science will invariably be used to model strategic and geostrategic concerns. Collapse"
105972,1990,conflict management and peace science,conversion:  the trade-off between military and civilian production in warsaw pact countries,conversion   the trade off between military and civilian production in warsaw pact countries,"The main task of Project LINK is to forecast and study the development of alternative prospects for the world economy. It is based on an integrated system of econometric models for each major country or region of the world. Currently the system consists of 79 models. The total number of equations is more than 20,000, and the LINK data set contains more than. one million observations. In Project LINK, it is assumed that model-builders know their own country best. Each model-builder is, thus, free to design their country's model in all respects except that (1) each country model needs to generate four categories of imported goods in real terms, and the associated four price export indices, and (2) the demand for real exports from each country and import prices in each country for the four categories of imported goods are to be generated by the LINK system. In this manner, a feedback mechanism operates. It is also possible to estimate changes in exchange rates, because in the leading industrial countries these rates are either related to internal and external variables or linked to the major foreign currencies. Because of its feedback characteristic and its flexibility, it is possible to incorporate in each country model certain variables that are important for the model of the world solution. For example, government consumption or military spending in each country may be interlinked and during simulation an outcome of the world model solution will influence the individual country results. In this work, we present the possible outcomes of military cuts in the Soviet and Eastern European economies using appropriate macroeconometric models. These models are based on the economic model of a centrally planned economy, described in the next section. In Section 3, we present some results of the simulation exercises. Collapse"
105973,1990,conflict management and peace science,progress in global modeling for world policy on arms control and environmental management,progress in global modeling for world policy on arms control and environmental management,"In this paper, I wish to report on progress on global modeling for world policy on arms and environmental control. Needless to say, in the minds of many, arms control and environmental management are the two most important problems confronting the world today. However, before spelling out the advances scored, it is pertinent to say a few words about the role and relevance of global modeling. There are, as is well known, scholars and analysts skeptical of the usefulness of big models who will undoubtedly be even more skeptical of the usefulness of still bigger models such as the one envisioned in this paper. They will make the usual claim that such big models are beyond the understanding of any one individual and that no one can understand the detailed operation of a model that embodies 50,000 equations, let alone the 20,000 in the LINK submodel to be used. Yet there are other scholars expert in a particular part of the global model who do, or should, recognize how, from the operation of a global model, some key variables in one part are linked to one or more key variables in other parts. Then there are other scholars who perhaps lack the ability to examine and fully understand in detail the operation of any part but who do have the capacity for perceiving the grand picture-for seeing how the parts are interconnected in the main, and for understanding those connections, For each part of a global model, one or more of these scholars will undoubtedly have the capacity for identifying an expert who understands that part in depth. The scholar, thus, can assemble a team to direct the use of each part in a global model operation. Finally, to reiterate, no model ever replaces intuition in both theorizing and attaching significance to any set of generated numbers. Collapse"
106165,1990,applied economics,technology and health expenditures: a demand approach applied to a process innovation,technology and health expenditures a demand approach applied to a process innovation,There is controversy among economists as to whether the diffusion of new medical technologies has been a contributing factor to rising health expenditures. The economic literature is critically reviewed and another approach is advocated. This alternative approach rests on the distinction between product and process innovations. It is argued that the relationship between process innovations and health expenditures can be illuminated by determining if the process innovation and the original procedure are utilized as substitutes. The empirical results provide no indication that alternative technologies for diagnosing diseases/conditions of the upper gastrointestinal tract have been utilized as substitutes by Australian medical practitioners operating on a fee-for-service basis. This study finds no evidence to indicate that the process innovation of fibre optic endoscopy has reduced health expenditures associated with diagnosis of diseases/conditions of the upper gastrointestinal tract. Collapse
106166,1990,applied economics,disentangling the effect of arthritis on earnings: a simultaneous estimate of wage rates and hours worked,disentangling the effect of arthritis on earnings a simultaneous estimate of wage rates and hours worked,Poor health affects both wage rates and hours of work. Here we develop a procedure for examining the separate impact of a specific chronic disease – arthritis – on wages and hours worked and then translate these results into earnings. Our methodology provides a consistent approach for measuring how a specific chronic disease affects total earnings as well as its component parts. We find that arthritis affects wage rates and hours worked differently and the pattern of these effects differs considerably between men and women. Collapse
106167,1990,applied economics,on unit roots and real exchange rates: empirical evidence and monte carlo analysis,on unit roots and real exchange rates empirical evidence and monte carlo analysis,"This paper presents some empirical evidence that real exchange rate series contain a unit root in their time series respresentation, which cancels out on first differencing, using the augmented Dickey–Fuller test. The power of this test is also tested using Monte Carlo methods and it is found to be quite powerful against a range of stationary local alternatives. The findings imply the absence of any tendency of the nominal exchange rate to converge on purchasing power parity, even in the long run. Collapse"
106168,1990,applied economics,current account deficits and long-term interest rates: is there a risk premium?,current account deficits and long term interest rates  is there a risk premium ,ERR
106169,1990,applied economics,economic information about ironmaking,economic information about ironmaking,"Ironmaking is an important world-wide industry. Iron produced in blast furnaces is a crucial inout for almost three-quarters of the steel produced in the world according to the International Iron and Steel Institute. Few industries have recieved as much attention as the steel industry, yet very little is known about the cost structure of ironmaking, one of its major inputs. This paper attempts to fill that void by the first application of the restricted translog cost function on time series data for the ironmaking plant of one of the world's major steel producers. All this information revealed is new. For the first time, estimates of the elasticities of inout demand for variables inputs and elasticities of substitution are presented. Major results include the findings that there is more substitution are presented. Major results include the findings that there is more substitution between variable inputs than was previously thought, that disembodies technical change exists and is Hicks neutral, and that te... Collapse"
106170,1990,applied economics,monetary regimes and money supply endogeneity,monetary regimes and money supply endogeneity,"This paper investigates the effectiveness monetary policy by Granger causality tests in the two regimes of inflation and deflation, respectively. The surplus lag rolling estimation is applied to deal with the problem of the frequent structural changes in the Chinese monetary system. We found that the monetary policies have become less effective in stabilizing the price level in the deflation era that started from 1998. There is also empirical evidence to suggest that money was endogenous in China during the inflation period. This implies that the People's Bank of China had difficulty exercising the power of money supply to reduce inflation if the endogeneity was the result of the market behaviour. However, if the endogeneity was due to the government inflation-targeting rule, then there is no evidence to suggest that this rule has been effective for M0, M1 and M2 instruments, except for the M0 instrument during the inflation period of April 1990 to March 1995. Although it was found that money ceased to be endogenous in the deflation periods, it does not support the proposal of utilizing the money supply as a policy instrument, as we found that money is impotent in influencing price in the deflation regime. Our findings provide some empirical evidence to support the Chinese government adopting alternative policy instruments such as an active fiscal policy in the era of deflation. Collapse"
106172,1990,applied economics,dynamic factor demands and market structure under rational expectations: some estimates for the italian manufacturing system,dynamic factor demands and market structure under rational expectations some estimates for the italian manufacturing system,This paper deals with a dynamic model of the behaviour of firms under rational expectations. The adjustment cost framework is followed to describe production technologies. The objective of firms is to maximize the stream of future profits under rational expectation. First-order conditions for the optimum are derived with respect to input demands and one output supply. The output market structure is modelled in order to test price taking behaviour. The adjustment costs of quasi-fixed inputs are internal and interrelated. The model is estimated for the Italian manufacturing system with encouraging results. Collapse
106173,1990,applied economics,do corporations sell houses for less? a test of housing market efficiency,do corporations sell houses for less a test of housing market efficiency,This paper examines the popular notion that houses sold by corporations as a part of the employee relocation process sell at a discount. We argue that such discounts conradict the notion of an efficient market in which indentical houses have the same expected sales price. We test for corporate–individual owned house price homogeneity using data from a metropolitan area in the US. The evidence supports our ‘single price’ hypothesis: corporate houses do not sell for less than they would if sold by individuals. Collapse
106174,1990,applied economics,the post-quota performance of dairy farms in england and wales,the post quota performance of dairy farms in england and wales,"Dairy quotas were introduced in April 1984. Since then, producers have been adjusting their production systems to cope with a constraint on milk output. In this paper, stochastic production functions are used to examine both allocative efficiency of input use and technical efficiency in the industry during the post-quota period. The results indicate that producers have successfully adjusted the use of the variable input, feed, to optimal levels but are finding it more difficult to adjust quasi-fixed inputs such as labour, land, machinery and the herd size. Technical efficiency has remained relatively constant. Collapse"
106176,1990,applied economics,consistent estimation of a disequilibrium model of primary commodity price adjustment,consistent estimation of a disequilibrium model of primary commodity price adjustment,"The Wu-Hausman endogeneity test is used to examine demand specifications for turkey meat. ln contrast to general poultry, quantity, not price, is found to be predetermined in demand models that use annual turkey data. Seasonal consumption and the long production cycle for turkeys, relative to broi lers, may account for this result. *The authors are assistant professors in the Department of Agricultural Economics at Michigan State University and the Department of Agricultural and Resource Economics at North Carolina State University, respectively. This paper was prepared for presentation at the American Agricultural Economics Association Meetings, San Antonio, TX, July 28-31 , 1996. Endogeneity Testing inTurkey Meat Demand Introduction While many researchers have focused on issues of demand estimation and industry structure in the livestock and poultry sectors, few have addressed these concerns specifically for the U.S. turkey industry. Previous poultry studies have generally concentrated solely on broiler chickens (Knoeber and Thurman, 1995; Moschini and Meilke, 1989) or have analyzed an aggregate poultry commodity (Thurman 1986, 1987). However, the dissimilar production and consumption patterns of turkeys and broiler chickens, the primary poultry commodities, suggest that studies addressing demand and industry structure issues would benefit by individually examining each industry. In this paper, the specification of the demand equation for turkey meat is analyzed. Specifically, the issue of treating either price or quantity as a predetermined variable in the demand equation is investigated. The U.S. Turkey Industry Although the organizational structure of the turkey industry parallels that of the broiler chicken industry, still there is reason to examine the turkey industry as one distinct from the broiler industry. Production and consumption patterns strongly differ between the two commodities. Whereas more than 90 percent of broiler chickens are produced via production contracts, the number of integrators contracting with farmers to grow turkeys is closer to 60 percent of total production (Knutson, Penn and Boehm 1995). Company-owned farms, marketing cooperatives, or independent farmers account for the rest of turkey production. Also in contrast to broilers, which have a short 5-6 week production cycle, the production cycle for turkey poults is approximately 5 months from the time of hatching to slaughter. Most of the turkeys marketed reach an acceptable market age and weight between 4-7 months. Finally, turkey production itself is highly seasonal. In 1960, more than 50 percent of all turkey consumption took place during the last three months of the year. In 1990, 35 percent of yearly turkey consumption still occurred during the last quarter with November, not surprisingly, as the highest consumption month (Perez, Weimar and Cromer 1991). Consumption and marketing standards have also been changing in the turkey industry. Especially notable is the increase in per capita consumption and the decrease in real turkey prices (Figures 1 and 2). Since 1960, the real price of turkey has declined by almost 150 percent while per capita consumption has nearly tripled. This relationship is not unique to the turkey industry, broiler chicken consumption has followed a similar trend, but it is unusual relative to pork and beef consumption. Contributing to the changes in the turkey industry are the new processing and marketing practices that have increased the accessibility and convenience of turkey meat for consumers. Ground and formed turkey products, such as turkey bologna, turkey ham and turkey hot dogs, have become standard items in deli and meat counters. All of these factors increase the value of studies specific to the turkey industry and raise the issue of how to specify the demand equation. Demand Specification Demand equations may be specified in one of three ways. The textbook treatment is a system of simultaneous equations that treat both price and quantity as endogenous variables in a 2 supply and demand framework. Alternatively, based on consumer preferences and the structure of the particular industry, there are cases when either price or quantity is argued to be predetermined in demand. For instance, given a price supported commodity, price may be treated as the exogenous and quantity as the endogenous one provided that the support price is binding during the period investigated. A more popular way to interpret the argument for predetermined price is the concept of fixed short-run retail prices. Again, for data evaluated within a particular time, price adjustments do not occur, rather, quantity adjusts to clear the market. Finally, price may be predetermined in annual data in a competitive industry that faces constant returns to scale and elastic factor supplies. In this case, price is determined by input costs and demand determines quantity. Thurman (1987) argues that this is indeed the case for annual poultry data for the period between 1955 and 1981. 1 Arguments for predetermined quantity also are found in the literature. The most popular one suggests that supply is perfectly inelastic due to the perishability of nonstorable commodities. Here, production decisions made well in advance of harvest cannot be altered at the time of market. Consequently, quantity is fixed and price adjusts to clear the market. It is not clear, a priori, if either consumer preferences or the structure of the turkey industry would cause price or quantity, if either, to be predetermined in a demand equation. Thurman's research suggests that price is predetermined in annual poultry data. However, given the production and consumption differences between broilers and turkeys, it is not necessarily 'An anonymous referee intuitively noted that free trade may be an additional argument for exogenous price. In this case, the world price is viewed as predetermined and quantity adjusts to absorb demand disturbances. 3 clear that this would hold for turkeys as well. The question of whether price or quantity, or neither, can be treated as predetermined in a demand equation is examined by using the WuHausman test. The Wu-Hausman Endogeneity Test The Wu-Hausman test for endogeneity was developed by Wu in his 1973 paper, Hausman in a later 1978 article, and applied to the poultry industry by Thurman in his 1986 and 1987 studies. The logic behind the Wu-Hausman test is this. Under the null hypothesis of no misspecification, both an OLS estimator, b, and an instrumental variables estimator, b · , will be unbiased and consistent estimators of the right-hand side variable (either predetermined price or quantity). However, under the alternative hypothesis of misspecification, the OLS estimator is no longer unbiased and consistent. If the null hypothesis is true, the difference between the two estimates, q = b b · , will have a probability limit of zero. If misspecification is present, then the probability limit of q will differ from zero. Based on this relationship, the Wu-Hausman test for endogeneity uses the following test statistic: w =cc b')[V(q)r c6 b'). Here, [ V(q)] is a consistent estimator of V(q). Under the null hypothesis of no misspecification, Wis distributed asymptotically chi-square. As pointed out by Hausman, construction of this test statistic is simplified by noting that under the null hypothesis of no misspecification, asymptotically V( q) = V ( b ·) V ( b) . In the case of either predetermined price or predetermined quantity, q is a scalar. Collapse"
106177,1990,applied economics,"government size and economic growth: the african experience, 1960-85",government size and economic growth  the african experience  1960 85,"Part I: The Cold War System Abroad and at Home Chapter 1: The Atomic Bombings Gar Alperovitz, ""Why the United States Dropped the Bomb,"" Technology Review (August 1990) Barton Bernstein, ""The Atomic Bombings Reconsidered"" Foreign Affairs, v. 74/1 (Jan/Feb 1995) Chapter 2: The Origins of the Cold War John Lewis Gaddis, We Now Know: Rethinking Cold War History (Oxford University Press, 1997) Carolyn Eisenberg, Drawing the Line: The American Decision to divide Germany, 1944-1949 (Cambridge University Press, 1996) Bruce Cumings, ""The Wicked Witch of the West is Dead. Long Live the Wicked Witch of the East,"" in The End of the Cold War: Its Meaning and Implications, ed.by Michael J. Hogan (Cambridge University Press, 1992) Chapter 3: The Red Scare Leslie Fiedler, ""McCarthy,"" Encounter, v. 3 (August 1954) Ellen Schrecker, No Ivory Tower: McCarthyism and the Universities (Oxford University Press, 1986) (excerpt) K.A. Cuordileone, ""'Politics in an Age of Anxiety': Cold War Political Culture and the Crisis of American Masculinity, 1949-1960,"" Journal of American History, v. 82 (September 2000) Chapter 4: Affluence, Domesticity and the Fifties Elaine Tyler May, ""Cold War-Warm Hearth: Politics and the Family in Postwar America,"" in Steve Fraser and Gary Gerstle, eds., The Rise and Fall of the New Deal Order, 1930-1980 (Princeton University Press, 1989) Alan Brinkley, ""The Illusion of Unity in Cold War Culture,"" in Peter Kuznick and James Gilbert, eds., Rethinking Cold War Culture (Smithsonian Institution Press, 2001) Margaret Rose, ""Gender and Civic Activism in Mexican American Barrios in California: The Community Service Organization, 1947-1962,"" in Joanne Meyerowitz, ed., Not June Cleaver: Women and Gender in Postwar America, 1945-1960 (Temple University Press, 1994) Part II: The System Under Stress Chapter 5: The Black Freedom Struggle Vincent G. Harding, ""Beyond Amnesia: Martin Luther King, Jr., and the Future of America,"" Journal of American History, v. 74/2 (September 1987) Timothy B. Tyson, ""Robert F. Williams, 'Black Power,' and the Roots of the African American Freedom Struggle,"" Journal of American History (v. 85/2 (September 1998) Charles M. Payne, ""The Whole United States is Southern!: Brown v. Board and the Mystification of Race,"" Journal of American History, v. 91/1 (June 2004) Chapter 6: The New Radicals Doug Rossinow, ""'The Break-through to New Life': Christianity and the Emergence of the New Left in Austin, Texas, 1956-1964,"" American Quarterly, v. 46/3 (September 1994) Max Elbaum, ""What Legacy from the Radical Internationalism of 1968? Radical History Review v. 82 (Winter 2002) Chapter 7: Second-Wave Feminism Sara Evans, ""The Origins of the Women's Liberation Movement"" Radical America, v. 9/2 (March-April 1975) Nancy MacLean, ""The Hidden History of Affirmative Action: Working Women's Struggles in the 1970s and the Gender of Class,"" Feminist Studies, v. 25/1 (Spring 1999) Chapter 8: The War on Poverty Allen Matusow, The Unraveling of America: A History of Liberalism in the 1960s (Harper & Row, 1984) (excerpt) Michael B. Katz, The Undeserving Poor: From the War on Poverty to the War on Welfare (Pantheon, 1989) (excerpt) Chapter 9: The Vietnam War Robert Buzzanco, Vietnam and the Transformation of American Life (Blackwell, 1999) (excerpt) George C. Herring, ""Vietnam, American Foreign Policy, and the Uses of History,"" Virginia Quarterly Review, v. 661 (Winter 1990) Chapter 10: The Crisis of the State Lewis L. Gould, The Modern American Presidency (University Press of Kansas, 2003) (excerpt) Michael Schudson, Watergate in American Memory (Basic Books, 1992) (excerpt) Part III: A New Domestic and World Order Chapter 11: The Conservative Ascendancy Thomas Byrne Edsall and Mary D. Edsall, Chain Reaction: The Impact of Race, Rights and Taxes on American Politics (W.W. Norton & Co, 1991) (excerpt) Lisa McGirr, Suburban Warriors: The Origins of the New American Right (Princeton University Press, 2001) (excerpt) Thomas Frank, What's the Matter with Kansas? (Metropolitan Books, 2004) Chapter 12: Reaganomics and Beyond Barbara Ehrenreich, Fear of Falling: The Inner Life of the Middle Class (Pantheon, 1989) (excerpt) Robert Collins, More: The Politics of Economic Growth in Postwar America (Oxford University Press, 2000) (excerpt) Robert Meeropol, Surrender: How the Clinton Administration Completed the Reagan Revolution (University of Michigan Press, 1998) (excerpt) Chapter 13: The End of the Cold War Dinesh D'Souza, ""How Reagan Won the Cold War,"" National Review, v. 49/22 (24 Nov 1997) Daniel Deudney and G.J. Ikenberry, ""Who Won the Cold War?"" Foreign Policy, v.87 (Summer 1992) Edward Pessen, Losing Our Souls: The American Experience in the Cold War (Ivan R. Dee, 1993) (excerpt) Chapter 14: Globalization Evelyn Hu-Dehart, ""Globalization and Its Discontents: Exposing the Underside,"" Frontiers-A Journal of Women's Studies, v. 24/2-3 (June-Sept 2003) Jagdish Bhagwati, ""The Human Face of Globalization,"" Global Agenda, v. 2 (January 2004) Douglas S. Massey, ""Closed-Door Policy,"" The American Prospect (July/August 2003) Chapter 15: The 9/11 Attacks The National Commission on the Terrorist Attacks Upon the United States, The 9/11 Report (Government Printing Office, 2004) Chalmers Johnson, Blowback: The Costs and Consequences of American Empire (Henry Holt and Co, rev. ed., 2004) (excerpt) Collapse"
106178,1990,applied economics,financial innovation and demand for money: some empirical evidence,financial innovation and demand for money some empirical evidence,"Recent criticism of money growth targets has been based on the implications of spreading financial innovation, since the latter has been considered to undermine monetary policy effectiveness both by bringing about an increase in the interest elasticity of money demand and by producing instability of the money demand function. The empirical results presented in this paper – focusing on a single and specific case of financial innovation particularly suited to study the isssue at stake – falsify both hypotheses. Collapse"
106179,1990,applied economics,structural change in uk industry 1975-85,structural change in uk industry 1975 85,"1 New Challenges to the Automobile Production Systems in Europe.- 1.1 Introduction.- 1.2 The European Automobile Industry in Global Context.- 1.2.1 The global significance of the European automobile system.- 1.2.2 Europe's position in the global strategies of the major automobile producers.- 1.3 The Automobile Production Systems Approach.- 2 National and International Regulatory Frameworks: The Politics of European Automobile Production and Trade.- 2.1 Introduction.- 2.2 National Regulatory Environments in Europe.- 2.3 International Regulation: the European Community's Policies Towards Auto Production and Trade.- 2.4 Concluding Comments.- 3 ""Europeanisation"" in the Automotive Components Sector and Its Implications for State and Locality.- 3.1 Introduction.- 3.2 The Changing Geography of Automotive Components Production in the United Kingdom.- 3.3 ""Europeanisation"" and Corporate Strategies in the Automotive Component Sector.- 3.3.1 Bosch.- 3.3.2 Valeo.- 3.3.3 Four major UK companies: Lucas, GKN, T & N, BBA.- 3.3.4 Four major US companies: TRW, Allied-Signal, ITT and Tenneco.- 3.3.5 Japanese component companies and Europe.- 3.4 Concluding Comments.- 4 The Japanese, the European Market and the Automobile Industry in the United Kingdom.- 4.1 Introduction.- 4.2 Competition Between Automobile Companies in the United Kingdom.- 4.3 Cooperation Between Automobile Companies in the United Kingdom.- 4.4 Cooperation Between Component Suppliers and Automobile Producing Companies in the United Kingdom.- 4.5 Competition and Cooperation Between Component Companies in the United Kingdom and European Community.- 4.6 Capital: Labour Relations.- 4.7 State Regulation of Japanese Competition: European Community Trade Policies, National Interests and Corporate Interests.- 4.8 The Local and Regional Development Implications of Japanese Inward Investment: Just-in-Time and In One Place ?.- 4.9 Concluding Comments.- 5 The German Automobile Production System Going European.- 5.1 Introduction.- 5.2 Market Structures During the 1980s.- 5.3 Competitiveness via the Technological Competences of Automobile Producers.- 5.3.1 The emergent spatial pattern.- 5.3.2 Choices in product and process.- 5.3.3 Two joint strategies: Europeanisation and flexibilisation.- 5.3.4 The 1990s.- 5.4 Organisational and Spatial Restructuring of the Components Sector.- 5.5 Consequences for Labour.- 5.6 The Changing Geography of the German Automobile Production System During the 1990s.- 6 The Italian Automobile Industry and the Case of Fiat: One Country, One Company, One Market ?.- 6.1 Introduction.- 6.2 The Italian Automobile Industry: Some Structural Features.- 6.3 The Relationship Between the Automobile Industry and Government Economic Policies.- 6.3.1 Protectionist policies.- 6.3.2 Bail-out policies.- 6.3.3 Policies of territorial re-equilibrium.- 6.3.4 Other forms of state intervention.- 6.4 Spatial Strategies and Reorganisation Strategies.- 6.4.1 The approach: the automobile industry as a complex industrial system.- 6.4.2 From expansion to crisis.- 6.4.3 The first turning point.- 6.4.4 The 1980s: the great rationalisation.- 6.5 Continuity and Discontinuity in the Geography of the Italian Automobile System.- 6.6 Concluding Comments.- 7 Competitive Strategies in the World Market: The Case of Renault and the Emergence of a European Group ?.- 7.1 Introduction.- 7.2 Why Did European Automobile Companies Have to Make Strategic Changes ?.- 7.3 A Basic Answer: Improving Efficiency.- 7.3.1 Project structures.- 7.3.2 Development of quality.- 7.3.3 Automation, just-in-time manufacturing and supply.- 7.3.4 A new supply policy.- 7.3.5 Restructuring the European plants.- 7.3.6 National and international agreements.- 7.4 A Missed Opportunity: Renault's Failure to Grow Outside Europe.- 7.4.1 The missed deal in the USA.- 7.4.2 The weak positions in other countries.- 7.5 The Result: The Weaknesses of Renault in the 1990s.- 7.6 A European Answer: Rise and Fall of the Renault-Volvo Merger.- 7.6.1 1990-1993: the deepening of the alliance.- 7.6.2 September 1993: the move towards a merger.- 7.6.3 December 1993: the merger failure.- 7.7 Privatisation and After?.- 7.7.1 Towards a Renault-Fiat deal ?.- 7.7.2 Towards new international alliances within Europe.- 8 The Restructuring of the Swedish Automobile Production System.- 8.1 Introduction.- 8.2 The Swedish Automobile Production System - Some General Characteristics.- 8.3 Volvo and Saab: Corporate Structures and Strategies.- 8.3.1 Diversification.- 8.3.2 Internationalisation.- 8.3.3 Production organisation and location.- 8.4 1970 to 1987: From Crisis to Success.- 8.5 1988 to 1992: Renewed Crisis and Intensive Restructuring.- 8.5.1 Saab Automobile.- 8.5.2 Volvo.- 8.6 Post 1992: Future Prospects for Swedish Car Production.- 8.6.1 Lost national identities ?.- 8.6.2 Prospects for component suppliers.- 8.7 Concluding Comments.- 9 Multi-purpose Vehicles, a New Opportunity for the Periphery ? Lessons from the Ford\VW Project (Portugal).- 9.1 Introduction.- 9.2 Prospects for the Growth of Multi-purpose Vehicles: The Visible Hand of the Single Market.- 9.3 The Automobile Industry in Portugal.- 9.3.1 The formation of the industry.- 9.3.2 The current situation.- 9.4 The Ford Wolkswagen (AutoEuropa) Project.- 9.4.1 Background.- 9.4.2 A brief description of the project.- 9.4.3 Investment and financing.- 9.4.4 Production and markets.- 9.4.5 Employment.- 9.4.6 The process of setting up supplier-networks.- 9.4.7 The geography of direct suppliers: possible outlines of an emerging archipelago.- 9.5 Concluding Comments.- 10 Interdependent and Uneven Development in the Spatial Reorganisation of the Automobile Production Systems in Europe.- 10.1 Introduction.- 10.2 Challenges to the Core from Western and Southern Europe.- 10.3 Central and Eastern Europe - the New 'Frontier' of European Automobile Production.- 10.4 Resistance to the Erosion of the European Automobile Core.- 10.5 Towards a New Map of Automobile Production in Europe ?.- Author Index.- Location Index. Collapse"
106181,1990,applied economics,testing neoclassical consumer theory with aggregate and household data,testing neoclassical consumer theory with aggregate and household data,ERR
106182,1990,applied economics,money supply and prices in indonesia 1969-1980:  an econometric investigation,money supply and prices in indonesia 1969 1980   an econometric investigation,"The purpose of the study is to integrate the influence on prices which flow not only from monetary changes but also from other exogenous shocks (harvest failures, devaluations or changes in world market prices on key export commodities) with the influences of money supply caused not only by government policy changes but also from previous or expected price changes. The findings suggest that although monetary authorities have some control over the money supply changes can be exogenously determined. By contrast, it is also found that the rate of inflation (proxy for interest rate in a developing economy) which is generally considered to be an important element in the money demand function does not turn out to be a significant factor in recent Indonesian experience. The formulated model in this paper is dynamically simulated and seems to possess desirable properties namely positive prices and stable solutions. Collapse"
106184,1990,applied economics,testing alternative hypotheses in economic history,testing alternative hypotheses in economic history,"This paper employs a statistical technique comparing non-nested quantitative models in order to address an important problem in economic history, namely, the appropriate role of microeconomic models in historical analysis. Discussion proceeds by example. As described in the paper, three research groups have offered explantations of the shift from corn to cotton production in the post-bellum US South but because their research methods are different, it is difficult to evaluate them effectively. This paper suggests a method for comparing them. The method is based on the Neyman–Pearson likelihood ratio and proceeds by focusing on the three groups’ models of crop choice, in turn hypothesizing each model as ‘truth’, and testing all other against it. Though restrictive in its own right, this excercise suggests one data-oriented approach to all-too-common problem of model proliferation in economic history. Collapse"
106186,1990,applied economics,monetary targeting in the netherlands:  an application of co-integration tests,monetary targeting in the netherlands   an application of co integration tests,No Result.
106187,1990,applied economics,real wage determination in bangladesh agriculture:  an econometric investigation,real wage determination in bangladesh agriculture an econometric investigation,Thsi paper develops and estimates a real wage model for the agricultural sector in Bangladesh for the period 1973:2–1985:4. The model is developed within the framework of the market theory of labour demand and labour supply. The empirical results are supportive of the market theory of wages. Collapse
106188,1990,applied economics,hospital care inflation in the united states:  price adjustments in a disequilibrium econometric framework,hospital care inflation in the united states price adjustments in a disequilibrium econometric framework,"Uncontrollable medical care expenditure inflation in excess of general price inflation in the United States has prompted the writing of this paper. It is argued that such a frustrating phenomenon is due to the existence of disequilibrium in the hospital market. This phenomenon can be verified only by modelling the market by disequilibrium methods. In this paper, a model of price adjustment in non-clearing market is presented and tested by using autoregressive techniques. It is found that excess demand for hospital care has been the source of disequilibrium for a large part of the period of the study, 1965–1984. It is observed then the key factor causing excess demand is low out-of-pocket expenditure for hospital care by patients due to third-party payments. Had the co-insurance rate been adjusted upward or co-payment patient rate not fallen, excess demand would have been eliminated. This study also finds that the response of providers and regulators to cost inflation through price control policy seems to ... Collapse"
106189,1990,applied economics,testing for stationarity and co-integration:  an application to saudi-arabian monetary data,testing for stationarity and co integration   an application to saudi arabian monetary data,ERR
106191,1990,applied economics,hospital efficiency and indigent care,hospital efficiency and indigent care,"the Government White Paper I agree with the editorial by Bass & Murphy (April 1990 JRSM, p 203) in calling for research on somatization before the Government's imposition of the White Paper compounds the problems of coping with chronic somatizers. However, I would like to comment on two challenges to research in this area. Firstly, abnormal illness behaviour covers a broad spectrum of doctor-patient relationships from malingerers who concoct symptoms for their own benefit, to patients who have no conscious control over their functional symptoms and gain no obvious advantage from them. These polar groups illustrate the complexities of attempting to delineate and understand this behaviour. Whether somatization disorder is a useful step in this direction is debatable. We found the arbitrary diagnostic criteria! excluded individuals with more severe abnormal illness behaviour than the somatization disorder patients, in terms of frequent GP consultations, multiple hospital referrals, and being poor copers with life in general. We concluded that the concept of somatization disorder had little value in British primary care'', Secondly, somatization disorder does not delineate a group of patients who always present with symptoms in the absence of organic disease. Consultation rates for organic and non-organic problems are highly correlated"" raising the possibility that these patients often have disease as well as illness, albeit with a lower threshold ofsymptom tolerance before attending the GP. For example, one of the patients I investigated Collapse"
106192,1990,applied economics,"price rigidity, inflation and market concentration:  some canadian evidence from the 1970s",price rigidity inflation and market concentration some canadian evidence from the 1970s,"In the literature on industrial pricing there is a long-standing dispute over the theoretical rationale for, and the empirical validity of, price rigidity. Briefly, the arguments is that the speed of price adjustment is a function of market structure: in competitive markets prices are flexible (they adjust rapidly to supply and demand shocks), but in oligopoly they can be relatively rigid (they adjust only slowly, if it all, to these same shocks). 1 But, because the theoretical basis fot the rigidity hypothesis is decidedly ad hoc, it is often summarily dismissed. 2 Empirically, however, a number of studies for various countries and time period have found relationship between concentration (a proxy for oligopolistic market structure) and price change and/or some other measure of the speed of price adjustment, compatible with price rigidity. 3 Although these result are not totally unambiguous, 4 the Canadian evidence, at least prior to the 1970s, strongly supports the rigidity hypothesis (Sellekaerts and L... Collapse"
106193,1990,applied economics,the relationship between unions and job satisfaction,the relationship between unions and job satisfaction,"Members of trade unions express greater dissatisfaction with the conditions of their jobs than nonmembers. Alternative explanations of this are examined. It is argued that Richard B. Freeman and James L. Medoff's (1984) ""voice"" model of trade unionism does not provide a satisfactory account of the dissatisfaction expressed by unionists in the Australian youth labor market. Evidence is presented to suggest that the negative relationship between job satisfaction and unionism may be attributable to unpleasant work environments, which both induce dissatisfaction and motivate workers to join unions. Copyright 1990 by Blackwell Publishers Ltd/University of Adelaide and Flinders University of South Australia Collapse"
106194,1990,applied economics,logistic regression and probability of default of developing countries debt,logistic regression and probability of default of developing countries debt,"This paper presents a logistic regression model for determining the default probability of developing countries debt. The study incorporates 79 countries over a period of 19 years. The model predicted the default of Mexico, Brazil, and Argentina two years in advance. Results indicate that the model has high predictive power. Collapse"
106196,1990,applied economics,application of normal income to metropolitan housing demand:  an econometric analysis,application of normal income to metropolitan housing demand an econometric analysis,ERR
106197,1990,applied economics,effect of housing stock inflation on homeowner expenditure patterns,effect of housing stock inflation on homeowner expenditure patterns,ERR
106198,1990,applied economics,automobile safety inspection:  further econometric evidence,automobile safety inspection further econometric evidence,"Studies of traffic safety typically assumethat policies are adopted to further thepublic interest, thereby ignoring thepolitical motives for policy. Sincepolitical motives can influence the designor enforcement of policies, accounting forpolitical motives has relevance forevaluating policy effectiveness. Weexamine the political motives concerning afrequently-studied traffic safety policy: state-mandated vehicle safety inspection. We distinguish between public interest andspecial interest explanations for safetyinspection. Our econometric models examinethe incidence of inspection across states,and determinants of regulated inspectionfees. The evidence strongly rejects apublic interest explanation, but specialinterest hypotheses also do not proveentirely satisfactory. Since recentstudies find that inspections fail toimprove highway safety, we attribute thecontinued existence of inspection programsto political transaction costs rather thanto the demands of interest groups. Collapse"
106199,1990,applied economics,how effective is automobile safety regulation?,how effective is automobile safety regulation,"Loeb(1989) questions the results of recent work on automobile safety inspection (Garbacz and Kelly, 1987) in which the results of a paper by Leob and Gilad (1984) were questioned. Specifically, their time series model, using New Jersey data, is missing a key economic variable (accident price) as well as some well-known variables associated with traffic accidents (yothful drivers and a proxy for drunk driving). An estimate of their model, with US data, results in no statistical effect for automobile inspection variables (Garbacz and Kelly, 1987). Collapse"
106202,1990,applied economics,"causality between money, interest rates and prices in taiwan:  a multivariate time-series analysis",causality between money  interest rates and prices in taiwan   a multivariate time series analysis,ERR
106203,1990,applied economics,are monetary services indexes superior to conventional monetary aggregates as intermediate targets?,are monetary services indexes superior to conventional monetary aggregates as intermediate targets,ERR
106204,1990,economica,choosing sides in matching games:  nash equilibria and comparative statics,choosing sides in matching games nash equilibria and comparative statics,"This paper characterizes equilibria in matching games where the institutional setting allows players to choose whether to become buyers, sellers, or not to participate in economic activity. Nash equilibrium consists of a partition of agents into buyers, sellers, and non-participants such that agents in each group do not have an incentive to change their activity given the behaviour of other agents. Comparative-static analysis demonstrates that, when participation costs are so high that some agents choose not to participate, a further increase in participation costs may increase the number of agents who participate in the matching process. Collapse"
106206,1990,economica,the interdependence between ownership status and market structure:  the case of privatization,the interdependence between ownership status and market structure the case of privatization,"One of the common justifications for privatization policy is the assertion that private firms are more efficient and thus potentially more profitable. The analysis of privatization policy, however, cannot be complete without specific attention to the market structure in which the firm operates and in particular to the interdependence between the ownership status and market structure. I examine a dupolistic market and demonstrate that a partly nationalized firm might realize higher profits than its private, profit-maximizing, competitor. The degree of nationalization also affects the interaction between an incumbent firm and a potential entrant. Using A. K. Dixit's framework, I consider the implications of privatization on the attractiveness of entry, the possibility of deterring entry, and the incumbent position as a natural monopoly. It is shown that it is possible that a firm is not a natural monopoly while being private but that alteration of its ownership status can transform it to a natural monopoly. Such an analysis establishes my main claim that ownership status may affect the market structure in which firms operate. Collapse"
106207,1990,economica,"real and money wages, output and inflation in the semi-industrialized world",real and money wages  output and inflation in the semi industrialized world,"Recent structuralist ideas about real and money wage changes are reviewed, addressing six questions: (1) In an economy closed to trade, do real wage increases cause output to rise or fall? (2) If output is at an upper bound, what are the roles of real wage and wealth movements in macro adjustment? (3) Taking effective demand into account, do money wage cuts stimulate employment? (4) In an open economy, does money wage reduction or devaluation make output rise? (5) At an output bound, what are the respective roles of wage reduction and monetary restraint in slowing inflation? (6) If inflation is driven by cost with output below capacity, how do wage dynamics affect stabilization programmes based on de-indexation and price freezes? Institutional factors suggest that wage-cutting may be a counterproductive answer to all these policy questions in semi-industrialized economies. Collapse"
106208,1990,economica,the flexibility to switch between different products,the flexibility to switch between different products,"Purpose – The main goal of this study is to demonstrate the value of flexibility within sugar and ethanol production. We use real data for the Southeast and Northeast regions of Brazil, which are subject to different taxes rates. Design/methodology/approach – The framework of this study is based on Real Options valuation. We valued the option to switch between sugar and ethanol production. The dynamics of prices was based on the recombinant trees of Nelson and Ramaswamy (1990) and on the bivariate trees of Hahn e Dyer (2011). Empirical data is from the Center of Advanced and Applied Economic Studies (CEPEA), ESALQ-USP, from May/2003 through July/2014. Prices were deflated using the Brazilian Price Index (IGP-DI), available at the Ipeadata site based on July/2014. Findings – The results show that, in both regions, flexible systems have greater value than those that produce only one product. Southeast plants exhibit greater value added if compared with Northeast plants, not only because of lower taxes, but also due to higher productivity. Originality/value – This paper quantifies value added by the flexible system in the production of sugar and ethanol in two Brazilian regions subject to different taxes. Moreover, it demonstrates that regulators can use policies and incentives to establish flexible plants. In this way, both producers and fiscal authorities will achieve greater gains. Collapse"
106209,1990,economica,incidence analysis of financing unemployment benefits in a partially unionized economy,incidence analysis of financing unemployment benefits in a partially unionized economy,"The incidence of unemployment benefits are analysed under alternative financing modes in a two-sector general equilibrium model with a segmented labour market where one sector is unionized and queue unemployment exists. Specific tax incidence as well as balanced budget incidence are analysed. The main results are: (i) The distributional effects are identical irrespective of which factor is taxed in the union sector, (ii) the distributional effects are also identical irrespective of which factor is taxed in the non-union sector, and (iii) labour gains by a uniform UI tax on capital in both sectors and may or may not gain from a uniform labour tax, while capital owners lose in both cases. Thus, what matters for the incidence outcome is which sector, and not which factor, that is taxed. Collapse"
106210,1990,economica,terms-of-trade shock and the current account in a monetary economy,terms of trade shock and the current account in a monetary economy,"Over the period 1973-81, the terms of trade of non-oil developing countries deteriorated at about 2 per cent per annum. This was accompanied by an increase in current account deficits in excess of 3 per cent above what they had been in the previous decade. (See Khan and Knight, 1983, for a discussion; these authors find the terms-of-trade shocks the major cause of the deficits.) This has led to a renewal of interest in the effects of changes in terms of trade for a small open economy. The conventional wisdom, first put forward by Laursen and Metzler (1950) and Harberger (1950), emphasized the fact that a terms-of-trade worsening lowered real income, which in turn lowered savings and worsened the current account-the Laursen-Metzler Effect. The recent reappraisal was initiated by Obstfeld (1982). He showed, in an infinite-horizon-optimizing framework with perfect capital mobilizing, that if the discount rate was a function of the level of utility, surpluses, and not deficits, accompany an unanticipated and permanent terms-of-trade worsening. The long-run level of the discount rate is tied down by the given world rate of interest. This in turn fixes the long-run level of utility. The terms-of-trade shock tends to lower the level of utility and agents save to offset this. Across steady states there is only a substitution effect. On the other hand, if the discount rate is fixed and equal to the world interest rate, then the economy adjusts to the terms-of-trade shock by immediately lowering real expenditure by the same amount as the decline in real income so that the current account is always balanced. Svensson and Razin (1983) showed that in a two-period framework this does not necessarily happen. For the infinite-horizon case they showed that, if preferences were separable over time and the discount rate was increasing in instantaneous utility, then Obstfeld's results were indeed obtained. In a two-period overlapping-generations model, the results could be modified owing to the fact that human wealth and non-human wealth are not perfect substitutes and a terms-of-trade shock affects these differently. (See Matsuyama, 1988, for a discussion.) But it is still true that an unanticipated permanent terms-of-trade shock has a one-shot effect on the current account unless some ad hoc costs of adjustment are introduced (see e.g. Persson and Svensson, 1985, and Matsuyama, 1988). The uncertain lifetime model (see e.g. Frenkel and Razin, 1988, and Matsuyama, 1987) generates non-trivial Collapse"
106211,1990,economica,"the interaction between indexation, contract duration and non-contingent wage adjustment",the interaction between indexation  contract duration and non contingent wage adjustment,"A model of the elasticity of indexation, contract duration and non-contingent wage adjustment is estimated using Canadian wage agreements. Results suggest that the degree of indexation and contract duration are interdependent and are affected by their past values, labour market conditions and inflation uncertainty. The correlation between the industry product prices and the CPI affects the degree of indexation. Non-contingent wage adjustment depends.on expected inflation, uncompensated past inflation, duration and indexation values, past and present, and labour market conditions. This model explains total wage adjustment as well as more aggregative models designed solely for that purpose. Collapse"
106281,1991,accounting review,the impact of sec mandated segment data on price variability and divergence of beliefs,the impact of sec mandated segment data on price variability and divergence of beliefs,No Result.
106282,1991,accounting review,the market reaction to 10-k and 10-q filings and to subsequent  the wall street journal earnings announcements,the market reaction to 10 k and 10 q filings and to subsequent  the wall street journal earnings announcements,ERR
106283,1991,accounting review,supplemental data and the structure of thrift share prices,supplemental data and the structure of thrift share prices,No Result.
106284,1991,accounting review,a perspective on accounting and stock prices,a perspective on accounting and stock prices,"companies’ dynamism and success depend upon assessing, counter posing, and identifying the pros and cons of their performance and it would be useful for managers, policy makers and stock market Investors as well as investors to compare companies operating in different domains of industries, yet the critical issue is to set appropriate criteria, indicators and models to do the rankings. Since there are a variety of companies-ranking criteria in the stock market which affect and take effect, it is essential to make use of multi-indexes decision models to set priorities. Environment uncertainty is an important factor challenging decision-making which could severely confuse investors. No integrative, inclusive conceptual algorithm has so far been introduces to lessen this uncertainty. Taking this approach, In this paper try to apply the integrative approach as well as fuzzy theories and independent multi-indexes technique in uncertain conditions (known as TODIM) to identify uncertainty present in assessing criteria that influence price index of the stock market. The TODIM approach has been developed to support MCDA under uncertainty. This research is the first study in the ranking the factors of stock market. At the end. An illustrative example is also presented to show the efficiency of our model. Keyword: : multi-indexes decisions, fuzzy theories, TODIM, stocks Introduction Nowadays, stocks, as a crucial tool in the capital market, play a significant role in the increased economy and lay the groundwork for economic prosperity through price setting, reducing risk, providing resources as well as optimal assets allocation. An index is a means to measure and compare phenomena that possess certain qualities and properties (pakdin, 2008). The fiscal part of each country’s economy is considered to supply financial resources needed for the expansion of true economic transactions generally including dollars and financial markets. The state of price index in the stock market is an important criterion to evaluate financial markets of countries which has been used as an important way in order to examine the stock market internal and external feedbacks and as a base for decisions made by investors. The application of price index of shares is extensive and important for both investors to put money into private jointstock sector and as economic index from macroeconomics’ perspective, for shares of companies and manufacturing and economic sectors are exchanged in the stock market. For that matter and recognizing the importance macroeconomics policies and variables as well which can have a considerable effect in the capital market, the present paper, for the first time, attempts to prioritize financial factors influencing the stock market price index using fuzzy TODIM technique. The fuzzy property used here considerably increases managers and decision makers’ capability of decision making in assessing each criterion so that opinions could get closer to reality. Strong mathematic foundation of this technique, on the other hand, as opposed to other methods in the area of MCDM has considerably increased its capability to support outcomes in validity and reliability. Next we review the literature. Then fuzzy TODIM method is introduced and last section will consider a case example, pointing out its numerical results. Literature review A lot of attention has been paid to factors influencing the stock market price index to identify and measure types of factors and their influence in today’s complex, fast-changing environment. Therefore, making reference to the relevant theories, we present an integrative, exhaustive model. Investigating the short and long term relations between inflation rate and true nominal price index in the U.S. in a 132 month time period (1991-2003), Kim and In (2005) found the shortterm relationship between the two variable is negative but the long-term relationship is the opposite. They concluded the relationship between inflation and price index is varying in time and depends on the time of investigation (kim and in, 2005). Gan et al (2006) examined the relationship between New Zeeland’s stock market index with seven major economic variables through 1990-2003, indicating that there is a significant relationship between the stock market index and the independent variables during the term (Gan et al, 2006). Based on Bhanot (2006)’s theory, we take account of political factors as the criteria being considered. According to him, government’s interventions impact on the stock market index, and unusual outcomes during the period would depend more on general activities from the government than its specific interventions. This is more n consistent with the impact of information than that of price pressure. Public ownership maintenance positively influences the stock market price performance (Bhanot, 2006). We adopted structural factors from Boardman and Claude’s (2000) theory suggesting that firm’s size has a negative impact on the stock market price performance (Boardman and Claude’s, 2000). Amiri (2009), in his theory, argues that managerial factors and marketing can influence shares price. In other words, he explains that managerial factors play a role in the company’s success or failure and, consequently, in shares price (Amiri, 2009). In their theory, Kevin and Vinod (2001) suggest Total International Journal of Scientific Management and Development ISSN:2345-3974 Vol.3 (6), 414-423 June (2015) 415 International Journal of Scientific Management and Development Quality Management (TQM) has effect on shares price and that TQM could bring wealth to the organization. Based on all said above, marketing and managerial factors can be adopted from the extant theories (Kevin and Vinod, 2001). Bartram (2007) considered how shares price as opposed to currency and inflation rates was affected, demonstrating amongst all economic variables, market interest rate and currency rate has the main effect on shares price index (Bartram, 2007). Amiri (2009) came to conclusion that disclosure of financial inventories, as a financial factor, influences shares price index (Amiri,2009). In his research, Jamshidi (1988) concludes the ratio of price on income impacts on shares price index. Examining the effect of mental factors from past variation in shares price on tendency to shares purchasing and price index, Huddart et al (2006) confirmed such effect exists. Scholars have found that the extent of transactions matters more, both in terms of statistics and economy, when the current prices exceed the prices of last year. The past highest price acts as a reference for investors to decide whether or not to purchase shares (Huddart et al, 2006). We have concluded that past variation in shares influence investors’ decision as to whether to purchase shares. Investigating effect of various factors on shares prices, Boardman and Claude (2000) confirmed such an effect and identified such factors as government, gold share, type of industry, and time periods (Boardman and Claude, 2000). Examining the relationship between a company’s shares price and the average price of other companies’ shares, Heins and Allison (1996) concluded that changes in a company’s shares price is contingent on numerous factors rather than the average stock of shares in the market which is not considered here. Considering the relationship between a company’s shares price with the average price of other firms’ shares, they found that the average price of other firms’ shares has no effect on variations in a company’s shares price and that its shares price is dependent on other variables and factors and is independent of the average price of other companies’ shares (Heins and Allison, 1996). In their study, Kalev, Liu and Pham (2002) found an increase in the extent of transaction, price variations, and news coming to the market in the beginning and ending of each day’s transaction suggest positive inter-relationships between these variables (Kalev, liu and pham, 2002). Based on the theory of effective markets, Ivsia (2001), of MIT, examined the financial behaviors of shareholders in the U.S. NASDAQ stock market in relation to the ways in which they make decisions over the shares of high tech companies; results showed that shareholders’ decisions over such shares aren’t in accord with the extant theories of financial management, and in spite of low gains and low EPS of such companies, their shares are increasingly on demand, because of shareholders’ expectations of bright future of these companies (Ivais, 2001). Epstein (1994) points to firms’ annual fiscal reports as invaluable and insignificant factor on investors’ decisions. The fuzzy TODIM Combining fuzzy set theory and TODIM is a novel approach. TODIM is a Crete multi criteria method founded on prospect theory (Kahraman et al, 2003) The TODIM method has been successfully used and empirically validated in different applications (Gomez et al,1992; Tzeng et al,2002). This method is based on a description, proved empirically, of how people make effective decisions when faced with risk. Although not all multi criteria problems address risk, the shape of the value function of TODIM is identical to prospect theory’s gain and loss function TODIM’s multi criteria value function is composed in parts whose mathematical descriptions reproduce the gain and loss function The global multi criteria value function of TODIM then aggregates all measures of gains and losses by considering all criteria. Introducing expressions of losses and gains in the multi criteria function is incorporated into the TODIM formulation. Gomes et al (2009) apply TODIM to investigate and recommend options for upstream projects for the natural gas reserves recently discovered in the Mexilhão field in the Santos Basin, Brazil. In addition Gomes and Rangel (Gomez et al, 2009) presented an evaluation of residential properties with real estate agents in Brazil and def Collapse"
106285,1991,accounting review,"budgetary participation, locus of control, and mexican managerial performance and job satisfaction",budgetary participation locus of control and mexican managerial performance and job satisfaction,"This study examines the effects of budgetary participation, and the personality variable, locus of control, on the performance and job satisfaction of Mexican managers working for US controlled maquiladoras on the US/Mexican border and within interior Mexico. This study follows the methodology employed by Frucot and Shearon (1991), finds similar empirical results, but reaches quite different interpretations and conclusions. While Frucot and Shearon interpreted their results as suggesting caution in the use of participative budgeting by US companies operating in Mexico, we find that Mexican managers working for US controlled maquiladoras in Mexico may exhibit cultural values much like their US counterparts and that the performance of these Mexican managers may benefit from budgetary participation. Collapse"
106286,1991,accounting review,configural information processing in auditing:  the role of domain-specific knowledge,configural information processing in auditing   the role of domain specific knowledge,"SYNOPSIS AND INTRODUCTION: Recent audit studies by Brown and Solomon (1990, 1991) reveal that careful consideration of domain-specific knowledge can result in the experimental detection of configural relationships between information cues. These findings suggest that auditors' decisions may be more complex than indicated by some previous research, and that additional research is necessary to identify conditions in which auditors utilize configural processes. This study investigates the role of environmental risk factors on the configurality of audit decisions. That is, we test whether the systematic consideration of audit risk variables results in the identification of higher order, interactive decision processes where linear relationships have previously been detected. The findings of Libby et al. (1985) and existing audit pronouncements are used to develop hypotheses regarding auditors' internal audit decisions.1 The hypotheses concern (1) the interactive effect of inherent risk and control strength on the extent to which auditors rely on internal audit functions to reduce planned audit work and (2) the extent to which these environmental factors affect consideration of three components of internal audit quality: objectivity, competence, and work performed. Audit managers from a Big Six accounting firm responded to a series of audit-planning cases concerning the receivables cycle of a mediumsized manufacturing firm. Inherent risk and strength of control architecture Collapse"
106287,1991,accounting review,information acquisition and resource allocation decisions,information acquisition and resource allocation decisions,"Chapter 1 -- Learning: Introduction, Issues, Historical Perspectives Learning Defined Learning Theory and Research Functions of theory Conducting research Methods of Assessing Learning Direct observations Written responses Oral responses Ratings by others Self-reports Precursors of Modern Learning Theories Learning theory and philosophy Beginnings of the psychological study of learning Structuralism and functionalism Critical Issues in the Study of Learning Which processes affect learning? What is the role of memory? What is the role of motivation? How does transfer occur? Which processes are involved in self-regulation? What are the implications for instruction? Relation of Learning and Instruction Historical perspective Instructional commonalities Integration of theory and practice Three Learning Scenarios Kathy Stone's third-grade class Jim Marshall's U. S. History class Gina Brown's educational psychology class Summary Further Reading Chapter 2 -- Conditioning Theories Connectionism Trial-and-error learning Laws of exercise and effect Other principles Revisions to Thorndike's theory Instructional applications Classical Conditioning Basic processes Informational variables Biological influences Conditioned emotional reactions Watson's Behaviorism Basic processes Little Albert experiment Contiguous Conditioning Acts and movements Associative strength Rewards and punishments Habit formation and change Operant Conditioning Conceptual framework Basic processes Behavioral change Behavior modification Verbal behavior Self-regulation Self-monitoring Self-instruction Self-reinforcement Instructional Applications Behavioral objectives Programmed instruction Contingency contracts Keller Plan Summary Further Reading Chapter 3 -- Social Cognitive Theory Conceptual Framework for Learning Reciprocal interactions Enactive and vicarious learning Learning and performance Modeling Processes Theories of imitation Functions of modeling Cognitive skill learning Rule learning Influences on Learning and Performance Developmental status of learners Model prestige and competence Vicarious consequences to models Goals and Expectations Goals Outcome expectations Self-Efficacy Conceptual overview Self-efficacy in achievement situations Models and self-efficacy Motor skills Instructional self-efficacy Health and therapeutic activities Self-Regulation Conceptual framework Social cognitive processes Cyclical nature of self-regulation Instructional Applications Models Self-efficacy Self-regulation Summary Further Reading Chapter 4 -- Cognitive Information Processing Theory Cognitive Information Processing System Assumptions Two-store (dual-memory) model Critique Levels of processing Activation level Attention Theories of attention Attention and learning Attention and reading Perception Gestalt theory Sensory registers Long term memory comparisons Two-Store Memory Model Verbal learning Short-term (working) memory Long-term memory Influences on encoding Long-Term Memory: Storage Propositions Storage of declarative knowledge Storage of procedural knowledge Production Systems and Connectionist Models Production systems Connectionist models Long-Term Memory: Retrieval Retrieval strategies Encoding specificity Retrieval of declarative knowledge Retrieval of procedural knowledge Long-Term Memory: Forgetting Verbal learning Information processing Mental Imagery Representation of spatial information Imagery in LTM Individual differences Summary Further Reading Chapter 5 -- Cognitive Learning Processes Conditional Knowledge and Metacognition Conditional knowledge Metacognition and learning Variables influencing metacognition Metacognition and behavior Concept Learning The nature of concepts Concept attainment Teaching of concepts Motivational processes Problem Solving Problem solving defined Historical influences Heuristics Information processing model Problem-solving strategies Problem solving and learning Experts and novices Implications for instruction Transfer Historical views Activation of knowledge in memory Types of transfer Strategy transfer Instructional applications Self-Regulation and Motivation Model of self-regulation Learning strategies Critique of strategy instruction Academic studying Summary Further Reading Chapter 6 -- Constructivist Theory Constructivist Assumptions and Perspectives Overview Perspectives Situated cognition Contributions and applications Vygotsky's Sociocultural Theory Background Basic principles Zone of Proximal Development Applications Critique Social Processes and Private Speech Private speech Verbalization and achievement Socially mediated learning Motivation Contextual factors Implicit theories Teachers' expectations Self-Regulation Sociocultural influences Implicit theories of self-regulation Constructivist Learning Environments Key features APA Learner-Centered Principles Instructional methods Reflective teaching Summary Further Reading Chapter 7 -- Cognition and Instruction Discovery Learning The process of discovery Teaching for discovery Meaningful Reception Learning Meaningfulness and expository teaching Advance organizers Conditions of Learning Learning outcomes Learning events Learning hierarchies Phases of learning Models of Instruction Learning time Mastery learning Inquiry teaching Instruction with worked examples Cognitive load Peer-assisted learning Research on Teaching Teacher planning and decision making Instructional practices Learner Characteristics Aptitude-treatment interactions Cognitive styles Learners' resource allocations Adapting instruction Technology and Instruction Functions of technology Technological applications Future directions Summary Further Reading Chapter 8 -- Development and Learning Beginnings of the Scientific Study of Development Historical foundations Philosophical foundations The Child Study Movement Perspectives on Human Development Issues relevant to learning Types of developmental theories Structural theories Piaget's Theory of Cognitive Development Developmental processes Implications for instruction Bruner's Theory of Cognitive Growth Cognitive growth and knowledge representation Spiral curriculum Contemporary Themes in Development and Learning Developmental changes Developmentally appropriate instruction Transitions in schooling Familial Influences SES Home environment Parental involvement Motivation and Development Developmental changes Implications Development and Instruction Case's model of instruction Teacher-student interactions Summary Further Reading Chapter 9 -- Neuroscience of Learning Organization and Structures Neural organization Brain structures Localization and interconnections Brain research methods Neurophysiology of Learning Information processing system Memory networks Language learning Brain Development Influential factors Phases of development Critical periods Language development Motivation and Emotions Motivation Emotions Implications for Teaching and Learning Relevance of brain research Educational issues Brain-based educational practices Summary Further Reading Chapter 10 -- Content-Area Learning Skill Acquisition General and specific skills Novice-to-Expert Research Methodology Language Comprehension Components of comprehension Parsing Utilization Reading Decoding Comprehension Writing Composition Processes Reviewing Processes Motivation and self-regulation Mathematics Computation skills Problem-solving skills Constructivism Individual differences Science Expert-novice differences Reasoning Constructivism and scientific beliefs Social Studies History Geography Summary Further Reading Chapter 11 -- Motivation Model of Motivated Learning Pretask During task Posttask Historical Perspectives Drive theory Conditioning theory Cognitive consistency theory Humanistic theory Achievement Motivation Expectancy-value theory Familial influences Fear of success Contemporary model of achievement motivation Self-worth theory Task and ego involvement Achievement motivation training Attribution Theory Locus of control Naive analysis of action Attribution theory of achievement Attribution change programs Social Cognitive Theory Goals and expectations Social comparison Goal Theory Goal orientations Conceptions of ability Implications for teaching Perceptions of Control Control beliefs Learned helplessness Students with learning problems Self-Concept Dimensions and development Self-concept and learning Intrinsic motivation Theoretical perspectives Overjustification and reward Motivation and Self-Regulation Volition Values Self-schemas Help seeking Summary Further Reading Chapter 12 -- Next Steps Learning Questions Which processes affect learning? What is the role of memory? What is the role of motivation? How does transfer occur? Which processes are involved in self-regulation? What are the implications for instruction Learning Theories Conditioning Social cognitive Cognitive information processing Constructivist Conclusion Glossary References Author Index Subject Index Collapse"
106289,1991,accounting review,the relation between firm size and effective tax rates:  a test of firms' political success,the relation between firm size and effective tax rates a test of firms political success,No Result.
106290,1991,accounting review,tradeoffs in the choice between logit and ols for accounting choice studies,tradeoffs in the choice between logit and ols for accounting choice studies,No Result.
106324,1991,economic development and cultural change,"trends in the terms of trade of primary commodities, 1900-1982:  the controversy and its origins",trends in the terms of trade of primary commodities  1900 1982   the controversy and its origins,"Primary commodity markets have offered an opportunity for much economic debate on a number of key issues. First, secularly declining terms of trade of primary commodities are used to explain the widening gap between developing (LDCs) and developed countries (DCs). Second, they are regarded as a contributing factor to the creation of a structural condition of unequal exchange between a ""dominating"" industrial center and a ""dominated"" agrarian periphery. And third, they are thought to be a source of instability, often causing financial vulnerability to foreign exchange equilibria and fragility to international market institutions. Collapse"
106325,1991,economic development and cultural change,what happened to political development?,what happened to political development,The subject matter of economic development and political development intersects over a broad front. Economic policy is made by incumbent politicians in the context of political institutions. The analysis of the economic impact of alternative policies is the stock in trade of the economist. The choice of the alternative policies that are subjected to economic analysis is influenced by the agendas of political parties and interests. The subject matter of political science includes the political decision process by which policies are adopted and implemented. It also includes the social consequences and the public response to policy. There is a deep fault line that divides scholarship in the two fields. Each field tends to treat the knowledge it draws on from the other as implicit rather than explicit. It seems apparent that the implicit theorizing by economists about political development and of political scientists about economic development should be replaced by more specific attempts to develop an integrated theory of political and economic development. Political scientists and economists loosely grouped within the collective choice school of political economy have advanced our understanding of the proceses by which economic resources are translated into political resources and political resources are translated into economic resources.' But a similar convergence of theory and analysis has not yet been achieved among students of political and economic development. This article represents an attempt to assess what development economists could learn from theory and research in the field of political development to advance knowledge and policy in the field of economic development. I proceed by first reviewing the contributions of several development economists who have attempted to give explicit attention to the political preconditions or conditions for economic development. I then review the evolution of thought in the field of political development. This leads me to a discussion of the central problem Collapse
106327,1991,economic development and cultural change,"formal-sector retail trade in the urban third world:  conceptual issues and the case of nasik city, india",formal sector retail trade in the urban third world   conceptual issues and the case of nasik city  india,"World was little understood. That is not true anymore. At least since Clifford Geertz's seminal discussion of the bazaar and firm types of economies in a Javanese town, many studies have appeared that have focused on informal retail trade.' In contrast to this, the formal retail sector has received less detailed and systematic attention.2 This neglect is unfortunate. It is not known whether the formal trade sector in the urban Third World is growing at the expense of the informal one, but there is little doubt that in most societies it is expanding in absolute terms. This development needs to be studied because it is especially through this sector that innovations are introduced that cause a transformation in the character of Third World urban trade, consumption patterns, and culture from traditional forms to ones attuned to a mass market. To offset this neglect, in this article I will clarify certain ambiguities concerning the concept of formal retail trade and describe the formal retail sector of a secondary city in India in order to suggest the degree to which it has introduced modern/Western themes into the local trade community. Collapse"
106328,1991,economic development and cultural change,the transition from high to low marital fertility:  cultural or socioeconomic determinants?,the transition from high to low marital fertility cultural or socioeconomic determinants,The authors test whether socioeconomic variables in England and Wales between 1850 and 1900 are significantly related to decline in fertility during a transitional period marked by no family planning at the onset. In contrast Carlsson study of Swedish fertility decline ascribes the decline to the diffusion approach from urban to rural and adjustment perspective which emphasizes societal setting and boundaries. Knodel and van de Walles macro analysis within the Princeton European Fertility Project (PEFP) presents the view that simultaneity occurs despite a variety of socioeconomic and demographic conditions. Lesthaeghe followed the innovative argument and reports that within the diffusion process cultural barriers put limitations on the extend of fertility. Conversely socioeconomic interpretations have been developed by Teitelbaum who also used the PEFP data and Friedlander in his analysis of 600 districts of England and Wales. Teitelbaum identifies the significance as socioeconomic with cultural variables having separate significant effects at later stages. Friedlanders somewhat different theory indicates that nuptiality marital fertility and migration are micro-interrelated and result from socioeconomic change. Crafts analysis of the 1911 fertility census in England and Wales reveals that spacing and stopping contributed to fertility decline instead of Knodel and van de Walles thesis of stopping being the only significant determinant. The authors analysis of 600 districts of England and Wales validates the adjustment hypothesis by showing that socioeconomic and demographic timing account for variation in timing of marital fertility (R2 = .50). In explaining the diffusion process again with R2 = .54 socioeconomic variables entirely account for the fertility change dependent variable with no effect from the cultural variable. This was substantiated in repeated trials at regional levels also testing the strength of the language variable. Further testing of the mode of transmission from urban to rural indicated the fertility change variable was not negatively related to the urban proximity variable including a separate regression for just London and environs. The Carlsson diffusion hypothesis is rejected. 24 repeated stepwise regressions were performed confirming the importance of the socioeconomic and demographic variables in explaning fertility decline with a minimum of 50% (range 70 - 90%) of the variation explained. Cultural and diffusion variables explain no more than 50% (range 5 - 25%). The largest beta coefficients for all equations with demographic and socioeconomic variables. The Lesthaeghe border hypothesis is also not substantiated. The Framework of Social Structure and Fertility is advanced as the significant analytical construct for understanding fertility decline. Notwithstanding the English experience other recent studies are reviewed which support the socioeconomic interpretation and suggestion is made to further research effort in this direction. Collapse
106329,1991,economic development and cultural change,son preference and contraception in egypt,son preference and contraception in egypt,No Result.
106330,1991,economic development and cultural change,determinants of the nutrition and health status of preschool children:  an analysis with longitudinal data,determinants of the nutrition and health status of preschool children an analysis with longitudinal data,"Preschool children are usually considered one of the groups at greatest nutritional risk.' Malnutrition affects the rate of morbidity and mortality among the young and also poses a threat to their physical and mental development. Preschool children account for a disproportionately large share of the deaths in most developing countries. Nutritional deprivation is either directly or indirectly associated with most of those deaths. The very young are less able to cope physiologically with nutritional deficiencies than older children and adults. In addition, children who suffer a loss of growth due to early nutritional deprivation have only a limited capacity to overcome the resulting stunting. For these reasons, there is particular interest in the determinants of the nutrition and health status of preschool children in developing countries.2 Collapse"
106331,1991,economic development and cultural change,immigration and internal migration as a mechanism of polarization and dispersion of population and development:  the israeli case,immigration and internal migration as a mechanism of polarization and dispersion of population and development the israeli case,"Using data from various official Israeli sources for the period 1950-1985 the author analyzes trends in immigration and internal migration and their impact on spatial distribution stressing their effects on economic development. ""The research presents the spatial results of external Jewish migration to Israel from 1882 to 1985 and internal Jewish migration in the period from 1961 to 1983. It also shows that the spatial results according to the new definitions are not static and that they vary dramatically over a period of time."" (EXCERPT) Collapse"
106351,1991,economic inquiry,"extending the equation of exchange:  dual equations, for output  and input",extending the equation of exchange dual equations for output and input,"EXTENDING THE EQUATION OF EXCHANGE: DUAL EQUATIONS, FOR OUTPUT AND INPUT I. INTRODUCTION As a framework to organize thinking about inflation, deflation, and recession or depression, an equation of exchange is often useful: either Irving Fisher's MV * PT where T is an index of all the economy's transactions during a time period (final real output, inter-firm, resource input, and even existing assets), or the more modern MV * PY where Y is an index of final real output only. (1) But neither equation directly and explicitly considers resource input prices and quantities. Doing so can illuminate a variety of economic events and issues. Because the all-transactions equation of exchange integrates less easily with modern macroeconomic models that explain Gross or Net National Product, our point of departure is the final output equation of exchange, MV * PY. We treat it as an identity that summarizes a larger macro-economic model, not as a non-tautological ""quantity theory of money"" that holds only in monetary equilibrium. (2) II. AN EQUATION OF EXCHANGE FOR RESOURCE INPUTS An equation analogous to the equation of exchange for output can be defined for resource inputs, giving dual equations of exchange: MV * PY and MV * WX, where W represents an index of resource input prices, and X an index of real resource inputs used. (The equation MV * PT has no such unexploited analogue, because it already encompasses all the economy's transactions.) Substituting and rearranging shows that nominal output equals nominal income, (3) and the ""productivity ratio"" of output to input quantities equals the ""input reward ratio"" of input to output prices: PY * WX and Y/X * W/P. Assume also a production function, relating the economy's output capacity to the resource inputs available: Y = f(X). The U.S. government routinely estimates the value of M,P,Y, and hence of V * PY/M, and private research has extended these estimates back to the 1800s, but there are no annual estimates of W and X for the entire economy, although at least in principle they could be estimated. Meanwhile, albeit empty empirically, (4) they are useful constructs. Dual equations of exchange make it possible to discuss directly not only production but also incomes--and income issues can fascinate students for whom the equivalence of real output and income may be a dimly understood abstraction. Illustrating Dual Equations of Exchange Dual equations of exchange formalize the ""circular flow of economic activity"": while resource inputs flow from the household to the producing sector, and real output flows from the producing to the household sector, money in exchange flows in the opposite direction. This circular flow depicts a world in which households ultimately own everything and therefore buy all final output while supplying all resource inputs and receiving all income: on the diagram's top half, MV represents the flow of money out of the household sector and PY represents the flow of money into the producing business and government sectors on the diagram's bottom half, MV (again) represents the flow of money out of the producing business and government sector and WX represents the flow of money incomes into the household sector. Clearly, MV * PY and also MV * WX, because payments must equal receipts, for output and for resource inputs. Also, PY * WX and Y/X * W/P, because households own everything and thus receive all funds that flow in to the producing sector. The Ownership Perspective: Justification and Implications For students, the diagram's ownership perspective needs some justification and elaboration: Workers of course belong to households all private businesses are owned by some person or group all governments are controlled (the essence of ownership) by some person or group (be they voters, politicians, bureaucrats, or despots), who by definition are part of the household sector. … Collapse"
106409,1991,journal of business and economic statistics,hierarchical models for cross-classified overdispersed multinomial data,hierarchical models for cross classified overdispersed multinomial data,"When a vector of sample proportions is not obtained through a simple random sampling, the covariance matrix for the sample vector can differ substantially from the one corresponding to the multinomial model (Wilson 1989). For example, clustering effects of subject effects in repeated-measure experiments can cause the variance of the observed proportions to be much larger than variances under the multinomial model. The phenomenon is generally referred to as overdispersion. Tallis (1962) proposed a model for identically distributed multinomials with a common measure of correlation and referred to it as the generalized multinomial model. This generalized multinomial model is extended in this article to account for overdispersion by allowing the vectors of proportions to vary according to a Dirichlet distribution. The generalized Dirichlet-multinomial model (as it is referred to here) allows for a second order of pairwise correlation among units, a type of assumption found reasonable in some biological data (... Collapse"
106420,1991,journal of economic dynamics and control,intertemporal issues in international macroeconomics,intertemporal issues in international macroeconomics,"Recent work in macroeconomics in general, and in international macroeconomics in particular, has emphasized the need to base such models on firm microeconomic underpinnings. Essentially, this has come to mean that the underlying behavioral relationships are derived from some form of intertemporal optimization by representative agents, with particular attention being devoted to ensuring that the equilibrium is consistent with the intertemporal resource constraints facing the individual and the economy. While it is fair to say that macroeconomic theory is increasingly being carried out within this type of framework, there are eminent economists who remain skeptical of what this approach has to offer. This view is expressed most forcibly by Riidiger Dornbusch, who in the introduction of his recent collection of essays writes: ‘Although much of policy-oriented open-economy macroeconomics stands unproved, I am impressed with near-complete sterility of the intertemporal approach in the face of actual policy issues. Perhaps as it matures in the hands of some of the excellent scholars now working in that mode, it will come to yield a richer harvest.’ This special issue of the Journal of Economic Dynamics and Control is devoted to studying intertemporal issues in international macroeconomics. There are two features to the intertemporal approach which those who adopt this framework find to be attractive. First, by deriving the model from underlying microeconomic principles, much of the arbitrariness associated with macroeconomic modeling is eliminated. But the extent to which this is accomplished should not be overstated, since a good deal of arbitrariness inevitably still remains. The nature of the objective function, the range of decision variables, the specification of the constraints, all typically remain subject to choice. Secondly, by explicitly introducing some intertemporal measure of welfare, the intertemporal approach provides a natural frame- Collapse"
106421,1991,journal of economic dynamics and control,on exchange-rate stabilization,on exchange rate stabilization,"Policy makers believe that exchange rate stabilization would induce greater price discipline for the participants. Economic analysis suggests some loss of output would also be incurred. This paper supplies some estimates of the net benefits by examining how the output-inflation trade-off varies as a function of the degree of exchange rate stabilization, for each of the G3 economies. It appears that exchange rate stabilization may give (Pareto) superior results for output and inflation, compared to a free float, but that outcome is the exception rather than the rule because the advantages of greater exchange rate stability are usually offset by greater monetary instability. Coordination as well as exchange rate stabilization is needed to secure price discipline without output losses. Copyright 1991 by Scottish Economic Society. Collapse"
106422,1991,journal of economic dynamics and control,tariffs and the current account:  on the macroeconomics of commercial policy,tariffs and the current account on the macroeconomics of commercial policy,"Abstract This paper examines the impact of a tariff on the current account in a model which emphasizes the fact that it takes time for the production sector of an economy to adjust to a change in relative prices. Income effects of the adjustment lead to a transitory current-account surplus because the deleterious effect of the tariff on national income is larger in the long run than in the short run. In addition, relative prices ‘overshoot’ in response to the tariff, leading to an increase in the consumption rate of interest which, in turn, generates an increase in desired saving. Collapse"
106423,1991,journal of economic dynamics and control,tariffs and sectoral adjustments in an open economy,tariffs and sectoral adjustments in an open economy,No Result.
106424,1991,journal of economic dynamics and control,imported input price and the current account in an optimizing model without capital mobility,imported input price and the current account in an optimizing model without capital mobility,Abstract This paper analyses the effect of an oil-price increase in an economy which cannot borrow in the international capital markets. Current-account surpluses and deficits are matched by reserve changes at the central bank. It is shown in an optimizing setup that an unanticipated oil-price increase worsens the current account and lowers the real interest rate contrary to models which assume perfect capital markets. Collapse
106425,1991,journal of economic dynamics and control,macroeconomic adjustment under alternative lending arrangements,macroeconomic adjustment under alternative lending arrangements,"Abstract This paper investigates macroeconomic adjustment for a small open economy under alternative lending arrangements. The arrangements considered all have the common feature of a link between international lending rates and measures of creditworthiness. By employing an intertemporal maximizing model of a small open economy, the paper analyzes adjustment paths for the stock of international debt, the real exchange rate, and consumption in response to economic shocks. The paper finds that arrangements linking interest rates on lending to the outstanding stock of debt are consistent with a stable adjustment trajectory, whereas arrangements linking interest rates to performance on the trade balance are likely to be unstable. Collapse"
106426,1991,journal of economic dynamics and control,export instability and the economic performance of developing countries,export instability and the economic performance of developing countries,"Abstract This paper explores the economic consequences of export instability by developing a stochastic optimal control model that is based on the maximizing behavior of a risk-averse representative agent. The model offers an analytical framework that can predict the consequences of export instability for the intertemporal choices of consumption, saving, and investment. The model's results are then used to interpret the large existing body of empirical findings on the relation between export instability and savings, investment, and output growth rates. Collapse"
106427,1991,journal of economic dynamics and control,a model of currency depreciation and the debt-inflation spiral,a model of currency depreciation and the debt inflation spiral,"This paper shows how a government that cannot make credible policy commitments might manage its exchange rate and fiscal stance in a world of rational expectations. The dynamic-game model developed here potentially can generate diverse patterns of macroeconomic behaviour, patterns that differ as a result of assumed differences in government objectives. Under some types of government, the ongoing strategic interaction between the public and private sectors leads to a disinflationary outcome. Other governments may push the economy into a spiral of rising debt and inflation. Collapse"
106428,1991,journal of economic dynamics and control,optimal taxation and inflation in an open economy,optimal taxation and inflation in an open economy,No Result.
106429,1991,journal of economic dynamics and control,temporary stabilization policy:  the case of flexible prices and exchange rates,temporary stabilization policy the case of flexible prices and exchange rates,Abstract The effects of a temporary slowdown in the rate of expansion of money supply are studied under Ramsey-Lucas and money-superneutrality assumptions. It is shown that such a policy expands consumption and leads to capital decumulation and current-account deficits in the short run. The model is a benchmark case in which temporary policy is always detrimental to welfare. Connection with the policy-credibility issue is briefly discussed. Collapse
106430,1991,journal of economic dynamics and control,the welfare economics of cooperative and noncooperative fiscal policy,the welfare economics of cooperative and noncooperative fiscal policy,"In a competitive two-country overlapping generations model with perfect capital mobility, a plan that is individually Pareto optimal (that is Pareto optimal with respect to individual preferences) can be sustained without coordination of national fiscal policies when the fiscal arsenal is restricted to lump-sum taxes and government borrowing. Cooperation is required to achieve a Pareto optimum with respect to the two utilitarian national social welfare functions. Cooperation and international side payments are required to achieve an optimum with respect to a utilitarian global social welfare function. Without international lump-sum transfers, when distortionary taxes on capital income are permitted, Pareto optima with respect to national social welfare functions and global social welfare optima will not be individual Pareto optima: efficiency is traded off for a more desirable intergenerational and international distribution of resources. With nationally provided international public goods, the achievement of individual Pareto efficiency requires coordination of public spending but not of financing. Collapse"
106441,1991,journal of environmental economics and management,global environmental problems: the effects of unilateral actions taken by one country,global environmental problems the effects of unilateral actions taken by one country,"Abstract In global environmental problems, each country's own contribution to worldwide emissions is small, so there is little a country can do by itself. To solve global environmental problems one needs coordinated actions between countries. Environmental groups often advocate that in spite of the global character of the problem, a country ought to take unilateral actions to reduce its environmentally harmful emissions. The argument for such unilateral actions is that they at least give a contribution in the right direction (however small), and also that by “setting a good example” of this type one might affect the behavior of other countries, and/or improve the chances of reaching international agreements of coordinated reductions of harmful emissions. The paper gives an analysis of the consequences of unilateral reductions of harmful emissions. It is shown that such a policy will generally affect the outcome of international negotiations about reduced emissions. The outcome of such negotiations may very well imply higher total emissions when one country reduces its emissions unilaterally than if both countries act selfishly. Collapse"
106442,1991,journal of environmental economics and management,how to set catch quotas: constant effort or constant catch?,how to set catch quotas constant effort or constant catch,"Abstract This paper considers whether the total allowable catch from a fish stock should be a fixed annual quantity or based on constant fishing effort. It consists of two parts, a theoretical part and an empirical part based on data from the Arcto-Norwegian cod stock. In the theoretical part it is shown that realistic cost and revenue functions have opposite effects on whether a constant quota or a constant effort yields the highest expected profit. A concave revenue function implies that a constant quota will be preferable, while a stock-dependent unit cost of landed fish has the opposite implication. The empirical part investigates how large the difference between the average profit yielded by the two strategies is likely to be, on the basis of some stylized facts about the Arcto-Norwegian cod stock. The size of this stock fluctuates considerably over time, due mainly to fluctuations in the size of year classes. Spectral analysis indicates cyclical movements, and so a sine curve was used to generate recruitment cycles. The difference in average profit yielded by the two harvest strategies is very small in most cases, or of the order of 1–2%. This result is relatively robust with respect to alternative specifications of the cost and the revenue functions, but a maximum difference of 20% was produced by a non-stock-dependent unit cost of fish and a kinked revenue function, where catches exceeding a certain quantity are worthless. Collapse"
106457,1991,journal of mathematical economics,communication requirements and strategic mechanisms for market organization,communication requirements and strategic mechanisms for market organization,"Abstract A mechanism realizes an objective if, when agents are honest, the objective is met when the mechanism is used. A mechanism implements an objective if, when agents act strategically, the objective is met when the mechanism is used. This paper addresses the question: do we incur an ‘informational cost’, in terms of increased communication requirements, when we insist on implementation of the (constrained) Walrasian correspondence as opposed to, simply, its realization? Communication requirements of a mechanism are measured by the size of its message space. We prove the existence of a mechanism that implements the (constrained) Walrasian correspondence with a message space which is no larger than that of the competitive allocation mechanism. We explore implementation with quasi-games which generalize games in the sense that they have outcome correspondences. Since a quasi-game designer is free to specify a set of outcomes rather than a single outcome for each strategy profile, a smaller amount of information needs to be conveyed to the center. We prove our result using a quasi-game whose solution is well-defined and invariant with respect to alternative beliefs held by agents about selections from the set of out-of-equilibrium outcomes. Collapse"
106485,1991,journal of urban economics,wasteful commuting:  a re-examination,wasteful commuting   a re examination,"In a provocative article, Hamilton [l] demonstrates that the simple monocentric model of urban location, modified to allow for decentralized employment, does a poor job of predicting actual commuting distances in major U.S. cities. According to the monocentric model, households locate to maximize the utility received from housing and all other goods (= income commuting costs housing costs). If households are otherwise indifferent among housing locations, and if there is one worker per household, utility-maximizing location choices minimize aggregate commuting distances, given the location of houses and jobs. The fact that average actual commutes are about 8 times the average minimum commute casts doubt on the validity of the monocentric model. Hamilton’s findings suggest that one should modify the monocentric model to incorporate other determinants of location choice. In the discrete housing-choice literature, for example, households receive utility from neighborhood amenities, as well as from the commuting distances of all workers in the household. When utility is defined in this way, utilitymaximizing location choices need not minimize aggregate commuting distances; thus, the more general location-choice model may explain the divergence between average-actual and average-minimum commutes. In this paper we examine by how much a broader definition of utility of residential location raises the average required commute. Specifically, we (1) estimate a utility function defined over housing and neighborhood attributes as well as the commuting distances of all workers in the Collapse"
106486,1991,journal of urban economics,urban land value functions with endogenous zoning,urban land value functions with endogenous zoning,"The main objective of the present research is to develop a methodology for the generation of trips using Geographic lnformation Systems and Remote Sensoring. According to the new developments in this technology, it is now possible to sectorise an urban structure, through the analysis of the data obtained from aerial photographs. Furthermore, this sectorization allows the systematic identification of sectors with similar characteristics. These characteristics allow the definition of the geometric and photographic patterns in the urban structure, which can be grouped up to form ""Homogeneous Aggregated Sectors"" (HAS). Based on the fundamentals from Geographic Information Systems (GIS) and Remote Sensing (RS), using the treatment of aerial photographs, a methodology was developed for the analysis and classification of the data from these images that results in of ""Photo-!nterpreted Classes"" (PIC), which are grouped up to form ""Homogeneous Aggregated Sedors"" (HAS). With respect to the case study of the town of Sobradinho, 16 PlCs and 15 HASs were identified. Furthermore, based on the relationship of these classes with the census data obtained from IBGE (Instituto Brasileiro de Geografia e Estatistica), the formulation of a model for the generation of trips was developed. This model of trip generation factors for each class, in Sobradinho town was obtained. This results shows the relation between land use pattern and trip generation. In this study, the highest value was obtained in residential sectors with buildings with many floors while, mixed occupation patterns, such as residences and workshops or residences and comercial area, presented low value comparing with residencial patterns. It was also verified that the factor values of a class are influenced by the age of the area. 1 INTRODUCTtON In the Transportation Planning process one of the main activities is to estimate the trip demand. It aims to determine the amount of trips generated and attracted by a traffic zone, that will be essential to continue the process. In this sense, it is fundamental to determine the characteristics and patterns of trips (type, place of origin and destination, etc.). In order to estimate the trip demand, the traditional methodologies require the attainment of a large number of information. These information are not only related to the system itself, but the socio-economic data of the population. Usually, it can be reached through extensive collection of data related to individual trips, that are called origin destination research. This collection phase consume the most part of the time and demand the use of expressive human and financial resources, making the process sometimes invalid for most cities, principally the small and medium sized zones. The trip generation / attraction variables are extremely dynamic in developing countries. Because of this rapid development, which has a narrow relationship with the transportation system and urban activities, it is necessary to obtain information about the trip characteristics. Then, these information has to be collected continuously for the analysis and the establishment of the future behaviour of the trips. This fact has as consequence the increase of the planning process costs and the necessity to develop alternative forms to collect, process and analyse the data for trip generation. In this paper a trip generation model is presented, based on land-use patterns and land inhabitation, obtained through the association of Remote Sensing (RS) and Geographical Information System (GIS). in search of speed and effectiveness in the attainment of information for transportation planning. This paper is structured in five sections. In the first section, the traditional trip generation models are discussed. The RS and GIS concepts are treated in the second section. In the third one, the formulation of the developed model is presented. A case study in the town of Sobradinho (District Federal Brazil) and the viability of the model are described in the fourth section. Finally, the conclusions are presented in the fifth section. 2 EvALGATiQN QF THE GENERATION MODELS The evaluation of the trip generation models shows a typically static level of analysis, since they don't possess resources which can enable them capture the urban changes related to land-use at a quick rate. Also, the large quantity of data needed to execute the modelling, impedes the updating of these data in a quick and continuous way, due to the high operational cost. These restrictions make the modelling, and evidently, the treatment of transportation problems complex. All these lead to the need to rely on tools which permit the treatment of large quantities of data, in a rapid and precise form, and at the same time facilitate the intervention of the modelling process in an interactive and dynamic way. In this sense, some attempts can already be observed. Silva et al. (1 995) developed a simplified model which facilitates the generation of trip rates, making use of GIS. Despite the effort involved in obtaining the additional data, one can still observe a strong dependence caused by lack of data, leading to the least exploration of the functional characteristics of GIS, together with other technologies. 3 GEOGRAPHIC INFORMATION SYSTEMS IN TRANSPORTATION (SIG-T) AND REMOTE SENSING (RS) The Geographic Information Systems can be defined as a type of information System which involves databases, technology and staff in a systematic and interactive way, in order to execute spatial analysis (Dantas et al., 1996). These analysis are defined by Goodchild (1988) apud (Gatrell, 1991) as a group of analytic methods which demand access to the attributes (properties) of the study objects and information of their location (georeference). Spatial analysis plays a fundamental role in the treatment of graphic data (aerial photographs and satellite images) obtained from Remote Sensing, thereby permitting the attainment of essential information for the treatment of transportation problems, not simply as a viewing tool and data storage, but as a direct planning instrument. The potential to be explored with the use of GIS and RS in transportation studies has presented considerable results. Recently, Bartoli et al. (1996), Taco et al. (1996) and Yamashita et al. (1 997), obtained essential information to evaluate the accessibility of locations at bus stops; traffic zoning of the urban area through the definition of Aggregate Sectors, to analyse the intervening variables in attracted and produced trips, and the definition of bicycle routes, respectively. 4 TRIP GENERATION MODELLING WITH THE APPLICATION OF GIS AND RS This model seeks to relate land-use patterns with the trip generation of a determined area. In other words, we intend to determine the rate of generated trips function in the inhabited area, for each type of urban construction. It is necessary to identify and characterise the different land-use patterns, in order to later correlate them mathematically with the trips. The modelling process is divided in five stages namely, identification and characterisation of photo-interpreted classes (PC) and Homogeneous Aggregate Sectors (HAS); definition of involved variables; structuring and formulation of mathematical theory; and mathematical calculus of the factors of trips generated in the area. In the modelling process, it is necessary to create data layers in the GIS software, which can relate transportation in terms of levels, types and location of the socioeconomic activities within the urban context. In this model, the land-use layer maps, the ""censitaire"" sectors (CS), the traffic zones (TZ), and the photo-interpreted classes are assumed to be created in GIS. With respect to land use, the existent distribution and type of use must de defined. In the ""censitaire"" sectors and traffic zones, demographic and socio-economic data must de aggregated, together with the limits and location of their centroids. On the other hand, the location of the centroids and their areas within the CS and TZ are important for the PICs. SCANNER 7 1 GEOREFERENCE REGISTER AERIAL PHOTOGRAPHS Collapse"
106489,1991,journal of urban economics,supply-side regional economics,supply side regional economics,Review of some fundamental previous misinterpretations of energy sector supply side economics
106490,1991,journal of urban economics,"regions, the dollar, and reindustrialization",regions the dollar and reindustrialization,"AbstractBackground International studies have shown that cancer survival was generally low in the UK and the Republic of Ireland compared to western and northern European countries, but no systematic comparative analysis has been performed between the UK countries and the Republic of Ireland. Methods Population‐based survival for 20 adult malignancies was estimated for the UK and the Republic of Ireland. Data on adults (15–99 years) diagnosed between 1991 and 1999 in England, Scotland, Wales, Northern Ireland (1993–99) and the Republic of Ireland (1994–99) were analysed. All cases were followed up until the end of 2001. Relative survival was estimated by sex, period of diagnosis and country, and for the nine regions of England. Predicted survival was estimated using the hybrid approach. Results Overall, cancer survival in UK and Republic of Ireland improved during the 1990s, but there was geographic variation in survival across the UK and Republic of Ireland. Survival was generally highest in Ireland and Northern Ireland and lowest in England and Wales. Survival tended to be higher in Scotland for cancers for which early detection methods were in place. In England, survival tended to be lower in the north and higher in the south. Conclusions The geographic variations in survival seen across the UK and Republic of Ireland are narrower than between these countries and comparable European countries. Artefact is likely to explain some, but not all of the differences across the UK and Republic of Ireland. Geographic differences in stage at diagnosis, co‐morbidity and other clinical factors may also be relevant. List of Tables, 9 Collapse"
106491,1991,journal of urban economics,urban agglomeration economies in the presence of technical change,urban agglomeration economies in the presence of technical change,ERR
106494,1991,journal of urban economics,separating tiebout equilibria,separating tiebout equilibria,"Viscusi (1978) shows how, in markets with quality uncertainty, perfect certification results in separation from top down due to an unraveling process similar to Akerlof (1970). De and Nabar (1991) argue that imperfect certification prevents unraveling so that equilibria with full separation do not exist. This note shows that, if one considers the buyers' buying decision explicitly, a separating equilibrium with imperfect certification does exist. Collapse"
106500,1991,public choice,the item veto and state government expenditures,the item veto and state government expenditures,No Result.
106501,1991,public choice,"the ""mess"" of the public industrial production in austria:  a typical case of public sector inefficiency?",the mess of the public industrial production in austria a typical case of public sector inefficiency,"Summary and conclusionThe economics of property rights, the X-efficiency approach, the economic theory of bureaucracy, the public choice theory and the findings about union-controlled firms have been used to explain inefficiency in the public sector in general. These approaches, however, have been rarely applied specifically to public industrial production in competitive markets with private goods characteristics. In Austria a sixth of the industrial production takes place in public enterprises. Until lately there were neither thorough audits nor strict efficiency controls by the government or administration. For a long time the obvious inefficiencies were tolerated by the taxpayers, the government and even by the opposition. Moreover in the Austro-keynesian era the public industrial firms were implicitly used for reaching the government's stabilization policy goals immediately. This brought about big losses and created a heavy burden on the federal budget. Nonetheless the microeconomic consequences of this kind of Austrian public industrial policy have not been analysed satisfyingly.Discussing the Austrian situation we assert that the above-mentioned approaches can be well applied for capturing the mess of the public enterprise operating in competitive private goods markets. On this theoretical base we form hypotheses of comparatively low productivity, high labor cost, high investment rates, strong growth of output and low profits (high losses) in the public industrial sector. We test our hypotheses by employing statistical means tests and regression analyses. By and large our hypotheses are supported by the empirical findings with one prominent exception: Despite the presumed input-maximizing behavior of the public managers, the low threat of illiquidity following investment failure in public enterprises and the evident stabilization behavior of public industrial firms it surprisingly turns out that the private industrial sectors largely show higher rates of investment as compared to the public ones. The most striking results indicating inefficiencies of public industrial enterprise can be obtained with respect to labor cost efficiency and profitability. Overall productivity is also unambiguously lower in the predominantly public industrial sectors. The pursuit of stabilization goals by public industrial firms can be identified empirically by comparatively high labor costs as a consequence of expansionist wage policy strengthening the demand for consumption goods and by the relatively strong trend of turnover.We argue that in the end it has always been the logic of collective action which has rendered inefficiencies in the public sector possible: The inherent lack of incentive to exert effective control of an efficient use of public resources and the institutional incentive to utilize this condition selfishly for personal and political goals are responsible for the pertinacity of inefficiencies caused by the factors mentioned at the beginning of Section 4. As for the Austrian case we demonstrate that this holds true even for economic environments as transparent as competitive markets. In this perspective the mess of the public industrial enterprise in Austria is indeed a typical case of public sector inefficiency. On the other hand this problem is not necessarily doomed to persist. The turn of mind in the Austrian public opinion and industrial policy in the mid-80s may serve as a “proof” for it. However, to achieve a permanent solution to the problem one would have to enter the domain of constitutional economics and policy. If Austria wants to continue her way in this direction the present situation looks promising. Collapse"
106505,1991,public choice,a constitutional interpretation of the firm,a constitutional interpretation of the firm,No Result.
106506,1991,public choice,burden sharing in the nato alliance:  an empirical test of alternative views,burden sharing in the nato alliance an empirical test of alternative views,"This paper undertakes an empirical test of two opposing views of interdependencies among members of military alliances. The first view, associated with Olson and Zeckhauser, argues that the public good aspect of defense induces free riding behavior by smaller alliance members, which imposes a disproportionate burden of supporting the alliance on the larger members. A second view argues that NATO defense activities produce a mix of outputs, some of which are not purely public, and some of which may be complementary across nations. Furthermore, the nature of the weapons systems and their relative use by alliance members may induce substantial cooperation by allies. The test proposed here analyzes the relationship between defense spending shares of NATO members and their population and relative wealth shares. A simple model is specified and tested using pooled time series cross sectional data. The empirical results indicate more support for the cooperative view of ally relationships than the Olson-Zeckhauser non-cooperative model. Collapse"
106508,1991,public choice,public sector employee voter participation and salaries,public sector employee voter participation and salaries,"The hypothesis advanced by Downs (1957) that rational, utility-maximizing citizens would calculate the benefits and costs of voting in deciding whether to vote often has been employed to argue that voter participation rates among government employees should exceed that of comparable private sector individuals. The basic rationale for that prediction was laid out by Downs (1957: 254) who argued that ""those who stand the most to gain are the men who earn their incomes there."" Higher voter participation rates for government employees coupled with an implied preference for the expansion of the public sector has also played a role in efforts to explain the growth of government (Borcherding, Bush and Spann, 1977; and Bush and Denzau, 1977). In addition to benefiting from the supply of public goods and services, as do all voters, government employees are seen as benefiting from the expansion of government through an increase in their salaries. In this paper, the issue of voter participation rates for government employees is reexamined. Current Population Survey (CPS) data tapes containing information on individual voter behavior for the 1984 and 1986 national elections are used to estimate a qualitative choice model.1 Consistent with previous findings, the results presented indicate that government employees, as a group, are more likely to vote than are private sector workers with similar socioeconomic and demographic characteristics (Wolfinger and Rosenstone, 1980). Unlike previous studies, however, we distinguish between federal and state and local employees. This distinction is important. Building on the Downsian hypothesis that the cost and benefits of voting matter, the arguments offered in the following section lead to the prediction that the probability of voting should be lower for federal employees than for state and local. The empirical Collapse"
106512,1991,public choice,"rights, equity, and economic efficiency",rights equity and economic efficiency,"Summary and conclusionsThe Adelman-Morris premise that economic development and equity are incompatible is not supported by the evidence of a strong relationship between equality of rights and the income distribution. The premise that equity requires the denial of individual rights and the affirmation of state control is an oxymoron. What has been overlooked in the literature on growth and equity is the effect of the rights structure on both economic efficiency and on the income distribution. The evidence is that free societies have much larger shares of income going to the middle 60 percent of the distribution than is observed in societies where men are not free to choose. In politically open societies compared to politically closed regimes the share of income to the middle three quintiles is: Q2 (10.7 vs. 8.0), Q3 (16.0 vs. 11.5), and Q4 (22.9 vs. 17.9). In the aggregate the shares to the middle quintiles are 49.6 vs. 37.4. In nations that obey the rule of law compared to regimes in which the rights of the state relative to the individual are paramount the comparisons are: Q2 (11.2 vs. 7.4), Q3 (16.3 vs. 10.6), Q4 (23.0 vs. 17.3). Summing the three quintiles yields a comparison of 50.5 vs. 35.3. In countries that have private property, market allocation of resources, and minimum intervention by the state compared to command economies the shares of income to the middle quintiles are: Q2 (10.8 vs. 8.0), Q3 (16.0 vs. 11.1), and Q4 (22.8 vs. 17.7). Aggregated the shares of the middle class in regimes with high levels of economic liberty are 49.6 vs. 36.8 for regimes with restricted private economic rights.Equally revealing as a matter of equity is the status of the poor and the rich in free and statist nations. The income share of the highest income group is much larger in nations that repress individual rights than in those where rights are protected. Averaging across the rights measures the share of income going to the highest income quintile is 58.3 percent among the least free nations and is 44.4 percent among the most free, a staggering difference of nearly 14.0 percentage points. Among the poorest members of society choice of the rights regime does not have much of an impact on their share of income. While the share averaged across rights measures is larger in the most free nations (5.7 vs. 5.3), the difference is not statistically significant.3 Economic progress and equity are not incompatible. Nations can move to a less restrictive rights regime and increase economic efficiency, economic growth, and equity. Collectivism is a lubricious path to economic progress and equity. Using Ward's composite inequality index, they correlated the Ward measure with GNP per capita, Gastil's rights measures, and a number of other policy and quality of life variables. They found evidence that choice of economic system (capitalism vs. socialism) had no effect on the inequality measure, but that inequality diminishes with increases in civil and political rights. Collapse"
106513,1991,public choice,an empirical investigation of interstate afdc benefit competition,an empirical investigation of interstate afdc benefit competition,"Claims that states which offer generous welfare benefits attract the poor and that some states pay low benefits intending to drive the poor away are neither uncommon nor entirely unfounded. This paper employs a two player (state) generalized game to model states' choice of a benefit level in the Aid to Families with Dependent Children (AFDC) program. Migration by the poor in response to interstate differentials in earnings and welfare opportunities, and the subsequent changes in AFDC caseloads, drive this game.Estimation of the model (using 1979 data) suggests that states within approximately 750 miles of each other do engage in a benefit-setting game. The rival's initial number of poor and preference for non-AFDC consumption appear to be the more influential rival characteristics.These findings, while derived from a different methodological approach, are consistent with previous studies which indicate that welfare recipients tend to move toward higher benefit states. Such migration may impede the efficient spatial allocation of labor. The results also indicate that states will tend to offer lower benefits given recipient migration than would be the case otherwise. State jurisdiction over benefits consequently leads to underprovision of AFDC. Federalization of the AFDC program would improve efficiency in terms of the spatial allocation of labor and the provision of AFDC. Collapse"
106515,1991,public choice,local benefit-seeking and national policymaking:  democrats vs. republicans in the legislature,local benefit seeking and national policymaking   democrats vs  republicans in the legislature,"ConclusionA simple economic theory may provide at least a partial explanation for the ticket splitting pattern that has been observed at the federal level in the United States: where Republicans do better in races for the White House than in contests for Congressional seats. To the extent that Democratic legislators, because of their ideology, are more willing to forgo national policymaking for local benefit-seeking than their Republican counterparts, rational voters have an investment incentive to: (1) lean Democratic when it comes to casting ballots for an individual representative to Congress — thereby attempting to secure as large as possible a slice of the total government spending pie; and (2) lean Republican when it comes to races for the presidency — so as to limit the net losses imposed by the prisoner's dilemma game that local benefit-seeking by Congressional representatives entails.The foregoing theory naturally depends on the validity of the assumption that Democratic legislators engage in greater local benefit-seeking than their Republican counterparts. By examining roll call attendance rates this paper provides some empirical support for such an assumption. To the extent that roll call votes concern national policymaking issues, Democratic legislators appear to be more willing than their Republican counterparts to forgo taking part in the resolution of such issues for the sake of narrower, local-benefit seeking activities such as casework and the pursuit of pork barrel. Collapse"
106518,1991,public choice,privatization and government size,privatization and government size,"The Boston Redevelopment Authority (BRA) is responsible for land use regulation and public development in the city of Boston. As part of its responsibilities, it has managed the disposition,'by sale and by ground lease, of real property owned by it and by other public entities. From 1980 to 1991 it has used property disposition to achieve a wide variety of different policy results. Financially, it obtained long-term streams of revenue for itself, allowing it to become fiscally independent in operations, and obtained over $80 million of sale proceeds for Boston's general fund. Its dispositions have also produced or sought to produce such social benefits as affordable housing, public access to the waterfront, and economic development of distressed neighborhoods. In its disposition activities, the BRA has managed its properties as a portfolio to maximize public benefits and has utilized the strength of the private real estate market to leverage the creation of public benefits, demonstrating a strong spirit of creativity and public entrepreneurialism. This thesis is the first systematic examination of the BRA's disposition strategy. Chapter One demonstrates the importance of dispositions as a means of carrying out public policy objectives. Chapter Two examines the response of the BRA's disposition strategy to the external factors of parcel size, location, and ownership, the real estate market, and political priorities. Chapter Three discusses the BRA's disposition policies in light of its overall institutional goals, its organizational structure, and the administrative processes by which it manages dispositions. Chapter Four examines specific dispositions to show how the BRA converted real estate into other financial and social assets. Chapter Five discusses the implications of the BRA experience for other cities and examines some of the policy dilemmas raised by the BRA's disposition strategy. Thesis Supervisor: Lynne B. Sagalyn Title: Associate Professor of Planning and Real Estate Development Collapse"
106520,1991,public choice,the war between the rent seekers,the war between the rent seekers,"Vedder and Gallaway (1991) develop and test a unique theory about the interactions between the levels of spending captured by rent-seeking interest groups. They hypothesize that initially rent seekers cooperate in ways that expand government spending and rents. At some point, however, groups can only expand their rents at the expense of other rent-seekers and that this relationship will strengthen over time. In this brief note, we update their empirical model 20 years into the future and find their prediction was accurate. The relationship is now stronger and more states have moved into the negative range. Collapse"
106548,1991,accounting review,experience and error frequency knowledge as potential determinants of audit expertise,experience and error frequency knowledge as potential determinants of audit expertise,No Result.
106549,1991,accounting review,auditor's representation and retrieval of internal control knowledge,auditors representation and retrieval of internal control knowledge,"SYNOPSIS: This article examines the characteristics of experienced and inexperienced auditors' retrieval of internal controls from memory. Because it is possible that knowledge retrieval is a function of the manner in which information is stored in memory, this study also investigates two kinds of knowledge organization: (1) a taxonomic representation, as typified by categoric checklists; and (2) a schematic representation, as typified by information flowcharts. Although the relationship between knowledge and expertise is well documented in the literature, few accounting researchers have attempted to study the memory structures of skilled individuals (Libby and Frederick 1990; Weber 1980). Thus, this work is motivated by two broad objectives: (1) to understand the nature of expertise, which will help in assessing the boundaries of auditors' abilities and performance; and, (2) to comprehend the cognitive skills of both expert and novice auditors, which will help in facilitating skill acquisition by unseasoned individuals. One hundred thirteen auditors and 97 auditing students were used to study the retrieval characteristics of the taxonomic and schematic knowledge structures and memory differences between experts and novices. Auditors freely recalled more internal controls when such controls were organized by transaction flow. Auditing students did not possess complete knowledge organizations, and the type of representation did not affect their free recall of the controls. Across a given knowledge representation, providing subjects with complete subsets of internal controls as a memory aid did adversely affect the retrieval of additional controls in the taxonomic but not the schematic organization. Within a specific class of internal controls, Collapse"
106550,1991,accounting review,auditors' evaluation of test-of-control strength,auditors  evaluation of test of control strength,"Ageing is associated with a progressive decline in cognitive function, which occurs according to heterogeneous trajectories, dependent on multiple physiological and environmental components. To tackle this major challenge, we designed a project to test the effect of a tailored physical exercise intervention program in the cognitive function of a Portuguese elderly cohort, included in the AGA@4life project. The exercise program included aerobic and strength components, prescribed in a personalized approach according to the AGA@4life model, and implemented under direct control of two experienced professionals. The 33 included elderly participants were divided into two groups (intervention group -IG - and control group - CG) according to their willingness to participate in the physical training program. Cognitive function was evaluated with the Cambridge Neuropsychological Test Automated Battery (CANTAB) platform at baseline ant three-months after the intervention period in all the participants. The groups had similar clinical and demographic characteristics at baseline. After the intervention program, significant improvements in cognitive function were observed in the IG, but not in the CG. Particularly, a significant improvement in motor control, spatial working memory and visuospatial associate learning were depicted in the IG, which revealed an overall better cognitive performance as compared with the CG after the follow-up period. The results clearly identify physical exercise as an effective non-pharmacological tool to positively modulate age-related decline in cognitive function in older adults, particularly when prescribed in a personalized approach with a multicomponent structure as foreseen in the innovative AGA@4life model. Collapse"
106551,1991,accounting review,a perspective on cognitive research in accounting,a perspective on cognitive research in accounting,"This chapter highlights some of the tensions and most promising points of convergence between the strategic management and stakeholder theory literatures. We briefly examine the early development of both areas, identifying some of the background assumptions and choices that informed how the fields evolved, and how these factors led the two fields to engage in scholarly pursuits that seldom intersected for a period of years, followed by a renewal of interest among strategists in themes that are central to stakeholder theory. From this discussion, we develop a larger agenda with specific topics as examples of areas that offer promise for integrative research that can advance knowledge in both fields. Our vision of the future is one in which the larger aspirations of scholars in strategy and stakeholder theory are more fully realized with human purposes, broadly defined, as the focal point. Stakeholder Theory and Strategic Management 3 Our focus in this chapter is two-fold: to identify some of the most promising points of convergence in both strategy and stakeholder theory and to provide our own vision of how these two fields might develop to create a richer platform for a study of organizations that puts human purposes at the center. Such inquiry would aim to be both academically rigorous and have practical relevance – to provide a set of ideas and practices that better connect what firms do to the aspirations and goals of the stakeholders who interact with those firms. While we will identify a range of works from both stakeholder theory and strategy in this chapter, our goal is not to do a comprehensive review of the literature. We want to provide ideas for how strategy and stakeholder theory might evolve together – both to build on the latent potential that has already been revealed in the extant literature and to envision where the conversation might go. We begin with a clear bias. As scholars who see themselves as both “stakeholder theorists” and “strategists” (even though our fields tend to put us each in one of these two camps), we believe that the purpose of business is to serve human goals and advance a humancentered agenda. Despite the concerted efforts of many of our colleagues, those aspirations have not been realized – and are often subverted either directly or indirectly. Direct subversion has resulted from embracing bottom-line finanancial objectives (and dependent variables in research) as superordinate goals with inadequate consideration of nonfinancial human costs. Indirect subversion has come from grossly oversimplified narratives about business, theories that create stumbling blocks, or assumptions that no longer make sense. We think we can do better, both as individual scholars and collectively as a field, to create richer academic inquiry and provide practical guidance to managers and firms. We do not pretend to have the answers to the big questions raised across strategy and stakeholder theory, yet we do think that how we frame the agenda of the field can have a major impact on our ability to deliver on this promise. Stakeholder Theory and Strategic Management 4 One of the factors that leads us to frame this article in a non-traditional way is what we see going on in the world and how that is challenging us to think differently about business – as well as what role it plays in the world. We have a convergence of forces putting immense pressure on business to evolve and operate differently – even if people don’t want to get rid of all forms of capitalism. From the financial crisis and the growth of the “occupy Wall Street” movement, to the increasing focus on the growing inequality of wealth in the US, to the data showing decreased mobility and opportunity in the US – there is a growing sense that our system is “rigged” to benefit the rich and powerful at the expense of the poor and middle class (e.g. Pew Research data; http://www.pewresearch.org/fact-tank/2013/12/05/u-s-income-inequality-on-risefor-decades-is-now-highest-since-1928/). Even prominent proponents (and beneficiaries) of free market capitalism are vocal in noting how big a threat these trends are to the background support for business and how they have the potential to undermine the core of markets (Freeland, 2013). We also see a new set of challenges that confront not only society, but business and business leaders – from growing health care costs and access to care, to education and worker training, to climate change and access to resources, to sustainability, to infrastructure deterioration, to terrorism. While each of these issues have been on the horizon and had some impact on business, the convergence of these issues, the intensity of conversations they are generating, and the direct connections to business make them inescapably tied into how we think about business moving forward. Our approach, in part, represents the evolution of an old debate. People have talked about the notion of “corporate social responsibility” for centuries (Husted, 2015). A variety of stakeholders have articulated the idea that firms are part of society, that they owe something back to the communities in which they operate, and that they ought to conduct themselves with some Stakeholder Theory and Strategic Management 5 larger sense of responsibility and values than simply doing what pays – and what doesn’t break the law (Bowen, 1953; Carroll, 1999; Korula and Delaliex, 2016). Stakeholder theory shares with CSR a common view that business is a human institution that serves larger human ends and benefits people; however, it brings these impulses into the conversation about business in a way that starts closer to the core of how we understand business (rather than as an “add-on”, which is one of the criticisms of this large and complex literature – see Freeman (1994) and Wicks (1996)). Simply, stakeholder theory puts value co-creation with stakeholders at the heart of business; CSR puts a focus on the larger social footprint of a business and what a firm owes back to communities and affected stakeholders, including those outside the “value-chain” of the firm. Finally, we also see the continued interest in evolving how we think about and evaluate business. From the growth of socially responsible investing,,to the creation of “B” corps, to the growth of “impact investing” in the developing world, to ratings agencies and a plethora of online tools that help stakeholders understand the footprint of a company and evaluate its practices – there is a clear desire from many stakeholders to bring a complex lens to sort out which businesses they want to collaborate with and on what terms. Financial performance and success in traditional terms matter, but so do a wide array of other factors that weren’t prominent in the minds of most managers as recently as two decades ago. Sustained success, keeping ahead of regulations and stakeholder expectations, has gotten more complicated – and has blurred many of the lines that used to exist between sectors (e.g. business and society, business and government). To think about the future of strategy as a field, and how it needs to evolve along with stakeholder theory, requires that we consider these background factors as critical input for any coherent account of that future. Stakeholder Theory and Strategic Management 6 In terms of framing, we will provide an overview of both stakeholder theory and strategy, selectively focusing on some of the key themes and background we see as relevant to our discussion. We will look at some of the beginnings of strategic management and stakeholder theory, note some of the background assumptions and choices that informed how the fields evolved, note points of convergence, and briefly identify ongoing challenges. From this discussion, we will develop a larger agenda to inform our reading of the literature and our sense of positive direction for both fields. Our vision of the future is one where we see the larger aspirations of scholars in these two fields (more fully) realized, and working toward common purposes using a variety of theories, tools, and approaches. TENSIONS, COMMON GROUND AND CONVERGENCE Srategic management has been heavily influenced by economics (Porter, 1985; Rumelt, Schendel and Teece, 1991). Much, if not most, of the literature in strategy has attempted to explain firm profitability, and economic theories and empirical tools have appeared to strategy researchers to hold the most promise for doing so. The economic assumption of rational decision making tends to obfuscate the reality of human factors associated with morality, emotion and cognitive limitations. Strategy attempted to address this problem, in part, by embracing agency theory (Jensen and Meckling, 1976) and transaction costs economics (Williamson, 1985); however, both of these research areas view humans as inherently self-serving and opportunistic. More importantly, the field of economics tends to simplify the economic world into a small set of measurable variables. So while an economics perspective brought with it a rigorous methodology that had a track record and institutional legitimacy, it also brought with it a set of blinders that have limited the potential of the field and skewed inquiry towards an impersonal Stakeholder Theory and Strategic Management 7 calculative rationality and away from a more humanistic inquiry that is grounded in human identity and complexity. This view of strategy has downplayed the significance of individual humans and personal identity (e.g. McVea and Freeman’s (2005) “names and faces”; Nelson’s (2006) “economics for humans”), the role of ethics and values, and the importance of narrative and mindset to how we theorize and talk about business. Instead, the discourse has emphasized the “harder” side of business – like profits and efficiency – that many in the business world identify with. While there is value in this approach, there are also considerable limitations, espec Collapse"
106552,1991,accounting review,the information content of annual reports: a price and trading response analysis,the information content of annual reports a price and trading response analysis,No Result.
106553,1991,accounting review,auditor credibility and initial public offerings,auditor credibility and initial public offerings,"Abstract This study tests for a relationship between auditor reputation and underwriting fees incurred by foreign companies making an initial public offering (IPO) of equity securities in the United States during the years 1984 through 1991. The study also seeks to determine whether the companies tended to switch to internationally known auditors prior to the offering. Previous research has reported that domestic issuers incurred a smaller underwriting commission if they retained a Big Six or Big Eight audit firm when making an IPO, and that among those companies making auditor changes, there was a clear preference for the internationally known audit firms. A regression analysis indicates that. after controlling for ex ante uncertainty, economies of scale, and underwriter class, the marginal effect of auditor reputation on underwriting fees incurred by foreign companies making an IPO in the United States is not significant. Thus, the results in this paper contrast with the evidence in previous research revealing a negative association between auditor reputation and IPO underwriting fees. On the other hand, the percentage of foreign companies switching auditors just prior to going public was slightly higher than in the previous research on domestic companies. Collapse"
106554,1991,accounting review,effectiveness of rectification in audit sampling,effectiveness of rectification in audit sampling,No Result.
106555,1991,accounting review,using information in addition to book value in sample designs for inventory cost estimation,using information in addition to book value in sample designs for inventory cost estimation,No Result.
106556,1991,accounting review,british entrepreneurs and pre-industrial revolution evidence of cost management,british entrepreneurs and pre industrial revolution evidence of cost management,"ccounting histories have dated the advent of sophisticated cost management from the mid-1880s (Solomons 1952). The scientific management movement is credited with instituting and popularizing cost management techniques. However, it might be suspected that British en- trepreneurs of the Industrial Revolution would have developed sophisti- cated costing techniques earlier, given their significant methodological ad- vances in other economic areas. This article reports the findings from surviving business records of 25 sizeable British industrial firms (mostly in the iron and textile industries) from 1760 to 1850. Substantial evidence of a relatively mature cost man- agement has been found in four major areas of activity: cost control tech- niques, accounting for overhead, costing for routine and special decision making, and standard costing. Speculations about the motivations for cost management and about specific factors influencing the iron and textile industries are considered. Because the accounting practices of these firms predated the genesis of ""the costing renaissance"" a century later, our un- derstanding of cost management practices in the Industrial Revolution is augmented by the survey. Collapse"
106557,1991,accounting review,earnings announcements and the convergence (or divergence) of beliefs,earnings announcements and the convergence (or divergence) of beliefs,No Result.
106558,1991,accounting review,volume of trading and the dispersion in financial analysts' earnings forecasts,volume of trading and the dispersion in financial analysts earnings forecasts,No Result.
106559,1991,accounting review,common stock returns surrounding earnings forecast revisions: more puzzling evidence,common stock returns surrounding earnings forecast revisions more puzzling evidence,"Investor expectations of company earnings are reflected in stock price. Thus by comparing analyst earnings forecasts to stock prices, a test can gauge analyst rationality and market efficiency. My research is based on this premise. Following Abarbanell (1991) and Lys and Sohn (1990), this paper will extend research on analyst rationality by examining how analysts revise their opinions as stock prices change. I use two sample groups consisting of quarterly earnings forecasts from 1996 to 2001, a Dow sample and an Internet sample. Analyst earnings revisions are regressed on stock and market returns since an analyst’s previous forecast, calculating “forecast response coefficients.” The Dow forecast response coefficients are positive and significant in all years and show that forecasts do move in the same direction as stock prices. However, the Internet sample forecast response coefficient is only significant in two years, moving from 1.089 in 1998 to –0.505 in 1999. Second, I run a regression of forecast error on stock and market returns for three time periods: before, during, and after the forecast announcement. The results show that analysts do not fully incorporate information available in prices in each group, so errors can be predicted based on price movements prior to an analyst’s forecast revision announcement. I find that while both Dow and Internet analyst revisions stray from informative prices, the largest departure occurred in the Internet group in 1999. Overall, evidence indicate that analysts and prices are rational at times and irrational at times, a puzzle that should motivate future research. Collapse"
106560,1991,american economic review,the mathematization of economic theory,the mathematization of economic theory,"Many economists show certain nonconformity relative to the excessive mathematical formalization of economics. This stems from dissatisfaction with the old debate about the lack of correspondence between mainstream theoretical models and reality. Although we do not propose to settle this debate here, this article seeks to associate the mismatch of mathematized models with the reality of the adoption of the hypothetical-deductive method as reproduced by general equilibrium. We begin by defining the main benefits of the mathematization of economics. Secondly, we address traditional criticism leveled against it. We then focus on more recent criticism from Gillies (2005) and Bresser-Pereira (2008). Finally, we attempt to associate the reproduction of the hypothetical-deductive method with a metatheoretical process triggered by Debreu's general equilibrium theory. In this respect, we appropriate the ideas of Weintraub (2002), Punzo (1991), and mainly Woo (1986) to support our hypothesis. Collapse"
106575,1991,american economic review,on the sign of the investment-uncertainty relationship,on the sign of the investment uncertainty relationship,No Result.
106576,1991,american economic review,savings and wealth in models with altruistic bequests,savings and wealth in models with altruistic bequests,"During recent years much attention has been given to the role of bequests in explaining various aspects of economic behavior. For example, authors have studied the impact of bequests on physical wealth accumulation (Laurence Kotlikoff and Lawrence Summers, 1981), the interest elasticity of savings (Owen Evans, 1983), tax reform (Laurence Seidman, 1984), and the distribution of wealth (Alan Blinder, 1976a). More recently, economists have begun to distinguish between human and physical bequests (Gary Becker and Nigel Tomes, 1986). Acknowledging the presence of bequest in human form is important in analyzing each of the issues mentioned above. Outlays on children affect the shape of the family's labor supply and expenditure profiles, upon which the level of aggregate savings crucially depends. If these outlays are interest-sensitive substitutes for physical bequests, they will also influence the interest elasticity of savings. In the area of tax reform, the neutrality of the consumption tax depends on the ability of the government to identify human capital bequests from general consumption (Raymond Batina, 1987). If this is not possible, the ""portfolio-choice"" of investment in human versus physical capital may be distorted at the margin. Finally, the unequal transmission of wealth across generations may be explained by the lack of physical bequests and the inefficiently low levels of human bequests made by wealthconstrained households (Becker and Tomes, 1986). To distinguish clearly the behavior of constrained households from unconstrained households, who make sizeable physical bequests, both types of bequests must be recognized.' This paper introduces a model that makes explicit examination of these issues possible. It extends Lord's (1989) multiperiod model of life-cycle savings and adult human capital investment by adding altruistically motivated human and physical bequests.2 The model is calibrated using microeconomic data on earnings, time and goods expenditures on children's human capital, and physical bequests. We then use the model to examine the contribution of bequests to wealth accumulation and the level of savings. This topic is the source of an important but yet unresolved debate between Franco Modigliani (1988) and Kotlikoff (1988). Kotlikoff maintains that U.S. wealth accumulation is primarily the consequence of bequests, as opposed to life-cycle savings for retirement. One type of evidence cited by Kotlikoff comes from simulation results demonstrating the failure of realistically calibrated life-cycle models to generate sufficiently high saving rates and wealth:income ratios (Alan Auerbach and Kotlikoff, 1987). We show, however, that if a pure life-cycle Collapse"
106577,1991,american economic review,a new estimate of the welfare loss of excess health insurance,a new estimate of the welfare loss of excess health insurance,ERR
106578,1991,american economic review,reconciling recent estimates of the marginal welfare cost of taxation,reconciling recent estimates of the marginal welfare cost of taxation,ERR
106579,1991,american economic review,does student aid affect college enrollment?  new evidence on a persistent controversy,does student aid affect college enrollment new evidence on a persistent controversy,ERR
106580,1991,american economic review,are workers permanently scarred by job displacements?,are workers permanently scarred by job displacements,"This paper investigates whether workers suffer lasting ""scars"" following job displacements. Using David T. Ellwood's (1982) terminology, ""scars"" represent persistent effects, whereas ""blemishes"" are transitory adjustments which dissipate over time. More precisely, dislocated individuals are defined as scarred if they continue to earn less or to be unemployed more than their nondisplaced counterparts, even after the conclusion of a several-year adjustment period. Article: Displacements may have a transitory impact if workers initially obtain unstable positions but later move to more secure jobs or if low wages are received during short- lasting probationary or training periods. These temporary effects, although important, cause less concern than persistent scarring. It is therefore somewhat surprising that, despite the proliferation of recent research studying the consequences of permanent layoffs, relatively little is known about the duration of the associated adjustment problems. 1 Data for this study were obtained for heads of households from the 1969-1982 waves (survey reporting years) of the Michigan Panel Study of Income Dynamics (PSID). Displacement status was ascertained for the five base years 1971-1975, with respondents defined as permanently displaced if they terminated jobs as the result of plant closings or layoffs (excluding departures from temporary or seasonal jobs) and failed to return to the original employer by the end of the second full calendar year following the layoff. Data were collected on unemployment and weekly wages for the three years preceding and four years following the base period. The base year is hereafter referred to as time t, the following four years are referred to as periods t + 1 through t + 4, and numbers subtracted from t refer to periods preceding the base year. Information was also assembled indicating whether respondents were displaced in year t + 5. Collapse"
106581,1991,american economic review,structural determinants of real exchange rates and national price levels:  some empirical evidence,structural determinants of real exchange rates and national price levels some empirical evidence,"Contrary to the long-held notion of purchasing power parity (PPP), economists have found systematic evidence that the general level of prices across countries at a point in time varies dramatically. Irving B. Kravis, Alan W. Heston, and Robert Summers (1982), for example, report that some countries' national price levels are no more than one-third the U.S. price level. Extensions of this work show that such departures from PPP have persisted for decades. Recently, efforts have been made to explain systematically these persistent, or structural, departures from PPP. Pioneering work by Kravis and Robert E. Lipsey (1983, 1987, 1988) has demonstrated that a positive correlation between the price level and (real) per capita gross domestic product is robust across numerous cross-sectional specifications. For instance, using data from Kravis et al. (1982 table 6-12), 87 percent of the variation in national price levels (PL) of 21 countries' in 1975 is explained by per capita GDP (y) and a constant: Collapse"
106582,1991,american economic review,the winner's curse:  experiments with buyers and with sellers,the winners curse experiments with buyers and with sellers,"This paper explores the winner's curse phenomena as it was studied experimentally by Kagel and Levin. Experiments with the winner's curse are complicated by the fact that subjects can lose money and the experimenter has only a limited means of collecting it from them. Thus subjects enjoy only limited liability which has theoretical implications for behavior. In the Kagel and Levin experiments subjects were removed from the bidders' competition after losses reached a predetermined value. This experimental procedure has unknown implications for the results so ambiguity exists about whether the winner's curse was actually observed. In this study their results were replicated in an environment in which subjects were not removed. The case in which competitors are sellers is also studied. Bankruptcy cannot be a problem in sellers' competition. In both cases the winner's curse is observed. Thus the limited liability cannot be an explanation for the phenomenon reported by Kagel and Levin. In addition the paper examines the bidding behavior of all individuals and shows that this behavior does not fit any of the tested theories either on the aggregate or individual level. The ""winner's curse"" did not disappear over time during the conduct of the research. Collapse"
106583,1991,american economic review,the winner's curse and public information in common value auctions: comment,the winners curse and public information in common value auctions comment,"In common value auction experiments, individual judgment errors significantly alter market outcomes relative to standard economic formulations. Experienced bidders show sensitivity to the strategic considerations underlying the auction but not to item valuation considerations. Nash equilibrium models accurately characterize auctions with small numbers of bidders_(3-4). However, auctions with large numbers_(6-7) produce more aggressive bidding, resulting in negative profits, the winner's curse. Public informationreducing value uncertainty raises seller's revenues in the absence of a winner's curse, but reduces revenue in its presence. Similarities between laboratory and field data from outer continental shelf lease sales are discussed. Collapse"
106584,1991,american economic review,the winner's curse and public information in common value auctions: reply,the winners curse and public information in common value auctions reply,"In common value auction experiments, individual judgment errors significantly alter market outcomes relative to standard economic formulations. Experienced bidders show sensitivity to the strategic considerations underlying the auction but not to item valuation considerations. Nash equilibrium models accurately characterize auctions with small numbers of bidders_(3-4). However, auctions with large numbers_(6-7) produce more aggressive bidding, resulting in negative profits, the winner's curse. Public informationreducing value uncertainty raises seller's revenues in the absence of a winner's curse, but reduces revenue in its presence. Similarities between laboratory and field data from outer continental shelf lease sales are discussed. Collapse"
106585,1991,american economic review,some evidence on the winner's curse:  comment,some evidence on the winners curse comment,No Result.
106586,1991,american economic review,ski-lift pricing with applications to labor and other markets: comment,ski lift pricing with applications to labor and other markets  comment,"The market for ski runs or amusement rides often features admission tickets with no explicit price per ride. Therefore, the equilibrium i nvolves queues, which are systematically longer during peak periods s uch as weekends. Moreover, the prices of admission tickets are much l ess responsive than the length of queues to variations in demand, eve n when these variations are predictable. Despite the queues and stick y prices, the authors show that the outcomes are nearly efficient und er plausible conditions. They then show that similar results obtain f or some familiar congestion problems and for profit-sharing schemes i n the labor market. Copyright 1987 by American Economic Association. Collapse"
106587,1991,american economic review,"ski-lift pricing, with applications to labor and other markets: reply",ski lift pricing  with applications to labor and other markets  reply,"The market for ski runs or amusement rides often features admission tickets with no explicit price per ride. Therefore, the equilibrium i nvolves queues, which are systematically longer during peak periods s uch as weekends. Moreover, the prices of admission tickets are much l ess responsive than the length of queues to variations in demand, eve n when these variations are predictable. Despite the queues and stick y prices, the authors show that the outcomes are nearly efficient und er plausible conditions. They then show that similar results obtain f or some familiar congestion problems and for profit-sharing schemes i n the labor market. Copyright 1987 by American Economic Association. Collapse"
106588,1991,american economic review,a model of housing tenure choice:  comment,a model of housing tenure choice comment,"In most developed countries, housing receives preferential tax treatment relative to other assets. In particular (i) the housing services provided by owner-occupied housing (generally referred to as imputed rents) are untaxed and (ii) mortgage interest payments reduce taxable income. The potential economic distortions resulting from the unique treatment of housing may be substantial, especially in light of the fact that residential capital accounts for more than half of the assets in the U.S. In particular, this tax treatment distorts the households' portfolio composition, their saving rates and their tenure choice. In this paper we build a general equilibrium model populated by heterogeneous agents subject to idiosyncratic risk. We use this framework to quantitatively assess the macroeconomic and distributional distortions introduced by this preferential tax treatment. We also study the effects of alternative tax schemes which could correct the current system's bias. "" Correspondence to Maria J. Luengo-Prado at m.luengo0neu.edu. We would like to thank as many people as possible. Diaz thanks the Direccion General de Investigacion, project BEC2001-1653, for financial support. LuengoPrado is indebted to the DirecciOn General de InvestigaciOn, project BEC2000-0173, for financial support. They both are grateful to the FundaciOn Ramon Areces for financial support. All comments are welcomed. Collapse"
106590,1991,aer pandp,procrastination and obedience,procrastination and obedience,No Result.
106594,1991,aer pandp,providing earth observation data from space:  economics and institutions,providing earth observation data from space economics and institutions,"Burgeoning interest in understanding global environmental change has greatly increased demand for remote sensing of the earth from space.1 Over 60 percent of a $1 billion U.S. budget to study global change during fiscal year 1991 was allocated to space-based environmental data collection, including down payment on a $52 billion space data program, the ""Earth Observing System"" (EOS), planned for the next 15 years. This substantial expenditure, together with the huge technological scale of EOS (it will consist of large orbital platforms each carrying a multitude of sensors), have promoted considerable debate and alternative proposals for smaller-scale, modular spacecraft each carrying one or a few sensors. Proponents of the alternatives hold that the smaller-scale approach, in addition to being less expensive, foregoes few scale economies and avoids the ""eggs-in-one-basket"" risk of EOS, in which failure in launching a spacecraft or failure of one instrument on a multi-instrument spacecraft causes failure of the whole system. Advocates of EOS are concerned that the small-scale approach not only fails to exploit scale economies but may yield much lower information content, claiming that significant economies of scope are obtainable in operating and taking measurements from sensors aggregated on large spacecraft. In this paper we first ask whether scale and scope economies appear substantial enough, and risk small enough, to justify the large-scale approach presently envisioned.2 We then ask two separate but related questions about the organization of the project, namely whether these supply attributes require the technology to be an almost exclusively governmental activity. A dearth of information about remote sensing economics forces our approach to the production technology to be heuristic rather than empirical. Subject to this constraint, we fail to find evidence of significant scale and scope effects in large spacecraft and sensor manufacturing and launch, even after taking account of risk differentials among largerscale and smaller-scale systems (the former tDiscussants: Arthur DeVany, University of California-Irvine; Nancy Rose, MIT; James Dearden, Lehigh University. Collapse"
106595,1991,aer pandp,trading orbit spectrum assignments in the space satellite industry,trading orbit spectrum assignments in the space satellite industry,"Is it feasible to trade rights to use orbital slots and associated satellite frequencies in a market? Can such rights be delimited today? What is the role of international organs like the International Telecommunications Union (ITU), national agencies like the Federal Communications Commission (FCC), or the United Kingdom's Frequency Planning Organs (FPOs)? What evidence is there that such rights or assignments are in fact configured now and do indeed exist? Are they frequently traded in international as well as domestic broadcast and mobile radio services? Even if practical and viable, what economic policy or equity purposes are served by configuring actual markets for transferable or exchangeable orbit spectrum assignments? Or for schemes where national or international authorities distribute such assignments under competition or specifically by auction to the highest bidder? That is, through public as well as private auctions? Do auctions in any case further distributive equity and not just economic efficiency?' Collapse"
106596,1991,aer pandp,torts and orbits: the allocation of the costs of accidents involving spacecraft,torts and orbits the allocation of the costs of accidents involving spacecraft,ERR
106597,1991,aer pandp,"the national aerospace plane:  an american technological long shot, japanese style",the national aerospace plane an american technological long shot japanese style,No Result.
106598,1991,aer pandp,regulation and the law of torts,regulation and the law of torts,"Compensation for personal damage, defined as any pecuniary or non-pecuniary loss causally related to a personal injury under civil-tort law, is strictly based on the local jurisdiction and therefore varies significantly across the world. This manuscript presents the first “International Guidelines on Medico-Legal Methods of Ascertainment and Criteria of Evaluation of Personal Injury and Damage under Civil-Tort Law”. This consensus document, which includes a step-by-step illustrated explanation of flow charts articulated in eight sequential steps and a comprehensive description of the ascertainment methodology and the criteria of evaluation, has been developed by an International Working Group composed of juridical and medico-legal experts and adopted as Guidelines by the International Academy of Legal Medicine (IALM). Collapse"
106600,1991,aer pandp,mispriced equity:  regulated rates for auto insurance in massachusetts,mispriced equity regulated rates for auto insurance in massachusetts,"From the Santa Monica Freeway to the New Jersey Turnpike, drivers are unhappy about the cost of automobile insurance and are asking government to do something about it. California voters approved Proposition 103 in 1988; it requires that all rates be approved by the state insurance commissioner, attempts to reduce rates by 20 percent, and dramatically limits the criteria that can be used to rate drivers for premium purposes. New Jersey enacted an insurance reform law that seeks to charge insurers for a deficit-burdened state underwriting pool and prohibits the use of age, sex, and marital status in rating drivers for premiums. Other states enacting or considering significant rate rollbacks or reform since 1988 include Arizona, Florida, Michigan, Nevada, and Pennsylvania. This article describes the current consequences of similar policies adopted in Massachusetts more than a decade ago. The experience suggests that recent moves by other states in the same direction will ultimately prove quite expensive as the proportion of high-cost drivers increases and as insurers lose the incentive to write policies and control costs. The trend away from insurance premiums based on expected cost also reduces incentive effects for drivers, since insurance premiums provide a link between tort judgments and consumer decisions. Collapse"
106601,1991,aer pandp,multiple equilibria and persistence in aggregate fluctuations,multiple equilibria and persistence in aggregate fluctuations,No Result.
106603,1991,aer pandp,history and industry location:  the case of the manufacturing belt,history and industry location the case of the manufacturing belt,ERR
106604,1991,aer pandp,"complementarities, momentum, and the evolution of modern manufacturing",complementarities momentum and the evolution of modern manufacturing,No Result.
106605,1991,aer pandp,why are prices sticky?  preliminary results from an interview study,why are prices sticky preliminary results from an interview study,"Negotiating the challenges of communicating HIV/AIDS messages, which include stigma, conflicting cultural ideals, and politicallycharged resource constraints, requires a systematic stream of research. Using a series of phenomenological interviews in our exploration of the main constituencies in successful message development and delivery, namely the creative producers, the message developers, and the target audience, we acknowledge the contributions of Cultural Studies theory (Hall 1993) and to a lesser extent the Political Economy of Media (Schiller 1995). Summary According to UNAIDS, in 2003 approximately 38 million people globally were estimated to be living with HIV and nearly 3 million died from the disease (www.unaids.org). In fact 95% of the people living with HIV/AIDS are in the developing world with the African, Caribbean and Pacific (ACP) countries suffering the most. Although awareness and knowledge of AIDS levels are steadily increasing among targeted populations, the spread of AIDS in these countries continues to rise at alarming rates. In response to repeated calls for transformative research and to the moral mandate for a compassionate response to the world?s AIDS crisis, this paper?on an admittedly micro level?investigates the impact of culture and government in mediated communications about HIV/AIDS awareness and prevention in the Caribbean (Rothschild and Anderson 1998; Nowak and Siska 1995; Hill 1993; Mody 1991). Negotiating the challenges of this sensitive topic, which include stigma, conflicting cultural ideals, and politically-charged resource constraints, requires a systematic stream of research. Using a series of phenomenological interviews in our exploration of the main constituencies in successful message development and delivery, namely the creative producers, the message developers, and the target audience, we acknowledge the contributions of Cultural Studies theory (Hall 1993) and to a lesser extent the Political Economy of Media (Schiller 1995). This study focuses on the perspective of the producer of the messages (i.e., creative personnel). Conceptualization Cultural Studies theory?s most applicable assertion in the context of this study is its challenge to the linear ?sender-message-receiver? pattern of traditional communication theory. According to Hall (1993), there are several complex relationships within each step of the communication process and they are often iterative and vulnerable to what is termed that ?oppositional readings? where the intended 476 / WORKING PAPERS meaning of the message is lost or misinterpreted. As a result the audience is empowered and can now be seen as active participants in the communication process. This concept of audience participation is not new to the marketing literature and has been addressed in terms of message comprehension (Burnkrant and Unnava 1995; Greenwald and Leavitt 1984), ascribing meaning (McQuarrie and Mick 1992, Mittal 2002; Thompson, Pollio and Locander 1994), and experiential consumption (Arnould and Price 1993; Bitner 1992) among other concepts. Additionally, Cultural Studies theory holds that in the course of communication another series of relationships determine what is produced and how it is produced. It is this aspect of the process that is the focus of this particular endeavor?the relationship between the message producers (i.e., advertising agencies and creative shops) and the message developers (i.e., government and community-based organizations). The cultural background of the message designers themselves is significant as the collective experiences of the message designer may help or hurt the message production process. For example, a message producer who belongs to the same cultural background of the audience ?...may be more knowledgeable about their problems, but may share their caste and class prejudices...? (Mody 1991, 105). Those prejudices may very well be the ones which contributed to adverse conditions for the audience?conditions which the communication campaign may be designed to reverse or eradicate (e.g., HIV/AID stigma). As Johar, Holbrook and Stern (2001) have duly noted, creativity is appreciated in the marketing discipline, but rarely studied. Much of the literature surrounding the creative aspect of advertising revolves around consumers? response to various forms (Stewart and Koslow 1989; McQuarrie and Mick 1992; Clow, Roy, and Hershey 2001), the interpersonal relationships among agency personnel and between client and agency (Thompson and Haytko 1997), or the motivational perspectives of a career in art direction or copywriting (Kover, James, and Sonner 1997; Young 2000). The call for scholars and practitioners to think more about the ?creative spark, which drives much of advertising? remains virtually unanswered (Zinkham 1995). Consequently, it behooves marketers to gain a richer understanding of the lived experience of the creative personnel who bring these messages to fulfillment. Method The proposed study will take place in May 2006 and is in response to an initial round of research by a native graduate student, which dealt with three key issues: 1) assessing the baseline knowledge of key HIV/AIDS issues among the producers and recipients of the message, 2) determining the degree of experiential overlap between these two groups, and 3) gauging the effectiveness of message communication. It considered three main constituencies: 1) the producers and developers of the work (via 11 interviews), 2) the target audience (via a five-person focus group), and 3) the crafted messages themselves (via content analysis of 17 posters). No formal hypotheses were submitted. All activity took place between January and June 2003; however the communication artifacts were used island-wide between 1999 and 2003. The results showed there was acceptable baseline knowledge of key HIV/AIDS issues among the producers and selected recipients of the message; however the degree of experiential overlap between these two groups was ambiguous as target audience participation was decidedly problematic. In a similar vein, the lack of a structured research instrument made the task of evaluating messages effectiveness difficult to ascertain. Before developing a subsequent round of research to address the gaps in the preliminary study and collect more statistically compelling data, the study has been reduced into smaller segments, the first being conversations with the producers. Participants will be asked to describe the events that led up to involvement with this project per the guidelines put forth in Thompson, Locander, and Pollio (1990) for phenomenological interviewing. Interviews will be conversational in nature with the interviewee leading the exchange (both researchers will be present whenever feasible and appropriate). Rather than include explicit inquiry about the professional relationship with non-profits and public sector organizations or the impact of cultural heritage on their work, those lessons are expected to emerge from the data. The interview transcripts will be revisited using a deliberately iterative interpretative approach (Thompson 1997). Current research objectives revolve around gaining a richer understanding of the following areas of inquiry 1) each participant?s personal journey in their career, 2) the knowledge of key HIV/AIDS issues among the producers, and 3) the process of negotiating form and content of the HIV/AIDS production project. This should help to develop a more theoretical understanding of the process used in message production and uncover any latent patterns in both the content and construction of the mediated communication. Two frameworks will likely guide the retrieval of meaning from these narrative accounts. The first is the Social Adaptation of Thompson?s Hermeneutic Framework, which includes the following constructs: 1) perception of benefits; 2) consumer resistance; 3) countervailing consequences; and 4) countervailing socio-cultural forces (Rothschild and Andreasen 1998). The second is the UNAIDS/ Penn State framework, which affirms five inter-related, behavior-shaping contextual domains?government policy, socio-economic status, culture, gender relations and spirituality (Airhihenbuwa, Bunmi, and Obregon 1999). Findings/Implications Preliminary findings from the first study suggest a possible relationship between ?consumer resistance? (from the Thompson framework) and ?gender relations? (from UNAIDS/Penn State framework), which may help explain, for example, why messages that feature a woman purchasing condoms lack credibility within the Caribbean context. Other issues such as the use of Jamaican patois (a national dialect of broken English), the challenge to solicit the insight of political influencers without obligation to physically represent them in posters, and the need to honor both legislative and cultural restrictions on the type of information shared (e.g., fear appeals or heavily rational, numbers-driven tactics are discouraged and likely will not be funded), exemplify the intersection of theory and practice within a social marketing context and offer tactical routes to improving message effectiveness. Nevertheless, it is anticipated that a richer understanding of the producer?s perspective will further illuminate the meaning of the preliminary findings. References Airhihenbuwa, C., Makinwa, B., & Obregon, R. (2000), ?Toward a New Communications Framework for HIV/AIDS?, Journal of Health Communications, 5 (Supp), 101-111. Advances in Consumer Research (Volume 34) / 477 Arnould, Eric J. and Linda L. Price (1993), ?River Magic: Extraordinary Experience and the Extended Service Encounter,? Journal of Consumer Research, 20 (1), 24. Bitner, M. (1992), ?Servicescapes: The Impact of Physical Surroundings on Customers and Employees,? Journal of Marketing, 56 (April), 57-71. Burnkrant, Robert E. and H. Rao Unnava (1995), ?Effects of Self-referencing on Persuasion,? Journal of Co Collapse"
106606,1991,aer pandp,projecting faculty retirement:  factors influencing individual decisions,projecting faculty retirement factors influencing individual decisions,"The impending elimination of mandatory retirement for tenured faculty on January 1, 1994 (Public Law 99-592, 1986), has raised the visibility of a number of questions about faculty retirement behavior. What are the factors that influence individual faculty members' retirement decisions? How important are financial and nonfinancial considerations? Why do faculty in private institutions work to a later age, on average, than their colleagues in public institutions? Are there other systematic (for example, genderor discipline-related) differences as well? This paper is an attempt to answer such questions about individual faculty retirement behavior. The paper utilizes data collected as part of a comprehensive national study that projected faculty retirements through the year 2003 for over 35,000 faculty at 101 doctoral research, comprehensive, and general baccalaureate institutions (see our 1990 paper). The discussion that follows uses data from that broader institutional survey and from a survey of 747 faculty members age 55 and over who had separated from this same set of 101 institutions. Among the information provided by the 518 usable responses were data on factors that influence faculty members' decisions regarding the appropriate time to retire. Collapse"
106607,1991,aer pandp,ending mandatory retirement in the arts and sciences,ending mandatory retirement in the arts and sciences,No Result.
106608,1991,aer pandp,the effects of pensions and retirement policies on retirement in higher education,the effects of pensions and retirement policies on retirement in higher education,"A structural retirement model is estimated using data for tenured, male faculty employed in the 1970's at 26 high quality private colleges and universities. Simulations of raising and then abolishing the mandatory retirement age suggest very large increases in full time work by faculty members in their late 60's and early 70's. Simulations also suggest that early retirement incentive programs would offset only a small fraction of the increase in work due to changes in mandatory retirement, and that rents created by these programs exceed savings from induced early retirements, with salaries of replacements further adding to costs. Collapse"
106609,1991,aer pandp,conflict and attitudes toward risk,conflict and attitudes toward risk,"This study examined park managers' attitudes toward adventure climbing and climbing regulations, especially concerning the management of: (1) conflicts (among visitors competing for use of the same resource); (2) impact on the environment; and (3) risk (i.e. implications for rescue and legal liability problems). Questionnaires were sent randomly to 100 managers of national, state, and local parks; 43 usable questionnaires were returned. Results of the survey indicated that almost 40 percent of the managers surveyed used some type of regulation in the management of climbing. Managers perceived climbing to be no different than other recreational activities with regard to the distribution and severity of environmental impact. Less than 89% of the areas studied had any record of visitor conflict involving climbers. Only 16.3% of the managers agreed that their staff is well trained to handle climbing accidents. Results indicated that 85.7% of the agencies who believed they were well trained to handle accidents regulated climbing while only 41.2% of the agencies who believed they wele not well trained regulated climbing. While most managers were concerned with the search and rescue issues, they were not unduly alarmed about the possible legal outcomes of climbing. Over 67% of the managers believed that the climbers should pay for the cost of their own rescue. Decisions about whether or not to regulate climbing appeared to be based on liability and search-and-rescue related concerns rather than on concerns about environmental impacts or visitor conflicts. (KS) k**************M***************************************************** Reproductions supplied by EDRS are the best that can be made from the original document. ********x*************x************************************************ U.S. DEPANTMENT OR EDUCATION Mice el Educational Research and Improvement EDUCA,TONAL RESOURCES INFORMATION CENTER (ERIC) This document has been reproduced as received from the person or organization originating it O Minor changes have Peen made to improve reproduction Quality Points 01 vie* or opinions staled in I MS dOCIP mint do not necessarily represent official OEM Oosition or policy ""PERMISSION TO REPRODUCE THIS MATERIAL IN OTHER MAN PAPER COPY HAS BEEN GRANTE BY TO THE EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC): PARK MANAGERS ATTITUDES TOWARD CLIMBING: IMPLICATIONS FOR FUTURE REGULATION Michael G. Huffman, Ph.D Memphis State University Rick Harwell United States Navy Morale Welfare and Recreation 137 Abstract This study examined the relationship of park and recreation arca managers' attitudes toward the sport of climbing as potential reasons for utilizing regulations to manage climbing activity. Managers were asked to compare the use of regulations for climbing to other outdoor recreation activities from the perspective of three major areas: (a) environmental impacts, (b) potential for visitor conflicts, and (c) issues related to legal liability and search and rescue opwations. With the exception of issues related to search and rescue operations, it appeared that managers were no more likely to regulate climbing than they were to regulate other outdoor recreational activities. Possible strategies for mitigating regulation and/or prohibition of climbing were presented.This study examined the relationship of park and recreation arca managers' attitudes toward the sport of climbing as potential reasons for utilizing regulations to manage climbing activity. Managers were asked to compare the use of regulations for climbing to other outdoor recreation activities from the perspective of three major areas: (a) environmental impacts, (b) potential for visitor conflicts, and (c) issues related to legal liability and search and rescue opwations. With the exception of issues related to search and rescue operations, it appeared that managers were no more likely to regulate climbing than they were to regulate other outdoor recreational activities. Possible strategies for mitigating regulation and/or prohibition of climbing were presented. Introduction Wii'xin recent years there has been a tremendous increase in outdoor adventure recreation participation (Ewert, 1987; Jensen, 1985). Such aztivities include backpacking, hang-gliding, mountaineering, nordic skiing, and spelunking just to na-,ie a few. Possible explanations for this increase in popularity include sociolocal and personal psychological benefits. People frequently engage in recreational activities to develop a feeling of belonging to a group or class of people. Many clubs are associated with outdoor adventure activities and thus provide the potential for sociological benefits (Jensen, 1985). Participants frequently indicate the need for new experiences, excitement, and challenge as reasons for participating in outdoor adventure activities. (Jensen, 1985). Because of the tremendous resource requirements associated with outdoor adventure activities, most participants look to government agencies (particularly state and federal) to provide areas for adventure recreation. The demand for such recreation has presented managers with three major problems. First, high levels of recreational use can produce severe environmental impazts such as: the deterioration of ground vegetation, the compaction and erosion of soil, stress on wildlife populations, and deterioration of water quality. Second, conflicts can occur between individuals participating in the same recreational wtivity or in different activities because they are competing to use the same resource. Finally, because of the element of risk associated with adventure activities, managers are occasionally confronted with problems related to legal liability and the need for search and rescue operations. Perhaps because of these problems, several areas have begun to limit participation in climbing and in some cases prohibited participation in the activity entirely. The purpose of this study was to examine park managers attitudes toward the high adventure activity climbing. Climbing was selected because (a) it has a long history of participation in the United States, (b) it can occur in many different climates and environments thus ensuring national appeal, and (c) because of its long history, many recreation area managers would be familiar with it as opposed to some of the newer adventure activities. Specifically, this study examined managers' perceptions regarding the activity of Collapse"
106610,1991,aer pandp,the east european revolution of 1989:  is it surprising that we were surprised?,the east european revolution of 1989 is it surprising that we were surprised,"Many aspects of the East European Revolution are controversial, but on one point everyone agrees: it caught the world by surprise. Even local dissidents were stunned by the sudden turn of events. We will never know how many East Europeans did foresee the explosion of 1989. But at each step, accounts painted a picture of nations united in amazement. To my knowledge, only one study addresses the issue systematically. Four months after the breaching of the Berlin Wall, the Allensbach Institute asked a broad sample of East Germans: ""A year ago did you expect such a peaceful revolution?"" Only 5 percent answered ""yes,"" though 18 percent responded ""yes, but not that fast."" Fully 76 percent admitted to being totally surprised. These figures are all the more remarkable given the I-knew-it-would-happen fallacy-the human tendency to exaggerate foreknowledge (Baruch Fischhoff and Ruth Beyth, 1975). Yet in hindsight the revolution appears as inevitable. In each of the six countries the leadership was despised, economic promises remained unfulfilled, and basic freedoms existed only on paper. More importantly, winds of change in the Soviet Union were making Soviet intervention increasingly unlikely. But if the revolution was indeed inevitable, why was it not foreseen? What kept us from noticing signs that now, after the fact, are so plainly visible? I. Preference Falsification and Revolutionary Bandwagons Collapse"
106611,1991,aer pandp,nations and states:  mergers and acquisitions;  dissolutions and divorce,nations and states mergers and acquisitions dissolutions and divorce,ERR
106612,1991,aer pandp,the technology of conflict as an economic activity,the technology of conflict as an economic activity,No Result.
106613,1991,aer pandp,international trade in carbon emission rights:  a decomposition procedure,international trade in carbon emission rights a decomposition procedure,"In recent years, a number of proposals have been advanced for the limitation of carbon emissions. Some have argued that such limits would be costless, but our analysis suggests that there is no free lunch. (See our forthcoming paper.) We have attempted to estimate the costs but not the global benefits of slowing down climate change through carbon limitations. All computations were performed in parallel for five geopolitical regions. Except for oil trade, these regions were treated independently-as though there were no opportunity for international trade in carbon rights. For stimulating ideas on the politics and economics of negotiating an agreement on greenhouse gas emission permits, see M. Grubb (1989). Whatever rule is adopted for the allocation of carbon emission rights, there are likely to be significant interregional differences in the value of these rights. International trade will be needed if economic efficiency is to be achieved. In the absence of such trade, there are likely to be significant distortions in the comparative advantage of individual locations for the production of tradeable basic materials such as primary metals. These distortions could lead to counterproductive regulations and new forms of nontariff barriers to trade. This paper is intended to quantify the potential for international trade in carbon emission rights. Collapse"
106614,1991,aer pandp,towards a comprehensive approach to global climate change mitigation,towards a comprehensive approach to global climate change mitigation,"Global climate change, its present control and even its future reduction have been considered. Energy has been proposed as the main feed of comprehensive development. The way for meeting energy needs which certainly affects the existing energy resources will have forceful climate consequences. The purpose of this study is to explore how, in practice, the energy requirements of sustainable development can be afforded without any climate-change aggravation. Two fundamental approaches were proposed and completely discussed: the first long-term developing approach is the use of advanced energy-productive technologies instead of the existing ones and/or implementation of hybrid processes, a combination of conventional systems with a newly low energy-consumptive and emission-reductive technology in a newly installed system. The second most sustainable scheme is the development of alternative renewable energy (RE) resources. In this case, available internationally legal documents discovering the RE policies were explored and their commitment strategies were taken into original and deliberate challenges. It was concluded that the best solution to afford energy requirements is the expansion of RE sources, excluding or rarely without any sanctions reflected on the main documents of international climate policy. Since the dominant conditions of international investments have more tendencies to the short-time cost-efficient methods, the documents have only provided a limited situation for the expansion of REs. As a result, the present weak status of REs in the documents comprises their attitude towards the long-term developing scheme (the first approach) and not to the most sustainable scheme of altering the energy resources (the second approach). Collapse"
106615,1991,aer pandp,a sketch of the economics of the greenhouse effect,a sketch of the economics of the greenhouse effect,No Result.
106616,1991,aer pandp,the role of off-the-job vs. on-the-job training for the mobility of women workers,the role of off the job vs  on the job training for the mobility of women workers,"The transition from school to work is typically a period in which many young workers experience a wide range of different jobs and experience some of their most rapid wage growth over their working life. Robert Hall (1982) has estimated that the first 10 years of an individual's working career will include approximately two-thirds of all lifetime job changes. Robert Topel and Michael Ward (1988) found that over half of young male new entrants held six or more jobs over the first 10 years of their work experience. Only one young male worker in twenty held a single job for 10 years in their sample. All of this suggests that young workers' early years in the labor market involve several employment transitions. The purpose of this paper is to examine for young workers in their first years of work the determinants of leaving an employer. In particular, this paper focuses on the role of different types of training on the probability of leaving an employer. In previous work (1990) I have examined the impact of private-sector training on the determination of wages and wage growth of young workers and reached the following conclusions. First, formal company-provided onthe-job training (ON-JT) appears to be highly firm-specific in the United States and, therefore, is not portable from employer to employer. Company-provided training raises wages in the current job but has no effect on the wages earned in subsequent employment. Second, formal off-the-job training (OFF-JT) received from ""for-profit"" proprietary institutions has little effect on the wages earned on the current job, but it does raise the expected wage in subsequent employment. Finally, there are important differences by race, gender, and education level in the probability of receiving different types of formal training and in the impact this training has on wages and wage growth. These findings have several implications for the impact of training on mobility. One implication is that if company-provided training is primarily firm-specific, then the probability of leaving an employer should decline if a young worker has experienced some ON-JT. An additional implication is that if a worker participates in an OFF-JT program, it appears that the worker should be more likely to leave the current employer. In this case, OFF-JT allows a young worker to change career paths and find a ""better match."" Using data from the National Longitudinal Survey Youth (NLSY) cohort, this paper examines in detail the factors that influence the probability of new entrants leaving their first job, including the differential effects of company-provided training, apprenticeships, and training from for-profit proprietary institutions. Collapse"
106617,1991,aer pandp,the impact of nonmarket work on market wages,the impact of nonmarket work on market wages,ERR
106618,1991,aer pandp,gender differences in labor market effects of alcoholism,gender differences in labor market effects of alcoholism,"Little is known about the role of specific health problems in affecting labor market productivity. Even less is known about gender differences in the labor market effects of such health problems. Current knowledge of health effects is based largely on samples composed exclusively of men, a common practice in both economics and health research. In the latter case, even a congressional mandate to incorporate females in study samples is reputed to have had little effect (Patricia Schroeder, 1990). In this study, we attempt to determine the structure of gender differences in labor market responses to alcoholism. We use a relatively new data source that allows such comparisons by gender in a large community-based sample. Previous studies have established that there are significant gender differences in labor market behavior. Differences in prevalence rates of alcoholism by gender are also well established. It is estimated that 3 percent of females are currently suffering from alcoholism and twice that many have exhibited symptoms at some time; for males the numbers are 10 and 20 percent, respectively. There is some medical evidence to indicate that physiologically, women and men respond differently to alcohol. For example, a recent study suggests that women have greater vulnerability to the acute and chronic health conditions associated with alcoholism (Mario Frezza et al., 1990). Collapse"
106619,1991,aer pandp,europe post-1992:  internal and external liberalization,europe post 1992   internal and external liberalization,"INTRODUCTION: INSIDE - OUTSIDE Whereas other aspects of the transition towards democracy (and market economies) are studied quite extensively (and although there has been a gradual change in this regard in the mid-1980s), the international dimension of this process is still an under-researched area; when it comes to comparative and quantitative research it is even largely left out. This situation is rather paradoxical in view of ""the salience of international factors in this process"" (Pridham/ Herring/Sanford 1994: 2) and the observation that democracy ""does not happen in an international vacuum"" (Di Palma 1990: 183). This paradox is due to the intrinsic complexity of international relations and the wide range of external factors which make it difficult to assess the influence and importance of external factors and to establish unequivocal causal relationships (Pridham 1994: 11). A glimpse at the empirical reality shows the variety and the magnitude of external factors: (A) In both the Federal Republic of Germany and Japan international politics was at the beginning of democratization. Here, one can think of the military defeat in World War II and the determination of the victorious and democratic countries, foremost the United States of America, to create and substantially support democratic political systems and governments in these countries in the face of the emerging antagonism vis-a-vis the Soviet Union. Alfred Stepan (1986: 71f.) coined the term externally monitored installation to describe these events. (B) Taiwan and South Korea are countries in which democratization can--at least partly--be attributed to the direct and indirect effects of an export-oriented strategy of development. This strategy involved a thorough integration into the world market. As a result, the number of the political and economic elite's as well as the urban population's external contacts with democracies markedly increased (university exchange programs, studies in foreign countries, business and trade relations, cultural exchange). These contacts provided the basis for a growing realization that membership in the club of western industrialized countries (the OECD) would be facilitated by the liberalization and the democratization of one's own political system (Diamond 1992: 121). (C) In the case of the democratic transitions and consolidations in Southern Europe, the regional structure was conducive to and promoted democratization. The European Community required democratic political systems as a ticket to EC membership; Brussels not only provided economic incentives, but, at times, also resorted to political pressure to initiate democratic reforms. (The ""freezing"" of the Greek application for membership after the military takeover in 1967 is a case in point.) Once a member of the EC, this membership furthered the democratic consolidation in Greece, Portugal and Spain (Pridham 1991). (D) By contrast, at times, the regional structure and, particularly, the policies of the United States, the hegemonic power, were adverse to democratization in a number of Latin American countries. Under the impact of the east-west-conflict and the global system confrontation with the USSR, Washington often perceived democratic mass movements in Latin America as potential or likely intrusion targets for communist action. To prevent the domino theory to become true, the US conducted various secret service operations, e.g. in Guatemala 1954, in Brazil 1964, in Chile 1973 and in Nicaragua 1984 (Forsythe 1992). In turn, after the end of the east-west-conflict, there is greater political space for democratic mass movements (Karl 1990: 15f.). It is obvious from these cases that it is very difficult to translate the international dimension into hard empirical data and variables; since he could not identify a ""reliable empirical indicator for that purpose,"" Tatu Vanhanen (1997: 161), for example, was persuaded to leave out an analysis of the significance of external factors and power resources in his recent study on the perspective of democracy in more than 170 states. … Collapse"
106620,1991,aer pandp,the challenges of german unification for ec policymaking and performance,the challenges of german unification for ec policymaking and performance,"Rarely have economists been treated to an economic ""experiment"" comparable to the one of German reunification. In less than a year, a single political and economic entity is being fused from two economies with fundamentally different underlying principles of economic organization and substantially different levels of economic development. (See Horst Siebert, 1990, and David Begg et al., 1990.) Nonetheless, major regional disparities between western and eastern Germany (what were the Federal Republic of Germany (FRG) and German Democratic Republic (GDR)) will undoubtedly continue for many years. Since the fall of the Berlin Wall on November 9, 1989, both German Monetary Union (GMU) and political reunification were realized on July 1 and October 3, 1990, respectively. In this process, the advice of economists was often ignored in favor of political imperatives (see Roland Vaubel, 1990). Concerned with the potential impact of German Economic and Monetary Union (GEMU) on the process of European economic and political integration, many European Community (EC) policymakers have perceived German reunification as adding impetus to the ""1992"" program. Nonetheless, such initial reactions to GEMU have generally been formulated with only fragmentary information regarding the subsequent economic costs and policy measures, necessitated by the real resource transfer in order to achieve the German government's stated objective of equalizing living standards between western and eastern Germany. To the extent that GEMU generates positive or negative spillover effects to other EC countries, it presents challenges that can potentially influence the speed and degree of European economic integration. This paper examines economic implications of GEMU for the European integration process. In so doing, it makes reference to the burgeoning literature on the economic consequences of German reunification. Consideration of statistical information related to the performance of eastern and western Germany since June 1990 permits some evaluation of the validity of certain hypotheses underlying earlier studies of GEMU, as well as an assessment of likely consequences for EC policymaking and performance. Nonetheless, it is contended that the most provocative implications of GEMU result from the specific case it provides for analyzing the costs involved in integrating the fundamentally different centrally planned economies (CPE) of Eastern Europe within a viable European ""economic space."" In this sense, the German reunification ""experiment"" offers not only the remarkable opportunity to measure the economic costs associated with the total reorganization and restructuring of a CPE to a free-market economy, but also to assess the contribution of government policy measures in this conversion process. Yet, certain unique characteristics of GEMU limit the validity of lessons from the German experience for understanding the adjustment processes in other Eastern European economies. Collapse"
106621,1991,aer pandp,integration of eastern europe into the world trading system,integration of eastern europe into the world trading system,"The sheer size of mandated trade among members of the Council for Mutual Economic Assistance (CMEA), and its composition and quality, means that its reorientation toward other markets entails a whole complex of structural adjustment policies. To be successful, policy reform must be comprehensive, with clarity of purpose and predictability of action. Nevertheless, while gradualism should not be used as an excuse for delay, reforms must be harmonized with the timetable of requisite institutional change. In any case, reform must be accompanied by trade liberalization to help break down domestic monopolies and to gain the efficiencies from division of labor. Collapse"
106622,1991,aer pandp,industry restructuring in east-central europe: the challenge and the role for foreign investment,industry restructuring in east central europe  the challenge and the role for foreign investment,"Two key reforms underpin the moves toward more market-oriented economies in East-Central Europe (ECE): price reforms that allow a freer alignment of relative prices toward international norms; and industry reforms that increase competition, flexibility, and efficiency. These reforms are linked; only competitive firms responding to rational price signals can create an industrial structure able to withstand the rigors of international competition. Collapse"
106623,1991,aer pandp,czechoslovakia:  recent economic developments and prospects,czechoslovakia recent economic developments and prospects,"Czechoslovakia provides a unique example of a country that became underdeveloped as a result of an externally imposed system. Before World War II, Czechoslovakia was a democracy, with GNP per capita similar to that of Belgium and Austria. Its industries were on the technological edge and its products were known worldwide for their superb workmanship. By 1990, Czechoslovak GNP per capita is estimated by the World Bank at $3,300, thus being comparable to that of Venezuela, Gabon, and Yugoslavia, but only slightly above one-fifth of that of Austria and Belgium.1 Most Czechoslovak products are now of mediocre quality and selling at a discount, if at all, in the West. This remarkable transition occurred over approximately 40 years. During the postWorld War II reconstruction, the country was still run as a market economy, although major parts of industry, banking, and insurance were already nationalized. After the 1948 Communist takeover, Soviet-type economic planning was imposed, the remaining private enterprises were nationalized,2 and priority was given to heavy industry. Czechoslovak foreign trade was reoriented from world markets toward Soviet bloc countries. The Czechoslovak government adhered to the Soviet-type planning system faithfully throughout the 1950's. The economic slowdown in the early 1960's led to a series of reform attempts, that culminated during the Prague Spring of 1968 with a short-lived and partial program of price liberalization, separation of economic policy from political decision making, enterprise autonomy, and workers' participation in enterprise management. However, central planning was reimposed after the 1968 invasion and remained virtually intact until the late 1980's. Collapse"
106624,1991,aer pandp,economic development in yugoslavia in 1990 and prospects for the future,economic development in yugoslavia in 1990 and prospects for the future,ERR
106625,1991,aer pandp,the economic integration of post-wall germany,the economic integration of post wall germany,"Almost two years after the fall of the Berlin wall, Germany's Ministry of Research and Technology (BMFT) is still struggling to merge R&D institutes in the unified nation. But while government officials pore over financing and integration schemes, many East German research groups are beginning to disintegrate. Tight money, cumbersome restructuring plans, questionable recruitment practices, and the politics of the Western industry and scientific communities are hampering the integration efforts. Experts now agree that the future for many East German research institutes is uncertain. Germany posted a deficit of DM66.4 billion ($40.5 billion) in 1991 due to the mammoth financial burden of promoting economic recovery in the five new federal states in eastern Germany. On top of financial constraints to prevent greater deficitary bloating, the resources of research minister Heinz Riesenhuber, who must foot much of the bill to remold the crumbling infrastructure of East German research, are already spread thin. Riesenhuber is a staunch supporter of space research. His involvement in the European Space Agency and various space projects, including the Ariane rocket system and the Hermes space shuttle, have almost absorbed the entire annual DM1.54 billion increase in his research budget this year. He is under more pressure than ever to reassess his priorities. The minister has negotiated a 9.7 percent increase in his 1992 budget to DM9.25 billion. He has agreed to contribute DM1.6 billion of the total to the new federal states. But, in the process, he has frozen funds for the large West German research centers, a move that could force them to cut several of their programs and block their own efforts to integrate East German R&D. Equally disturbing, state and federal officials agreed in September that the present scheme to finance the restructuring of R&D in eastern Germany was not working and must be changed. At the heart of the problem, they said, was the so-called ""blue list,"" a select group of research institutes that were to receive half their funds from the BMPT and the other half from state budgets. The scheme, widely used in western Germany to fund such research groups as the Fraunhofer Society, Max Planck Society and Heinrich Herz Institute, has failed in the east. The reason: unification costs have stretched the budgets of the new federal states to the limits. The states say they are DM63 million short of the money needed to meet their DM180 million share for the blue list institutes. To avoid a full collapse of East German research, the Bonn government has now agreed to add DM60 million to its two-year educational reform program for the blue list group. Consequently, the states are now expected to provide 35 percent instead of 50 percent of the funding for the institutes. However, some critics say the promised funds may come too late. Bias, Haste and Bureaucracy A number of scientists in the east charge that bias, haste and bureaucracy, coupled with efforts by West German industry and research bodies to exploit the situation, have grossly handicapped the integration process. Many say integration has already robbed eastern Germany of some of its best scientists and research teams. The problem stems from Germany's unification treaty, which dissolved the Soviet-inspired Academy of Sciences in East Germany. The treaty called for a government assessment of East German research and a plan to restructure it. The task fell to the Science Council, a federal science policy advisory body, based in Cologne. In July, the Cologne group completed--at a cost of DM3 million--a comprehensive survey of the academy's 130 non-university institutional offshoots. It concluded that less than half of the institutes--primarily those with complementary programs to institutes in the West or with research in new areas--should be maintained. These included scientific centers with basic research in physics, medicine and photonics. ? Collapse"
106626,1991,aer pandp,chinese enterprise behavior under the reforms,chinese enterprise behavior under the reforms,"With persisting slower growth worldwide and in China, over-capacity in some heavy industry sectors, declining profitability, and intensifying competition from other, lower-cost emerging economies, corporate behavior in China needs to change and focus more on efficiency and sustainability. A larger proportion of firms, including state-owned enterprises, should improve corporate governance practices. To this end, fraudulent corporate practices must be halted and State assets need to be better managed. Reforms are under way or envisaged that will help improve corporate performance and, more broadly, deliver more resilient and environmentally sustainable growth and continuing progress in living standards. Collapse"
106627,1991,aer pandp,why has economic reform led to inflation?,why has economic reform led to inflation,"Productivity growth has an important linkage with economic growth and standard of living. It increases output and also improves the efficiency of firms/industries through better utilization of factors of production. This study estimates firm level productivity in cement industry in India and examines the impact of trade liberalization. The period of the study is from 1991-1992 to 2012-2013 which is sub-divided into two sub-periods, i.e., from 1991-1992 to 2001-2002 and from 2002-2003 to 2012-2013. Productivity has been estimated using Levinshon-Petrin (gross revenue case) method. The firm levels productivity growth (semi-log trend rate of growth) in cement industry in balanced and unbalanced panels are negative 0.14 per cent and negative 0.01 per cent respectively. A significant difference in productivity growth among large, medium and small scale firms is found. Trade liberalization has led to decline in input-tariffs, output-tariffs and effective rate of protection. Increase in trade openness led to increased productivity. The study shows positive impact of increase in construction sector growth rate and decline in rate of interest and inflation rate on firm level productivity in this industry. Collapse"
106628,1991,aer pandp,economic reform of the distribution sector in china,economic reform of the distribution sector in china,No Result.
106629,1991,aer pandp,shareholder heterogeneity:  evidence and implications,shareholder heterogeneity evidence and implications,"The perfect market paradigm provides a powerful foundation for financial theory. In perfect capital markets, there are no transaction costs, all traders have equal and costless access to information, and traders act as price takers. If existing claims ""span"" the state space, excess supply curves are perfectly elastic. Moreover, differences in preferences or beliefs do not result in disagreement among shareholders about firm policies. Underlying this unanimity is the shared valuation of the stock, which translates into agreement about firm strategies. The ability to transact without affecting the market price is central to many important propositions, including the ModiglianiMiller irrelevance theorems. This paper examines the nature of supply curves for corporate equity. Until recently there has been little direct empirical assessment of their elasticity. At issue is whether or not the supposition of shareholder homogeneity of valuations (and its implications) represents a good approximation to actual markets. This paper's call for further empirical evaluation of shareholder valuations echoes the perspective offered by Eugene Fama and Merton Miller, who in discussing perfect markets observed that Collapse"
106630,1991,aer pandp,investor diversification and international equity markets,investor diversification and international equity markets,"The benefits of international diversification have been recognized for decades. In spite of this, most investors hold nearly all of their wealth in domestic assets. In this paper, we construct new estimates of the international equity portfolio holdings of investors in the U.S., Japan, and Britain. More than 98% of the equity portfolio of Japanese investors is held domestically; the analogous percentages are 94% for the U.S., and 82% for Britain. We use a simple model of investor preferences and behavior to show that current portfolio patterns imply that investors in each nation expect returns in their domestic equity market to be several hundred basis points higher than returns in other markets. This lack of diversification appears to be the result of investor choices, rather than institutional constraints. Collapse"
106631,1991,aer pandp,window dressing by pension fund managers,window dressing by pension fund managers,"This paper takes a first look at investment strategies of managers of 769 pension funds, with total assets of $129 billion at the end of 1989. The data show that managers of these funds tend to oversell stocks that have performed poorly. Relative sales of losers accelerate in the fourth quarter, when funds' portfolios are closely examined by the sponsors. This result supports the view that fund managers ""window dress"" their portfolios to impress sponsors and suggests that managers are evaluated on their individual stock selections and not just aggregate portfolio performance. Collapse"
106632,1991,aer pandp,the rationality struggle:  illustrations from financial markets,the rationality struggle illustrations from financial markets,"The May 2009-10 budget handed down by the Australian Federal Government aimed to manage 'the worst global recession since the Great Depression' (Taylor 2009). The budget reinforced the strategy that the relatively new Labor Government had begun the previous year to stimulate the economy and address the global financial crisis which has bitten deeply into the Australia economy and economies around the world. Similarly, other national governments have been struggling to manage their national economic destiny in the grip of the crisis. This state of affairs has catalysed reflection from left and right about the neoliberal approach to economic management and the appropriate role of markets. The Great Depression has been an important reference point for discussion about the current global financial crisis in two main respects. Firstly, the extent and breadth of the current crisis and how it might proceed has been compared to the Great Depression. While there have been other serious recessions, according to the International Monetary Fund, the world economy will suffer its worst year since the Great Depression (Wrought 2009). Projected increases in unemployment and reduced spending continue to erode business and consumer confidence and both employers and employees are at risk. However, the current crisis has been global in a stronger sense than the 1930s as it also involves emerging economies that have been incorporated into the global capitalist market. Secondly, like the Great Depression, there is a virtual policy vacuum within the canons of economic liberalism about how to respond. The Great Depression has shown that economies cannot count on the market to naturally rebound and post World War Il anti-recessionary policy has already been found to be flawed. In any case, a return to stronger market intervention which underpinned the Keynesian recessionary policies would have been antithetical to the prevailing direction of neoliberal economic reform prior to this recent and potentially devastating global recession (Pierson 1991, 141-165, Mishra 1990, 1-8). Neoliberal approaches to economic management began to dominate government agendas from the late 1970s onwards (Marginson 1997, 73-81). In the early 1970s, post-war enthusiasm for Keynesian macroeconomic management had begun to wane. It had not delivered guaranteed economic growth despite all its early promise, and had delivered both stagflation (simultaneous inflation and high unemployment) and social dissent over unanticipated post-material externalities. As the reinvigorated right gained political ascendancy, the belief in the efficacy of markets was strengthened although naive notions of markets as reified natural mechanisms, had been largely replaced by the recognition that market conditions had to be constructed (Harvey 2005, 1-5, 64-70). Governments of western nations embarked on microeconomic reform and at times radical policies to ensure that policy and civil society obstacles to a more marketised economy were overcome. Generally more so this occurred in English speaking countries than in Europe. At an international level, trade liberalisation strategies pursued by global financial institutions such as the World Trade Organisation, World Bank, and the Organisation for Economic Cooperation and Development, as well as numerous regional trade treaties, disciplined domestic economies towards participation in a global market comprising the relatively free flow of capital, goods and services (Mishra 1999, 5-12). For developing nations, economic aid was linked to compliance with the neoliberal economic agenda. Significantly, neoliberal economic strategy sought to place more prerogative into the hands of managers, investors and lenders, with the view that rational selfinterest should be allowed to prevail to achieve better economic decisions and outcomes for society (Harvey 2005, 64-70). The 1930s Great Depression had caught unprepared those governments adhering to classic economic liberalism, leaving political space for a new strategy that eventually emerged as Keynesianism. … Collapse"
106633,1991,aer pandp,rational addiction and the effect of price on consumption,rational addiction and the effect of price on consumption,"Legalization of such substances as marijuana, heroin, and cocaine surely will reduce the prices of these harmful addictive drugs. By the law of the downward-sloping demand function, their consumption will rise. But by how much? According to conventional wisdom, the consumption of these illegal addictive substances is not responsive to price. However, conventional wisdom is contradicted by Becker and Murphy's (1988) theoretical model of rational addiction. The Becker-Murphy (B-M) analysis implies that addictive substances are likely to be quite responsive to price. In this paper, we summarize B-M's model of rational addiction and the empirical evidence in support of it. We use the theory and evidence to draw highly tentative inferences concerning the effects of legalization of currently banned substances on consumption in the aggregate and for selected groups in the population. Addictive behavior is usually assumed to involve both ""reinforcement"" and ""tolerance."" Reinforcement means that greater past consumption of addictive goods, such as drugs or cigarettes, increases the desire for present consumption. But tolerance cautions that the utility from a given amount of consumption is lower when past consumption is greater. These aspects of addictive behavior imply Collapse"
106634,1991,aer pandp,alcohol consumption during prohibition,alcohol consumption during prohibition,"We estimate the consumption of alcohol during Prohibition using mortality, mental health and crime statistics. We find that alcohol consumption fell sharply at the beginning of Prohibition, to approximately 30 percent of its pre-Prohibition level. During the next several years, however, alcohol consumption increased sharply, to about 60-70 percent of its pre-prohibition level. The level of consumption was virtually the same immediately after Prohibition as during the latter part of Prohibition, although consumption increased to approximately its pre-Prohibition level during the subsequent decade. Collapse"
106635,1991,aer pandp,who uses illegal drugs?,who uses illegal drugs,"Gary Becker and Kevin Murphy (1988) present a theoretical model of rational addiction that requires information on past, present and future prices. They do not study illegal drugs empirically in this and their related papers. A major data source is the annual survey of high school seniors (HSS), also known as ""Monitoring the Future."" This work is summarized in L. Johnston et al. (1988). They find a growing use of drugs (measured by monthly, annual, and lifetime prevalence) over time, and differences by region and sex. Cocaine use showed marked increases from 1976, though this levelled off from 1986 to 1987. They often rely on cross-tabs that leaves many variables uncontrolled and the results subject to omitted variable bias. J. Bachman et al. (1984) use ordinary least squares (OLS) regressions in which the drug use in the three years post-high school is related to various characteristics such as living arrangements. While an improvement over cross-tabs, OLS applied to categorical dependent variables yields inefficient estimates (see M. Nerlove and S. Press, 1973). Johnston et al. used follow-up surveys of a subsample drawn from each cohort. They find the use of some drugs decline at older ages, say 35, though they do not determine if the heavy users have died, dropped out of the sample, or have been rehabilitated.' The HSS's initial restriction to high school seniors removes about 30 percent of the population who drop out of high school perhaps because of taking drugs. Studies based on the National Institute of Drug Abuse (NIDA) sample of people 12 and older show some heavy drug use of people less than 17 years old. (See J. D. Miller et al., 1982, and NIDA, 1985.) Richard Clayton (1985) using the 1980 HSS presents univariate regressions that shows the frequency of use of cocaine is positively related to lifetime marijuana use and days out of school in the past month, but negatively related to high school grade point average. H. Abelson and Miller (1985), using the NIDA surveys covering 1974-82, show differences in the percentage using cocaine by education and race-for lifetime, 12 months, and last month measures. They find strong trends. D. Kandel (1980) presents a recent survey of drinking and drug use among youth. People in their late 30's ""mature out"" of heroin use rather than die. Collapse"
106638,1991,aer pandp,the determinants of investment in new technology,the determinants of investment in new technology,"See Comment page 101 In 2004, ministers of health from around the world gathered in Mexico City for the fi rst ever global ministerial summit on health research. Resolutions from that meeting were later approved by the 58th World Health Assembly in Geneva. Since then, many organisations, countries, and donors have been working to deliver on those recommendations. In November this year, an even broader group of ministers and other stakeholders will gather in Bamako, Mali, to assess progress and determine new priorities. The 2008 Global Ministerial Forum on Research for Health will provide a unique platform for ministers of health, science and technology, and social development to discuss research with global experts and stakeholders. Refl ecting this diversity, and the broad vision of research for health, the meeting is being co-organised by a partnership involving the Council on Health Research for Development (COHRED), Global Forum for Health Research, Government of Mali, UNESCO, World Bank, and WHO. On the road to Bamako 2008, it is appropriate to refl ect on where we have been, what progress we have made, and where we wish to go in the future. In 1990, the Commission on Health Research for Development noted that only 5% of global spending on health research went to problems aff ecting the poorest 93% of the world’s people, now known as the “10/90 gap”. The Commission recommended investment in essential national health research, inter national partnerships, and mechanisms to monitor progress, and noted that “research should not be limited to the health sector, but should also examine the health impact of development in other sectors and the socioeconomic determinants of health”. The Commission’s pivotal report was followed in 1993 by the World Bank’s World development report: investing in health, and by the establishment of COHRED to promote the conduct and use of essential health research in developing countries. In 1996, WHO’s Ad Hoc Committee on Health Research Relating to Future Intervention Options identifi ed best buys for investment, including research on maternal and child health, microbial threats, health systems, noncommunicable diseases, and injuries. The Global Forum for Health Research was created in 1998 to advocate for greater invest ments in health research to meet the needs of poor people. In 2000, the Global Forum’s annual meeting was co-convened with WHO and the World Bank in Bangkok, Thailand, subsumed under an International Conference on Health Research for Development, in which COHRED had a leading organisational role. Participants emphasised the strengthening of national health research systems as a key priority to reduce the 10/90 gap. 4 years after the Bangkok meeting, the Global Forum’s eighth annual meeting was held in Mexico City in parallel with the Ministerial Summit on Health Research organised by WHO. The two overlapping events focused on research to achieve the Millennium Development Goals (MDGs). Ministers attending the summit committed to three key priorities: health-systems research, securing public confi dence in research, and bridging the gap between knowledge and action. These developments might have helped to stimulate the recent explosion of innovative new ideas and initiatives in global health. In this context, two additional phenomena deserve mention. First, some innovative developing countries (IDCs) such as Brazil, China, India, and South Africa have become important producers of low-cost drugs and vaccines. Second, donors and developing countries have both begun, rightly, to embrace science and technology as key drivers of social and economic development. So what progress have we made? Overall, this is a story of both new resources and new challenges. Global investments in research and development for health have quadrupled since 1986 (fi gure), though Collapse"
106640,1991,aer pandp,diffusion of development:  post-world war ii convergence among advanced industrial nations,diffusion of development   post world war ii convergence among advanced industrial nations,"The Stalin era is traditionally identified with the ""return"" of Russian nationalism, which was denigrated by the Bolshevik regime in its early years but later became an increasingly important element of its official discourse in relation both to the other Soviet peoples and to the ""brother countries"" of the Eastern Bloc. Unlike other authoritarian regimes that were less ideologically oriented, the Communist Party claimed to embody the movement of history and emphasized the interdependence of political action and scientific development: historical discourse, and related disciplines such as ethnology or archaeology, came under powerful ideological pressure and were subjected to frequent outside meddling. From World War II until the death of Stalin in 1953, the historiographies of the USSR's various federal entities were subjected to abrupt changes. However, despite the domination of Zhdanovism, the national historiographies--at least in Central Asia--were able to retain an interpretation of history that valorized the titular nation. The research conducted in the different republics' academies of sciences can therefore be seen as one of the matrices that permitted the symbolic justification and appropriation of the state and its territory: in its choice of historical reference points, the resulting shared sentiment of national identity would never be called into question during the entire second half of the 20th century, not even after the fall of the USSR in 1991. Autochthonism as a political and narrative matrix for the identities of the Soviet peoples is a well-known subject. Terry Martin has shown how much the Soviet Union was built on the principle of ""positive discrimination"" toward minorities, who were granted cultural and linguistic rights according to their administrative status as union republics, autonomous regions, and so on. Francine Hirsch's work on the role of ethnologists and local elites in the construction of identity referents, as well as Ronald Suny's on primordialism in Soviet national identities and Yuri Slezkine's on ""ethnophilia"" in Soviet science, have shed new light on the tight bond between the political environment, the development of the social sciences, and the articulation of discourses on identity. (1) Those studies, however, are mainly concerned with the 1920s and 1930s. The war, the postwar era, and the post-Stalinist upheavals after the mid-1950s remain little known in their impact on Central Asia. Yet it was at just this time, after the large-scale violence of Stalinism had ended, that the elites in the union republics could achieve stability, benefit from greater autonomy in publishing, and take advantage of the institutionalization of the academies of sciences. Autochthonization took shape in the form both of people--through the preference given to the titular nationality in hiring decisions in the human sciences--and of ideas, with the creation of discourses to legitimize the new republican entities. The historiographic autochthonization of the 1940s-50s that I discuss was made possible when the ""archaeological patriotism"" available to each republic joined with the conceptualization of the principle of the ""ethnogenesis"" of eponymous peoples to anchor the notion that there existed an authentic connection among a people, its territory, and the state. This ethnogenetic discourse emerged within the specific context of the late 1930s and was advanced by historians, mainly Russians, who sought to apply in the Central Asian republics the autochthonist principles that had emerged in Russian history. Their discourse had its hour of glory in the last decade of the Stalin era and is considered even now as orthodoxy in the area of ethnogenesis. After a brief introduction on the specific political context of Zhdanovism, this article discusses the diffusion of the concept of ethnogenesis in Central Asia, the carriers of this discourse, and their social space--the republican academies of sciences--before concluding with a consideration of how certain meanings have shifted through the rewriting of history by more nationalist historians in Central Asian academia. … Collapse"
106641,1991,aer pandp,diffusion of development: the soviet union,diffusion of development the soviet union,"Abstract Nowadays science applies agricultural innovations in a wide range all over the world; however, number of water users in innovations is in smaller amounts. This might happen to a number of factors, for example lack of adequate knowledge exchange system, nominal extension services at places, lack of well-defined policies, barriers in ?human? minds change?, barriers at policy level. As for Uzbekistan, it could be said that practice of extension of innovations application and its diffusion in agricultural irrigation sector in Uzbekistan does not have much experience, however, before 1991 Uzbekistan was one of the Soviet Unions? republics and as it is known, the Soviet Union had high practice in innovations in different sectors, as well as in agriculture. Although, since independence, Uzbekistan has continued to experience innovations in agricultural sector independently, their diffusion is at a challenging shape. This article captures the policy issue, how Uzbekistan started to develop water management issues in its economic reforms, it describes a case research on application of innovative technique on a farm level and accordingly, it tries to propose the aspects that need to be involved in future reforms to make the current situation be better managed. Collapse"
106642,1991,aer pandp,diffusion of development: the late-industrializing model and greater east asia,diffusion of development  the late industrializing model and greater east asia,"Diffusion of Development: The Late-Industrializing Model and Greater East Asia Author(s): Alice H. Amsden Source: The American Economic Review, Vol. 81, No. 2, Papers and Proceedings of the Hundred and Third Annual Meeting of the American Economic Association (May, 1991), pp. 282-286 Published by: American Economic Association Stable URL: http://www.jstor.org/stable/2006870 Accessed: 15/01/2010 13:30 Collapse"
106644,1991,aer pandp,immigration and wages:  evidence from the 1980's,immigration and wages evidence from the 1980s,"In Chapter 1, 1 introduce the reader to immigration in Canada. This includes the contextual environment. immigration policies and brief reviews of three types of immigrant studies: those on assimilation, those on take-up rates of transfer payments and those on the labour market impact of immigration on native-born Canadians, In Chapter II, 1 investigate the economic assimilation experienced by immigrants over the 1980s. Previous studies of the labour market experience of male immigrants to Canada have uncovered two disturbing trends: declining entry earnings for successive new immigrant cohorts, and low assimilation rates. These findings suggest that many cohorts may never assimilate. The 1991 Census provides a first look at the immigrant cohorts arriving in the 1980's. These immigrants appear to avoid the plight of their predecessors; entry earnings have stopped falling, and those immigrants arriving between 198 1 and 1985 experienced a 17% assimilation rate. 1 am unable to explain this tumaround based on the observable characteristics recorded in the census dataIn Chapter III, I explore the economic impact of immigrants' presence within the Canadian labour market on the wages of native-born Canadians and earlier immigrants. Most of the existing evidence on this question uses data from the United States and finds no impact. In this paper 1 estimate the impact of immigration on native wages using Canadian data, paying close attention to the important econometric issues, such as the potential endogeneity of immigrants' choice of location upon amival. The ""iabour market"" is defined by occupations. Two important innovations are the use of immigrant source-country charactenstics and immigrants' intended occupation at entry as instruments in predicting immigration flows to the labour market. My results consistently point to no evidence of adverse effects of immigrants on the labour market outcomes of native-born Canadians. This chapter also contains an andysis of the impact of immigration on educational and age wage differentials. Larger impacts are found on the wage differential between young and older workers. Lastly, 1 find some evidence that natives exit labour markets that immigrants enter and that new immigrants may be confined to lower paying occupations. Collapse"
106645,1991,aer pandp,"immigrants in the american labor market:  quality, assimilation, and distributional effects",immigrants in the american labor market quality assimilation and distributional effects,This paper provides evidence on immigrants performance and impact in the U.S. labor market. We document that new immigrants do bring fewer marketable skills to the United States than did earlier cohorts and that changes in the source countries of recent immigrants account for all of this decline in immigrant quality. We find no important evidence that quality has declined within immigrant ethnic groups. We also show that immigrants assimilate rapidly in the U.S. market (10 years of U.S. experience offsets most of the earnings disadvantage of new immigrants) and that assimilation is more rapid for groups who start with lower initial wages. (EXCERPT) Collapse
106650,1991,aer pandp,"institutional legacies and the economic, social, and political environment for transition in hungary and poland",institutional legacies and the economic social and political environment for transition in hungary and poland,ERR
106651,1991,aer pandp,transformation programs:  content and sequencing,transformation programs content and sequencing,"Conventional wisdom will have it that Poland opted for a dramatic, compressed dash towards market capitalism-a ""Big Bang"" that in one stroke shed central planning and dismantled bureaucratic interference in large areas of economic decision making. Hungary, it is believed, has taken a more gradual, time-consuming, less forced approach towards the same end. These perceived contrasts in strategy (""shock therapy"" vs. ""gradualism"") are keenly debated in policy reform circles throughout the region, both with respect to necessity as well as to feasibility. The conventional wisdom greatly simplifies the contrast between the two countries. Recent political developments in Hungary have generated a substantial acceleration of reform efforts. The Hungarian reform program now operates on as broad a front as the Polish program: abandonment of central controls, price liberalization, introduction of competition, privatization, financial and fiscal system modernization, development of social safety nets, legal, regulatory and institutional reform, and so on. It seeks a no less fundamental and far-reaching change. The perception of gradualism probably has more to do with the past 20 years of Hungarian experimentation with piecemeal modifications to the economic mechanism (Janos Kornai, 1986) than to the present reality. That past experience certainly suggests that without genuine political commitment to a market economy, and particularly to a broad-based system of private property rights, systemic reforms in formerly socialist economies will not succeed. Once these precepts are accepted, the specific agenda items for reform appear to take on the same character in all reforming economies. The pace and sequencing of economic and institutional changes are then dictated by initial starting conditions that differ greatly between countries (Stanley Fischer and Alan Gelb, 1990). Price liberalization and the dismantling of central coordination can be accomplished overnight; the establishment of new legal systems, privatization, the restructuring of enterprises, generating attitudinal and behavioral changes in populations, all perforce take much longer. As a result, it may be expected that the ambit for genuine choices on pace and sequencing on policy and institutional change will narrow over time and that the content of reforms in Hungary and Poland and other countries will increasingly resemble one another. Collapse"
106652,1991,aer pandp,foreign economic liberalization in hungary and poland,foreign economic liberalization in hungary and poland,"Of the former socialist countries, it is Hungary, Poland and Czechoslovakia that are the most advanced in the transition towards a market economy. Given their relatively high level of development, they are generally accorded the best chance of succeeding in this historically unprecedented endeavour. One of the challenges of their economic reforms is to abandon the state monopoly of foreign trade and reintegrate into the international market. This article has three interrelated objectives: to review the literature that shows why the reforms of external relations of former socialist countries became inevitable, to compare the reforms initiated in the three countries, and, to analyze the complications and the preliminary results of the reform process. Collapse"
106653,1991,aer pandp,"derivation of ""rational"" economic behavior from hyperbolic discount curves",derivation of rational economic behavior from hyperbolic discount curves,ERR
106654,1991,aer pandp,economic analysis and the psychology of utility:  applications to compensation policy,economic analysis and the psychology of utility applications to compensation policy,ERR
106655,1991,aer pandp,negative time preference,negative time preference,"This paper considers a number of parallels between risky and intertemporal choice. We begin by demonstrating a one-to-one correspondence between the behavioral violations of the respective normative theories for the two domains (i.e., expected utility and discounted utility models). We argue that such violations (or preference reversals) are broadly consistent with three propositions about the weight that an attribute receives in both types of multiattribute choice. Specifically, it appears that: (1) if we add a constant to all values of an attribute, then that attribute becomes less important; (2) if we proportionately increase all values of an attribute, or if we change the sign of an attribute, from positive to negative, then that attribute becomes more important. The generality of these propositions, as well as the constraints they would impose on separable representations of multiattribute preferences, is discussed. Collapse"
106656,1991,aer pandp,designing economic agents that act like human agents:  a behavioral approach to bounded rationality,designing economic agents that act like human agents a behavioral approach to bounded rationality,"Most economists accept that there are limits to the reasoning abilities of human beings-that human rationality is bounded. The question is how to model economic choices made under these limits. Where, between perfect rationality and its complete absence, are we to set the ""dial of rationality,"" and how do we build this dial setting in to our theoretical models? One approach to this problem is to lay down axioms or assumptions that suppose limits to economic agents' computational ability or memory, and investigate their consequences. This is useful, but it begs the question of how humans actually behave. A different approach (the one I suggest here) is to develop theoretical economic agents that act and choose in the way actual humans do. We could do this by representing agents as using parametrized decision algorithms, and choose and calibrate these algorithms so that the agents' behavior matches real human behavior observed in the same decision context. Theoretical models using these ""calibrated agents"" would then, we could claim, furnish predictions based on actual rather than idealized behavior. It is unlikely there exists some yet-to-bedefined decision algorithm, some ""model of man,"" that would represent human behavior in all economic problems-an algorithm whose parameters would constitute universal constants of human behavior. Different contexts of decision making in the economy call for different actions; and an algorithm calibrated to reproduce human learning in a search problem might differ from one that reproduces strategic-choice behavior. We would likely need a repertoire of calibrated algorithms to cover the various contexts that might arise. Nevertheless, for a particular context of decision making, calibrating theoretical behavior to match human behavior would allow us to ask questions that are not answerable at present under the assumption of either perfect rationality or idealized learning. We might want to know whether a given neoclassical model with human agents represented by ""calibrated agents"" will result in some standard asymptotic patterna rational-expectations equilibrium, say. We might ask whether agents calibrated to learn as humans do converge to some form of optimality, or interactively to a Nash equilibrium.' And we might want to study the speed of adaptation in a particular economic model with human agents represented by calibrated agents. What would it mean to calibrate an algorithm to ""reproduce"" human behavior? The object would be algorithmic behavior that reproduces statistically the characteristics of human choice, including the distinctive errors or departures from rationality that hutDiscussants: Ken Binmore, University of Michigan; Drew Fudenberg, MIT; John Geanakoplos, Yale University. Collapse"
106657,1991,aer pandp,experiments on stable suboptimality in individual behavior,experiments on stable suboptimality in individual behavior,"SFI WORKING PAPER: 1991-02-015 SFI Working Papers contain accounts of scientific work of the author(s) and do not necessarily represent the views of the Santa Fe Institute. We accept papers intended for publication in peer-reviewed journals or proceedings volumes, but not papers that have already appeared in print. Except for papers by our external faculty, papers must be based on work done at SFI, inspired by an invited visit to or collaboration at SFI, or funded by an SFI grant. ©NOTICE: This working paper is included by permission of the contributing author(s) as a means to ensure timely distribution of the scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the author(s). It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright. These works may be reposted only with the explicit permission of the copyright holder. www.santafe.edu Collapse"
106658,1991,aer pandp,artificial adaptive agents in economic theory,artificial adaptive agents in economic theory,"Economic analysis has largely avoided questions about the way in which economic agents make choices when confronted by a perpetually novel and evolving world. As a result, there are outstanding questions of great interest to economics in areas ranging from technological innovation to strategic learning in games. This is so, despite the importance of the questions, because standard tools and formal models are ill-tuned for answering such questions. However, recent advances in computer-based modeling techniques, and in the subdiscipline of artificial intelligence called machine learning, offer new possibilities. Artificial adaptive agents (AAA) can be defined and can be tested in a wide variety of artificial worlds that evolve over extended periods of time. The resulting complex adaptive systems can be examined both computationally and analytically, offering new ways of experimenting with and theorizing about adaptive economic agents. Many economic systems can be classified as complex adaptive systems. Such a system is complex in a special sense: (i) It consists of a network of interacting agents (processes, elements); (ii) it exhibits a dynamic, aggregate behavior that emerges from the individual activities of the agents; and (iii) its aggregate behavior can be described without a detailed knowledge of the behavior of the individual agents. An agent in such a system is adaptive if it satisfies an additional pair of criteria: the actions of the agent in its environment can be assigned a value (performance, utility, payoff, fitness, or the like); and the agent behaves so as to increase this value over time. A complex adaptive system, then, is a complex system containing adaptive agents, networked so that the environment of each adaptive agent includes other agents in the system. Complex adaptive systems usually operate far from a global optimum or attractor. Such systems exhibit many levels of aggregation, organization, and interaction, each level having its own time scale and characteristic behavior. Any given level can usually be described in terms of local niches that can be exploited by particular adaptations. The niches are various, so it is rare that any given agent can exploit all of them, as rare as finding a universal competitor in a tropical forest. Moreover, niches are continually created by new adaptations. It is because of this ongoing evolution of the niches, and the perpetual novelty that results, that the system operates far from any global attractor. Improvements are always possible and, indeed, occur regularly. The everexpanding range of technologies and products in an economy, or the everimproving strategies in a game like chess, provide familiar examples. Adaptive systems may settle down temporarily at a local optimum, where performance is good in a comparative sense, but they are usually uninteresting if they remain at that optimum for an extended period. A theory of complex adaptive systems based on AAA makes possible the development of well-defined, yet flexible, models that exhibit emergent behavior. Such models can capture a wide range of economic phenomena precisely, even though the development of a general mathematical theory of complex adaptive systems is still in its early stages.' The AAA models complement current theoretical directions; they are Collapse"
106700,1991,applied economics,occupational segregation and selectivity bias in occupational wage equations: an empirical analysis using irish data,occupational segregation and selectivity bias in occupational wage equations an empirical analysis using irish data,"The gender wage effects of occupational segregation are assessed within the framework of occupational attachment and wage equations. This paper exploits the modified ‘index number’ approach suggested by Brown et al.(1980) and used recently by Miller (1987). However, neither of these papers allowed for the potential effects of selectivity bias in occupational wage equations. In this study selectivity bias is corrected for using the method outlined in Lee (1983). This allows for the effects of occupational selectivity buas on the gender wage gap to be assessed. The findings for this sample of young lrish workers suggest that the greater part of the female wage disadvantage lies in intraoccupational wage differences. Furthermore selectivity bias is found to be important for a number of occupational categories. Collapse"
106702,1991,applied economics,efficiency and farm size in egypt: a unit output price profit function approach,efficiency and farm size in egypt a unit output price profit function approach,"The main objective of this study is to analyse size and efficiency in Egyptian agriculture using an econometric approach, free of the problem of simultaneous equation bias, associated with the production function technique. The identification of factors which affect farm level productivity in the study area are also considered. The results reveal that the efficiency of management in both large and small farms is the same; and that large farms appear not to be more efficient than small farms. The study also indicates that increasing the ulitilization of cultivated land and machinery service and raising the output price are important if production is to increase. Collapse"
106703,1991,applied economics,government borrowing and tax-adjusted real and nominal interest rates,government borrowing and tax adjusted real and nominal interest rates,"The purpose of this paper is to test for the effect of Federal Government budget deficits on one-year tax-adjusted nominal and real interest rates in an IS-LM-AS model. The results suggest that the specification of the dependent variable is a crucial issue in the debate over the linkage between deficits and interest rates, The evidence shows a positive and significant effect of various measures of the Federal Government deficit and debt on the real tax-adjusted Treasury bill rate in both the levels and the first-differenced equations but not on the nominal tax-adjusted rate. The positive and significant deficit-debt coefficients are also confirmed when the first-differenced equation is estimated by instrumental variables and when the equation is estimated over various time periods. Thus, there is robust evidence of a positive linkage between federal government borrowing and the tax-adjusted real but not nominal interest rates. Collapse"
106704,1991,applied economics,volatility and time-varying risk premiums in the stock market,volatility and time varying risk premiums in the stock market,"The stock returns generating process is analysed using alternative specifications of the autoregressive conditional heteroscedasticity model. Empirical anaylysis using monthly data from the Finnish stock market shows a clearly significant persistence of volatility of stock prices.In addition,The presence of a time-varying risk premium in the stock returns is also clearly supported by the data. This suggests a cautious attitude towards traditional, standard empirical analysis of capital markets, which is based on the assumption of time-invariant risk premiums. The results obtained do not support the view that the risk premiums have increased during the 1980s, when instability has been the main characteristic of most financial markets. Collapse"
106708,1991,applied economics,an econometric investigation of capital flight,an econometric investigation of capital flight,No Result.
106711,1991,applied economics,the relationship between concentration and prices and concentration and costs,the relationship between concentration and prices and concentration and costs,"Abstract A comparison has been made between the changes in absorption spectra and chlorophyll fluorescence emission occurring upon the induction of non-photochemical dissipation of excitation energy (qE) in isolated thylakoids and those accompanying the aggregation of detergent-solubilised spinach light-harvesting complex (LHCII). In support of a recent hypothesis for the mechanism of qE (Horton et al. (1991) FEBS Lett. 292, 1–4), it was found that absorbence changes at 530 nm were associated with qE and LHCII aggregation. Antimycin A inhibited these changes and prevented LHCII aggregation, as indicated by the electrophoretic mobility of the complex and its low-temperature fluorescence spectrum. An antimycin-insensitive partial aggregation of LHCII was associated with an absorbance change at 505 nm. Low concentration of detergent caused disaggregation of LHCII and the reversal of qE. These data are discussed in terms of the relationship between structural change in LHCII and the mechanism of non-photochemical quenching of chlorophyll fluorescence in thylakoids. Collapse"
106713,1991,applied economics,the reaction of stock returns to anticipated and unanticipated changes in money: a re-examination of the evidence in the frequency domain,the reaction of stock returns to anticipated and unanticipated changes in money  a re examination of the evidence in the frequency domain,"The relationship between money and stock returns is analysed for the 1977–1985 period. Spectral analysis and a VAR (vector autoregressive) model are used in the analysis. The preliminary results suggest a positive relationship between stock returns and money supply along a business cycles and high frequency cycles. A detailed analysis, however, suggests the spurious character of this observed relationship. The money/stock returns relationship over business cycles can be explained by a contemporaneous adjustment of stock returns and anticipated money in response to business cycle anticipations. The high frequency relationship, on the other hand, can be related to contemporaneous adjustment of stock returns and money anticipations in response to expected inflation in a framework characterized by the anticipation of the Federal Reserve Reaction Function. The empirical results strongly suggest that both stock returns and money anticipations are determined in a forward-looking manner incorporating the informat... Collapse"
106714,1991,applied economics,the relationship between the brazilian interest payments moratorium and bank equity returns: evidence from the london stock exchange,the relationship between the brazilian interest payments moratorium and bank equity returns evidence from the london stock exchange,"This study examines the effects of the Brazilian interest payments moratorium announcement on the equity return levels of several large US and European commericial banks traded on the London Stock Exchange. The empirical evidence suggests that, in general, the equity prives of the sample banks immediately reflected the relevant information associated with the announcement. However, some degree of pricing inefficiency was determined to exist, as the market was unable to discriminate among banks of the basis of exposure to Brazilian loans. Collapse"
106715,1991,applied economics,the performance of exchange rate forecasting models: an economic evaluation,the performance of exchange rate forecasting models an economic evaluation,"Theoretical models of exchange rate determination have not proven useful as accurate predictors of movements in exchange rates. However, these conclusions are the result of evaluation on the basis of statistical criteria of performance; implicitly assuming such criteria are optimal in the subsequent economic decision framework. This assumption is suspect, though, because statistical criteria do not account for market timing is applied in this study to four forecasting models of both the Canadian dollar/US dollar and the West German mark/US dollar exchange rates over the period 1976:11–1984:8. Of these models, only the unconstrained static and dynamic forecasting models exhibited significant markete timing value with respect to the Canadian dollar and then only over a one month time horizon. The constrained dynamic model had market timing abillity at the three month time horizon under Decision Rule I. With respect to the West German mark, the constrained static forecasting model showed market timing abilit... Collapse"
106716,1991,applied economics,ex ante valuation of atmospheric visibility,ex ante valuation of atmospheric visibility,"In a world incomplete markets for environmental goods, ex ante planned expenditures rather than ex post realized outcomes explain the values which individuals attach to these goods. We use the distance function to develop restrictions from a model of ex ante consumer behaviour involving these goods. A Contigent valuation approach is then employed to estimate policy-relavant components of the ex ante economic values that recreationists attach to differing subjective probabilities of alternative atmospheric visibility levels at a wilderness location and at an urban location in Oregon. Marginal valuations of probability changes are similar at the two sites. Marginal rates of time preference for resolving uncertainities vary between 10 and 50% and are inversely related to education. Existence value averages 10% of total value, and evidence is mixed that site-specific value act as surrogates for general environmental preferences. Collapse"
106717,1991,applied economics,precautionary savings under income uncertainty: a cross-sectional analysis,precautionary savings under income uncertainty  a cross sectional analysis,"The first part of this paper derives a closed fro solution for present consumption as a function of current and expected economic variables, which contains a precautionary component that is directly affected by future income risk. An estimating equation forthe structural relationship between savings and their determinants is discussed and tested using cross-sectional data from the 1984 UK Family Expenditure Survey. Collapse"
106718,1991,applied economics,absenteeism in a medium-sized manufacturing plant,absenteeism in a medium sized manufacturing plant,"As a result of its high density and durability, wood of Myracrodruon urundeuva (aroeira) is highly valued, and its exploitation in Brazilian primary forests has been prohibited by law since 1991. One way to maintain this status and, at the same time, supply the increased demand for this wood is through the establishment of plantations. Therefore, the knowledge of M. urundeuva wood properties in plantations with young trees is essential. Thus, we evaluated we evaluated some physical and mechanical properties of wood from 20-year-old M. urundeuva (aroeira) twelve trees in a homogenous plantation. Basic density was 770 kg m-3 and apparent density was 984 kg m-3. The anisotropy coefficient of 1.69 is considered regular, indicating that wood can be used in the manufacture of fine furniture, frames, boats, musical instruments, sports equipment or flooring. The wood presented good mechanical behavior, with an average strength for parallel compression of 53.84 MPa, MOE of 12632 MPa and MOR of 123.58 MPa. The wood was identified as class 40, thus showing potential for use in medium-sized structures and construction. Basic density and apparent density correlated positively with compression parallel to the grain and MOR. Wood quality, a positive characteristic, added to good technology, even in young trees, shows that values of physical and mechanical properties are suitable for various uses when compared to the same values of older trees based on specialized literature. Our results confirmed that this homogeneous planting was adequate to obtain M. urundeuva wood with such quality. Collapse"
106720,1991,applied economics,"customer preferences, attendance and the racial structure of professional basketball teams",customer preferences attendance and the racial structure of professional basketball teams,"This paper examines the importance of customer-based discrimiantion in professional basketball over the 1980–81 to 1985–86 seasons. Empirical rsesults offer strong support of the hypothesis that, if fans prefer to see players of their own race, teams will be influenced in their selection of players by the racial composition of their market area. By the same token, attendance is found to be positively related to the extent to which the racial composition of the team matches that of the corresponding market area. Collapse"
106721,1991,applied economics,forecast performance of exchange rate models revisited,forecast performance of exchange rate models revisited,"This paper re-evaluates the performance of reduced form exchange rate models by updating the Messe-Rogoff study (1983). This paper confirms earlier tests showing that simple monetary models do not perform well, but it finds more positive results for other monetary models that incorporate more dynamic econometric specifications. A simple error correction monetary model out-forecasts a random walk almost half of the time. Collapse"
106722,1991,applied economics,post-secondary vocational education and on-the-job training,post secondary vocational education and on the job training,No Result.
106724,1991,applied economics,"unions, wages and productivity: some evidence from u.k. engineering firms",unions wages and productivity some evidence from uk engineering firms,"Estimates of union wage and productivity effects are derived using primary micro-level panel data for a sample of firms in the UK engineering industry. Union wage differentials of the order of 10% are suggested from the results, whereas union productivity impacts appear to be non-linear with respect to union density. Collapse"
106726,1991,applied economics,"interactions between hospital admissions, cost per day and average length of stay",interactions between hospital admissions cost per day and average length of stay,"Hospital cost per day (COST), annual admissions per capita (ADMIT), and average length of stay (ALOS) are important health policy variables because each variable provides important information about hospital industry performance. This study uses country-level data to explore the empirical relationships between COST, ADMIT, and ALOSin three steps: OLS estimation, application of the Wu-Hausman endogeneity test, and re-estimation with two-stage least squares. The major results are (i) ADMITand ALOSexert exogenous influences on COSTand (ii) ADMITand ALOSare endogenously related with possible one-way casuality from ADMITto ALOS. Collapse"
106727,1991,applied economics,a new approach to determining sectoral priorities in an economy: input-output elasticities,a new approach to determining sectoral priorities in an economy  input output elasticities,This paper uses input–Output elasticities to identify important economic sectors. Elasticities of output employment and income are used to identify key sectors of the Greek economy. A comparison of the rankings of economic sectors based on input–output elasticities with those based on net backward linkages indicates significant divergence in sectoral rankings obtained from the two approaches. The elasticity approach yields more consistent estimates of sectoral output employment and income potentials than the net backward linkage approach. Measured in terms of the potential to generate output employment and income agriculture services and textiles are found to be the key sectors for the Greek economy. Collapse
106728,1991,applied economics,monetary policy and inventory investment in greek manufacturing industry,monetary policy and inventory investment in greek manufacturing industry,"The paper reports an attempt to estimate the role of Greek monetary policy in inventory investment in manufacturing. Interest and inflation rate expectations exercise a significant influence. Expected fluctations in the exchange rate, for the first time examined, are of major importance especially for stocks of raw materials which are mostly imported. Being strongly affected by monetary variables, inventory investment can play a major role as a transmission channel of monetary policy changes to the Greek economy. Collapse"
106729,1991,applied economics,"international technological knowledge differences and economic growth comparisons: usa versus west germany and sweden versus norway, 1963-1988",international technological knowledge differences and economic growth comparisons  usa versus west germany and sweden versus norway  1963 1988,"The main interest in this paper is governed by the methodology for making evolutive growth comparisons in the international context. The paper analysis differences in levels, and changes of these levels of aggregate output between the above mentioned countries, in terms of differences in factors input and levels of technological knowledge, for the period 1963–1988. Sources of economic growth in each country are also examined on an annual and average annual basis. Th conclusion is that technological knowledge differences between countries hold a central role in explaining output differences between them. A slight overtake of the technological lead is observed in comparing Sweden and Norway between 1977 and 1985 when a recovery by Sweden begining in 1986 is detected. A certain part of the output differences between the US and West Germany is explaining by the differences in technology between these two countries. However, capital intensity in the US is found to be almost double in 1988, and this accounts fo... Collapse"
106730,1991,applied economics,economies of superscale in commercial banking,economies of superscale in commercial banking,No Result.
106731,1991,applied economics,characteristics of west german demand for international tourism in the northern mediterranean region,characteristics of west german demand for international tourism in the northern mediterranean region,"The purpose of this paper is to establish the economic determinants of West Germany's demand for international tourism in six northern Mediterranean nations. The methodology makes use of the translog utility function. Homotheticity and additivity restrictions of the utility theory are tested as hypotheses. Expenditure (income), own and cross-price elasticities are claculated for each of the twenty years from 1966 to 1985. Collapse"
106734,1991,applied economics,the stability of cigarette demand,the stability of cigarette demand,"The photograph on the cover of Susan Seymour’s biography of anthropologist Cora Du Bois (1903–1991) presents an image of an intense woman, a cigarette hanging out of the corner of her mouth, looking off at an angle at some unknown object or person. Du Bois looks as if she belongs in a 1940s film noir. There is no hint of a smile. In fact, with all the photographs of Du Bois, only in those from her childhood and the one from when she was retired do we see her clearly smiling. The book’s images may reflect only the author’s choices, but the photographs’ message that those years came with difficulties is consistent with the written narrative. Even among anthropologists, Cora Du Bois is not a well-known name—another pioneering woman and lesbian whose story has been lost. Anthropologist Susan C. Seymour was first Du Bois’ student, then friend, colleague, and now biographer. She wrote this book to bring back into prominence this scholar, the first women full professor with tenure at Harvard University, who also served as a high-ranking intelligence officer during World War II. As a lesbian, Du Bois’ remarkable life was complicated by changing global events and shifting political and social climates in the United States. Du Bois’ story adds to our understanding of a relatively understudied cohort of professional women born too late for the great wave of women’s 19th-century and progressive-era activism and change and too early to benefit from the reemergence of feminism. Throughout this biography the author sets a theme that our culture ceased to have spaces for someone such as Du Bois during most of the 20th century. While Du Bois’ class and race privilege allowed her to enter and to succeed in her chosen field, the sexism, homophobia, and political repressions of her era meant that even her accomplishments came at a personal cost. Her abilities and accomplishments demanded recognition, but the positions she earned were grudgingly given and far from comfortable or accommodating. Cora Du Bois’ earliest years were unsettled and atypical. As her Swiss-born father worked to reestablish his business in the early 1900s, Cora moved between the United States and Europe. Eventually, the family settled in the United States, in a large home in Perth Amboy, New Jersey. Her father was loving and supportive. Her mother was capable but intimidating. This period of stability ended when Cora’s father died the summer she graduated from high school. After a year helping her mother deal with this loss, Cora began commuting to Barnard College. Although her major was history, her senior-year anthropology course with Franz Boas and Ruth Benedict (Margaret Mead was the T.A.) determined her future. Concurrent with her mother’s remarriage, which gave her daughter sufficient wealth to be financially independent, Cora entered the graduate program in anthropology at the University of California at Berkeley. There she excelled, finding in California mentors—Professors Lowie and Kroeber—as well as a lover and circle of lesbian friends. Du Bois earned her PhD from Berkeley in 1932. Fulltime positions were scarce during the Depression, especially for women; Du Bois’ wealth and a few grants gave her the flexibility to continue with part-time or temporary positions. She built an impressive reputation through innovative fieldwork and teaching, produced respected publications, and pioneered the “culture and personality movement” within American anthropology. JOURNAL OF HOMOSEXUALITY 2016, VOL. 63, NO. 10, 1439–1441 http://dx.doi.org/10.1080/00918369.2016.1210941 Collapse"
106738,1991,applied economics,charitable donations by uk households:  evidence from the family expenditure survey,charitable donations by uk households evidence from the family expenditure survey,"This paper reports estimates based on both standard Tobit and generalized Tobit techniques of the determinants of charitable giving in the UK using 1984 Family Expenditure Survey data. This is the first such study using UK data. Separate estimates are presented of the determinants of participation and of the level of donations by giving households. Participation is seen to be sensitive to income, the tax-price of giving, and a range of demographic variables, while the level of donations varies primarily with income. Donations are found to be inelastic with respect to changes in disposable income. Collapse"
106739,1991,applied economics,the economics of university computer provision:  a case study in public production and cost,the economics of university computer provision a case study in public production and cost,Is the provision of computer services in British universities provided efficiently? The pattern of expenditure in the last twenty years is explored and a study is made of current spending across universities. Variations in unit computing costs and the output of computing expreience is studied for British universities using conventional cost and production function estimation. In addition the three-demensional production surface for the output of computer provision is estimated using distance weighted techniques. This case study has more general applicability to other areas of public production and cost. Collapse
106740,1991,applied economics,the effect of the youth training scheme on employment probability,the effect of the youth training scheme on employment probability,ERR
106741,1991,applied economics,road accident external effects:  an empirical assessment,road accident external effects an empirical assessment,"The relationship between the number of road accidents and the volume of traffic is econometrically estimated in this paper. A stratified random sample of 399 road segments covering urban and rural roads in New York State(USA) in 1985 is analysed. No significant externality is detected. On ver high volume urban roads the marginal to average accident rate ratio is 1.06. On all other roads, a ratio of 0.98 is estimated. These findings contrast sharply with those of Vickrey, who estimated a ratio of 1.5 for California freeways in 1962. The results are, however, consistent with the ‘official view’ of both the US and UK highway authorities that no significant accident externality exists. Collapse"
106742,1991,applied economics,telecommunications divestiture in the uk:  a crossed line?,telecommunications divestiture in the uk a crossed line,No Result.
106743,1991,applied economics,non-nested tests of new classical versus keynesian models: further evidence,non nested tests of new classical versus keynesian models  further evidence,"This paper re-runs the experiment conducted by Dadkhah and Valbuena (1985), using data generated by the Australian, Canadian, Japanese, Swiss and Swedish economies. The aim of this exercise is to discriminate between new classical and Keynesian accounts of the business cycle. The results obtained in the present study are broadly supportive of Dadkhah and Valbuena's earlier findings in that the Keynesian model generally outperforms the new classical alternative. Collapse"
106744,1991,applied economics,derivative securities and cash market stability,derivative securities and cash market stability,"This study investigates the effect of introducing interest-rate futures and options on the price variances in related financial cash markets. Standard research approaches to this issue relate cash-price stability before the introduction of futures and options trading to cash-price stability after trading in the derivative security begins. However, controlling for the additional factors that may also effect cash markets is difficult. The approach employed here to deal with this obstacle is motivated by recent theoretical research relating cash and futures markets, but hitherto not operationalized to empirically test for a relationship between the markets. Varying-parameter models of (1) the demand for short-term Treasury securities, (2) the demand for large time-deposits, and (3) the supply of large time-deposits are specified such that changes in the parameters imply changes in the volatility of the cash price. These parameters are modelled as functions of the trading volume of interest-rate futures and o... Collapse"
106745,1991,applied economics,organizational status and performance: the effects on employment,organizational status and performance the effects on employment,"DAVID PARKER Finance, Economics and Accounting Group Cranfield School of Management Cranf ield Institute of Technology Cranfield, Bedford MK43 OAL (Tel: 0234 - 751122) and KEITH HARTLEY Institute of Social and Economic Research University of York York Copyright: Parker and Hartley 1989 Collapse"
106746,1991,applied economics,government employment and the provision of fringe benefits,government employment and the provision of fringe benefits,No Result.
106749,1991,applied economics,the structure of production in urban housing:  a multi-input cost function approach,the structure of production in urban housing   a multi input cost function approach,"This paper uses a two-stage cost function model to analyse substitution possibilities among inputs in the production of urban housing. Aggregate price indexes for groups of inputs are generated in the first stage, and then used in the estimation of the aggregate cost function in the second stage. The procedure is valid under the assumption of weak separability. The use of flexible functional forms at both stages allowed the analysis to be carried out by imposing fewer restrictions as a part of the maintained hypothesis than has been the case in the previous literature. The findings are consistent with the postulates of cost-minimising factor demand theory. Collapse"
106750,1991,applied economics,demand uncertainty and foreign trade in the uk tobacco industry,demand uncertainty and foreign trade in the uk tobacco industry,"The concern about the potential health harzards of various tobacco products in the UK has led many tobacco companies to exploit markets in underdeveloped countries. These markets are relatively safe since the health risks of tobacco are not yet fully known. The possibility, therefore, arises that domestic producers might exploit certain markets to provide a form of profit insurance, exporting surpluses when domestic demand is unexpectedly low. This study concentrates on the UK cigarette industry. However, similar analysis could be performed for any uncertain market where a certain alternative exists that can be exploited at relatively low cost. Collapse"
106751,1991,applied economics,optimal algorithms and lower partial moment:  ex post results,optimal algorithms and lower partial moment ex post results,"Portofolio management in the finance literature has typically used optimization algorithms to determine security allocations within a portfolio in order to obtain the best trade-off between risk and return. These algorithms, despite some improvements, are restrictive in terms of an investor's risk aversion (utility function). Since individual investors have different levels of risk aversion, this paper proposes two portfolio-optimization algorithms that can be tailored to the specific level of risk aversion of the individual investor and performs ex postevaluation tests of the algorithm performance. Collapse"
106752,1991,applied economics,central bank credibility and forecasting,central bank credibility and forecasting,"The relation between money and the economy during the 1980s has caused many economists to reassess their position on the desirability of using the money supply to guide monetary policy. Primary issues include the measurement of money and variability in the lags in monetary policy. The importance that many economists and policy-makers place on money growth has diminished as many believe that the lagged effects of money on real output and prices have either increased, become less reliable or vanished. As the experi­ ence of the 19808 continues to confound the profession, it is commonplace for economists to suggest that more data are needed before we can understand how money is now affecting the economy. This paper examines the hypothesis that .the predictive ability of money is affected by central bank policy. It is hypothesized that the ability to forecast the short-run and long-run lagged influences of money on real output and prices is affected by central bank policy. This hypothesis follows the 'Lucas (1976) critique' which argues that model predictability is influenced by policy regimes. In our case, predictions of real output and inflation based on a model whose coefficients are estimated from one policy regime may be expected to deteriorate rapidly when policy regimes change. An implication ofthis hypothesis is that when policy regimes change the conventional dictum that 'more data are better than less data' is not necessarily true when one forecasts economic variables that are influenced by those public policies. As a test of this hypothesis, this paper examines the predictive ability of money in the 1980s. It is argued that 1980s monetary policy is more similar to the 1960s than that of the 1970s and, therefore, real output and inflation in the 1980s should be better predicted from a model based on the experience of the 1960s versus the 1970s. Because support for this hypothesis is found, it is concluded that the 1980s relation between money and the economy is better under­ stood when recent data (1970s) are excluded in favour of past data (1960s) that more closely approximate central bank behaviour in the prediction interval. The paper conclu­ des with several policy implications. Collapse"
106753,1991,applied economics,"persistence, seasonality and trend in the uk egg production",persistence seasonality and trend in the uk egg production,"This paper discusses the time-series properties of UK egg production in order to provide an empirical analysis of the possible long-run impact of the shock on the industry following the recent incidence of salmonella poisoning. Our analysis shows that although the short-run properties of the UK egg production are consistent with the presence of persistent shocks, a shock duration is unlikely to have large long run efefcts. This result is remarkably robust to the choice of the persistence measure obtained. Collapse"
106754,1991,applied economics,advertising in demand systems:  testing a galbraithian hypothesis,advertising in demand systems testing a galbraithian hypothesis,In this paper we argue that the influence of advertising upon the inter-product distribution of demand is a system-wide phenomenon which is investigated better through a demand equation system rather than through single equation methods. Attention is focused upon a dynamic version of Deaton Muellbauer's AIDS model modified to include advertising terms. Within this framework using data on consumer's non-durable expenditure in the UK we carry out a preliminary test of Galbraith' hypothesis that advertising may affect the composition of aggregate consumer demand. Collapse
106755,1991,applied economics,the econometrics of markets with quantity controls,the econometrics of markets with quantity controls,No Result.
106756,1991,applied economics,self-service activities and formal or informal market services,self service activities and formal or informal market services,"Household can provide themselves with services in different ways. Generally a household can produce a service itself, or buy it on a formal or informal market. On an informal market services are sold at a price substantially lower than on a formal market, because of tax evasion by both consumer and producer. This article investigates the choices made by Dutch households with respect to three services: small home repairs, car repair and maintenance, and ladies' hairdressing. The analysis shows that the mode of provision of the services is influenced not only by financial considerations, but also by the ability of the household to engage in home production, skills for and attitudes towards home production and the availability of informal market services in the local area. Collapse"
106758,1991,applied economics,identifying competitors from market share data:  a technique and an application,identifying competitors from market share data a technique and an application,"This paper develops a technique for identifying competitors from data on the correlations in the market shares of different firms. The technique is based on an application of a ‘complete system’ demand model, and results are presented graphically by an application of principal componens. The technique is illustrated with an example of competitors in the UK Commercial Vehicle Industry, and would be applicable to other markets. Collapse"
106759,1991,applied economics,leakages from the money demand function,leakages from the money demand function,The performance of alternative scale variables is explored in a simple demand function for narrow money. Sequential test establish consumers' expenditure as the preferred measure. The implications for fiscal policy and the paradox of thrift are outlined. Collapse
106761,1991,applied economics,growth and distribution:  a test of the induced innovation hypothesis for the korean economy,growth and distribution a test of the induced innovation hypothesis for the korean economy,"This study investigates technical change biases and their effects on distribute shars in Korea. We are particularly concerned with whether technical change biases have been induced by changes in relative factor prices accompanied by rapid economic growth in Korea. Our test results show that technical change in Korea has been biased in the direction in the direction for saving labour and using capital, thus rendering support to the induced innovation hypothesis. These findings are roughly in parallel with empirical studies on the induced innovation hypothesis for the US and Japanese economies. The similar pattern in technical change biases between Korea and the two advanced economies is noteworthy in light of their basic differences in factor endowments. Another salient feature of our empirical analysis is that the price-corrected labour share in Korea has continually shrunk in the process of rapid economic growth, which has been partly precipitated by factor price-disorting policies. Collapse"
106790,1991,british journal of industrial relations,'them and us': social psychology and 'the new industrial relations.',them and us social psychology and the new industrial relations,"This article sets out to examine the impact of ‘new industrial relations’ techniques on worker attitudes to management and to worker-management relations. We found 17 case studies of share schemes, profit-sharing, quality circles and autonomous work-groups which reported relevant evidence on worker attitudes. Although workers often welcome new industrial relations techniques, there is very little evidence of any impact on ‘them and us’ attitudes. Drawing on social-psychological theories of attitude change, the persistence of ‘them and us’ attitudes can be explained by the ways in which new industrial relations techniques have been implemented and managed in organisations. Workers have often lacked choice over participation in new schemes; there has been a lack of trust between the parties involved, together with inequality in status and benefits and a lack of institutional support for the schemes among senior management. It is argued that these conditions explain the failure of new organisational initiatives to bring about changes in ‘them and us’ attitudes. Collapse"
106913,1991,econometric theory,asymptotically efficient estimation of cointegration regressions,asymptotically efficient estimation of cointegration regressions,"An asymptotic optimality theory for the estimation of cointegration regressions is developed in this paper. The theory applies to a reasonably wide class of estimators without making any specific assumptions about the probability distribution or short-run dynamics of the data-generating process. Due to the nonstandard nature of the estimation problem, the conventional minimum variance criterion does not provide a convenient measure of asymptotic efficiency. An alternative criterion, based on the concentration or peakedness of the limiting distribution of an estimator, is therefore adopted. The limiting distribution of estimators with maximum asymptotic efficiency is characterized in the paper and used to discuss the optimality of some known estimators. A new asymptotically efficient estimator is also introduced. This estimator is obtained from the ordinary least-squares estimator by a time domain correction which is nonparametric in the sense that no assumption of a finite parameter model is required. The estimator can be computed with least squares without any initial estimations. Collapse"
106914,1991,econometric theory,estimation of the covariance matrix of the least-squares regression coefficients when the disturbance covariance matrix is of unknown form,estimation of the covariance matrix of the least squares regression coefficients when the disturbance covariance matrix is of unknown form,This paper deals with the problem of estimating the covariance matrix of the least-squares regression coefficients under heteroskedasticity and/or autocorrelation of unknown form. We consider an estimator proposed by White [17] and give a relatively simple proof of its consistency. Our proof is based on more easily verifiable conditions than those of White. An alternative estimator with improved small sample properties is also presented. Collapse
106915,1991,econometric theory,estimating nonlinear dynamic models using least absolute error estimation,estimating nonlinear dynamic models using least absolute error estimation,"We consider least absolute error estimation in a dynamic nonlinear model with neither independent nor identically distributed errors. The estimator is shown to be consistent and asymptotically normal, with asymptotic covariance matrix depending on the errors through the heights of their density functions at their medians (zero). A consistent estimator of the asymptotic covariance matrix of the estimator is given, and the Wald, Lagrange multiplier, and likelihood ratio tests for linear restrictions on the parameters are discussed. A Lagrange multiplier test for heteroscedasticity based upon the absolute residuals is analyzed. This will be useful whenever the heights of the density functions are related to the dispersions. Collapse"
106916,1991,econometric theory,robust  m-tests,robust  m tests,"This paper investigates the local robustness properties of a general class of multidimensional tests based on M -estimators. These tests are shown to inherit the efficiency and robustness properties of the estimators on which they are based. In particular, it is shown that small perturbations of the distribution of the observations can have arbitrarily large effects on the asymptotic level and power of tests based on estimators that do not possess a bounded influence function. An asymptotic ‘admissibility’ result is also presented, which provides a justification for tests based on optimal bounded-influence estimators. Collapse"
106928,1991,econometrica,solution to a problem of stochastic process switching,solution to a problem of stochastic process switching,"FUTAJEET 9 (2) (2015) (89-95) 89 Iyanda and Odejobi / FUTAJEET 9 (2) (2015) (89-95) FUTAJEET 9 (2) (2015) (89-95) 90 Figure 1: System Design The fuzzy rule based consists of 15 If-Then statements that characterized the behaviour of the Fuzzy Logic module. These rules are formed using the following guidelines: (i.) if all the input parameters are high then electric load consumption is characterized by L15. (ii.) if four of the input parameters are high and one is mid then electric load consumption is characterized by L14. (iii.) if three of the input parameters are high and two are mid then electric load consumption is characterized by L13. (iv.) if four of the input parameters are high and one is low then electric load consumption is characterized by L12. (v.) if two of the input parameters are high and three are mid then electric load consumption is characterized by L11. (vi.) if three of the input parameters are high and two are low then electric load consumption is characterized by L10. (vii.) if two of the input parameters are high and two are mid and one is low then electric load consumption is characterized by L9. (viii.) if three of the input parameters are mid and one is high and one is low then electric load consumption is characterized by L8. (ix.) if two of the input parameters are high and three are low then electric load consumption is characterized by L7. (x.) if three of the input parameters are mid and one is high and one is low then electric load consumption is characterized by L6. (xi.) if three of the input parameters are mid and two are low then electric load consumption is characterized by L5. (xii.) if four of the input parameters are low and one is high then electric load consumption is characterized by L4. (xiii.) if three of the input parameters are low and two are mid then electric load consumption is characterized by L3. (xiv.) if four of the input parameters are low and one is mid then electric load consumption is characterized by L2. (xv.) if all the input parameters are low then electric load consumption is characterized by L1. After identifying the input parameters to the system as well as output data from the system, the first stage in the system development is the fuzzification of the data. FL toolbox in Matlab have different type of membership functions but the triangular (trimf) and trapezoid (trapmf) membership functions were used. The fuzzy inference system (FIS) is created using FL toolbox in Matlab and it is named stlfnew1. There are two types of fuzzy inference method; Mamdani and Sugeno. The FIS used is Mamdani because it is the most commonly seen fuzzy methodology and its output membership functions are fuzzy sets while that of Sugeno are either linear or constant. Figure 2 shows the Fuzzy Inference System (FIS) editor Iyanda and Odejobi / FUTAJEET 9 (2) (2015) (89-95) 91 Iyanda and Odejobi / FUTAJEET 9 (2) (2015) (89-95) 91 for STELF than other types of electric load forecasting. 2. Approaches to Load Forecasting There are three major approaches to LF. These include manual, statistical, and artificial intelligence. a. Manual approach: This is the approach in which the operator uses his or her experience and intuition to obtain a good guess of the load demand. This approach is inefficient, noneffective and time consuming. It is prone to too much error. These limitations render this method not appropriate for load forecasting in getting correct result. b. Statistical approach: This is the approach that depends largely on historical data of electric load consumption. This approach usually requires a statistical model that represents load as function of different factors such as time of the day, weather, and customer class. It involves the use of historical data and produces instant result. Statistical packages are readily available, though expensive. The historical load data used by this approach may not always be available especially in the developing countries. Load behaviour experiences sudden change which statistical method cannot always cope with. Therefore, this approach is not most appropriate for use in STELF. Statistical approach includes multiple linear regression (MLP), Adaptive models, general exponential smoothing, stochastic time series and state-space methods. c. Artificial Intelligence (AI) Approach: This is the approach which is used to study and design intelligent agents (a system that perceives its environment and takes actions which maximize its chances of success). In this work, this approach is subdivided into four major parts which includes: i. Genetic Algorithm: Genetic Algorithms (GAs) have recently received much attention as robust stochastic search algorithm for various problems (ELNaggar and AL-Rumaih, 2005). This method is based on the mechanism of natural selection and natural genetics, which combines the notion of survival of the fittest, random and yet structured, search and parallel evaluation of the points in the search space. GA accommodate all the facets of soft computing, namely uncertainty, imprecision, nonlinearity, and robustness. GA can be used to provide a good set of initial weights for Neural Network (NN), or can be used to fully train the NN or to find the optimal network structure (Haque and Kashtiban, 2005). ii. Neural network: This approach depends solely on data. Artificial Neural Network (ANN) or simply put Neural Network (NN) acquires input variables from the historical database and output the forecasted load. When the ANN is in the learning stage (Feinberg and Genethliou, 2006), the database is used as training set and the errors are back propagated until satisfactory levels of accuracy are achieved. This approach is weak at generalization and if not well trained, it can over fit the data. ANN approach is not interactive in nature and cannot provide explanation to the operator on what he needs to know. NN though accurate in weekday load forecasting (Dash et al, 1995) is poor at forecasting peak loads and holiday loads. NN based load forecasts give large errors when the weather profile changes very fast. iii. Rule-based approach: This approach entails using a set of assertions, which collectively form the 'working memory', and a set of rules that specify how to act on the assertion set. Rule-based systems are a good choice (Caudil, 1991) when one has only a few data samples, when one has experts that are readily available for consultation, or when one finds that an explanation for answer is essential. Rule-base is modular in nature, in that it allows for modification. New rules can be added and old ones can be deleted, since rules are independent of one another. An example of rule-based system is an expert system (ES). ES differs from other kinds of AI in that it deals with subject matter of realistic complexity that normally requires a considerable amount of human expertise; exhibits high performance in terms of speed and reliability (Jackson, 1999). This makes ES to be a useful tool, capable of explaining and justifying solutions or recommendations to convince the user that its reasoning is in fact correct. ES provides a number of benefits according to Beckman (1991) which include reduce costs; improved quality; increased revenue; retention of expertise and easy distribution of expertise. iv. Fuzzy Logic: Fuzzy logic is a generalization of the usual Boolean logic used for digital circuit design (Feinberg and Genethliou, 2006). An input under Boolean logic takes on a truth-value of ""0"" or ""1"". Under fuzzy logic, an input has associated with it a certain qualitative ranges. Fuzzy logic allows one to (logically) deduce outputs from fuzzy inputs. In this sense, fuzzy logic is one of a number of techniques for mapping inputs to outputs (i.e. curve fitting). Among the advantages of fuzzy logic are the absence of a need for a mathematical model mapping inputs to outputs and the absence of a need for precise (or even noise free) inputs. With such generic conditioning rules, properly designed fuzzy logic systems can be very robust when used for forecasting. Iyanda and Odejobi / FUTAJEET 9 (2) (2015) (89-95) 92 Iyanda and Odejobi / FUTAJEET 9 (2) (2015) (89-95) 92 2.1 Related Work Sun et al. (2004) presented fault diagnosis of electric power systems using fuzzy Petri nets (FPN) as a modeling Tool. The validity and feasibility of this method is illustrated by simulation examples which shows from seven cases that the faulted system elements can be diagnosed correctly by use of these models, and a satisfying result can also be achieved even in the situation with large amount of incomplete and uncertain alarm information. Also, the merits of easy reasoning, fast diagnosing speed and strong practicability of fault diagnosis models based on FPN were demonstrated.. Wu (1998) presented a Petri net algorithm for multiple contingencies of Distribution system operation by combining with a new restoration approach. This approach is performed by releasing some loads required to maintain the rest system in safe operation in overload cases and opening all of the switches in the out-of-service areas for fault cases, and then closing proper switches to restore the isolated areas. Heuristic rules and valuation functions were presented for the best first search in the Petri nets (PN). A practical Taiwan power (Taipower) distribution system was simulated to demonstrate the effectiveness of the proposed method. Combining with the proposed restoration algorithm, the Petri net and its inference processing are efficiently implemented using C++. It was concluded that multiple contingencies of distribution systems could be solved by the proposed method effectively. Iyanda et al. (2011) presented a short-term electric load forecasting in uncertain domain using fuzzy decision tree approach. The fundamental requirement for the proposed model is the production of robust and accurate performance with minimal computational and data resources. Solution strateg"
106929,1991,econometrica,stochastic process switching:  some simple solutions,stochastic process switching some simple solutions,"When changes in the economic policy regime occur stochastically, asset prices will reflect the possibility of such shifts. In this paper we apply techniques of regulated Brownian motion to obtain closed-form analytic price solutions when policy reaction functions are subject to prospective changes. We focus on the case in which the authorities promise to peg a currency's exchange rate once it reaches a predetermined future level. We also show how an open-ended commitment to exchange-rate targeting may lead to multiple equilibria. Collapse"
106940,1991,econometrica,the division problem with single-peaked preferences:  a characterization of the uniform allocation rule,the division problem with single peaked preferences   a characterization of the uniform allocation rule,ERR
106941,1991,econometrica,a more powerful method for triangularizing input-output matrices:  a comment,a more powerful method for triangularizing input output matrices   a comment,"Studies in the production structure of any national economy, as revealed in its inputoutput table, have been closely related to the question of the interindustrial dependence or hierarchical structures of productive sectors leading from primary to final production. In particular, the notion of hierarchy provides a useful tool when one wants to draw inferences on the structural change or international difference of industrial structures. The standard technique for studying this notion is to triangularize the input-output table by interchanging sectors in order to maximize the entries below the main diagonal. In a perfect triangularized table, the entries above the main diagonal should be zero. For example, such a strong one-way interdependence relation as cotton-textiles-clothing can easily establish the hierarchy. Due to the existence of circular relations like coal-steel-mining equipment-coal, it is not possible to triangularize the input-output table perfectly. This paper extends and revises the previous triangulation method which is based on a permutation theorem deriving from the interchange of adjoining two industrial groups, into that among three industrial groups (Theorem 2). The new algorithm based on Theorem 2 is demonstrated by actually computing the suboptimal orderings for the four input-output tables for such more developed countries (MDC's) as the United States, Italy, Norway, Japan, and the two tables for such less developed countries (LDC's) as India and Korea. The empirical results suggest that sectors can be arranged in a similar hierarchical order among MDC's and LDC's. Transport Equipment, Machinery, Apparel, Leather and Products, Grain Mill Products, and Processed Foods, which link directly to the final demand, are recorded as higher-order sectors. Trade, Transport, and Services are lower-order ones, and Energy sectors such as Electric Power, Coal Products, Coal Mining, Petroleum, Petroleum Products and Natural Gas are the lowest. Consequently, this paper provides some more evidence in support of the similarity of hierarchical structures of production among these countries. Collapse"
106942,1991,economic development and cultural change,"interprovincial disparities in china:  output and consumption, 1952-1987",interprovincial disparities in china   output and consumption  1952 1987,"The flood of economic information from China in recent years has permitted reassessment of the ""Chinese model of development"" and the extent to which China's own development experience has conformed to it. The prevailing consensus of the 1970s concerning the substance of the model itself-typically characterized in terms of the Chinese leadership's objectives and priorities and its strategy for pursuing them-has stood up quite well. The same cannot be said about past appraisals of the extent to which China's economic reality has been reshaped to accord with the model's prescriptions. One of the most widely remarked elements of the Chinese model is the high priority assigned to regional objectives and, more specifically, to ""balance"" in the distribution of productive capacity and equity in the distribution of income and, hence, consumption. During the Maoist era (i.e., prior to 1979), the central government pursued regional objectives through such measures as interregional transfers of investment resources (effected via the planning system), subsidization of health and education expenditures in poor regions, and attempts to maintain a safety net of state relief (effected largely via control of grain surpluses). In short, there is little doubt that prior to 1979 the Chinese leadership placed an unusually high value on the spread of modern growth and the improvements in welfare associated with it and persistently acted to limit the emergence or widening of interregional disparities.' Although China's post-Mao leaders have shown greater concern with efficiency and with the potential trade-offs between efficiency and such goals as balance and equity, their continuation of many redistributive policies indicates that they have not abandoned regional objectives. But how successful has China been in achieving these objectives? Collapse"
106943,1991,economic development and cultural change,"relative efficiency, self-containment, and comparative costs of less developed countries",relative efficiency  self containment  and comparative costs of less developed countries,"International differences in comparative costs can in principle be attributed to three kinds of forces: factor proportions, economies of scale, and relative efficiency. To take a specific example, the low relative price of certain services in less developed countries might be attributed to the fact that the services in question are not at all intensive in either physical or human capital (the factor proportions account), that economies of scale are unimportant in these services, or that LDCs are relatively efficient in producing these services (as compared to their efficiency in producing, say, manufactured products). While it is obvious that differences in relative efficiency can affect comparative costs (and trade patterns), economists have tended to emphasize the factor proportions theory and economies of scale in their explanations. Part of the reason for this emphasis is that the relative efficiency explanation is rather empty, unless one can come up with explanations for the patterns of relative efficiency. The present article attempts to address these issues by looking at some data on relative product prices in a sample of less developed countries (LDCs) and more developed countries (MDCs). The data are taken from the purchasing-power parities collected by the International Comparison Project.' In the course of the research project it was discovered that neither the factor proportions theory nor the economies of scale phenomenon could account for more than a rather small part of the observed variation in relative product prices. The research was then directed toward explaining the remaining variation in relative product prices by looking at some product characteristics that were thought to affect relative efficiency. There are many characteristics of goods that may be hypothesized to affect the relative efficiency of LDCs compared to MDCs. These Collapse"
106944,1991,economic development and cultural change,the rise and demise of textiles and clothing in economic development: the case of japan,the rise and demise of textiles and clothing in economic development the case of japan,"In recent decades, producers of textiles and clothing in advanced industrial economies were the first large group of manufacturers who went into a decline as a result of import competition from newly industrializing economies. This occurred primarily because many processes in textile and clothing production tend to be intensive in the use of unskilled labor and so, as unskilled labor becomes relatively scarce in the advanced economies, comparative advantage gradually moves to countries less well endowed with physical and human capital per worker.' However, only a subset of countries with low capital-labor ratios are likely to become exporters of labor-intensive manufactures. That subset is limited to newly industrializing economies which are also poorly endowed with natural resources per worker and hence characterized by low real wages for labor that is attracted from primary production to industry as industrial capital expands. The dominance of East Asia's resource-poor, rapidly growing economies in satisfying the growing demand for imports of textiles and clothing by advanced industrial countries certainly supports this theory. One purpose of this article is to show that this theory is also supported strongly by evidence provided over the longer term by Japan's experience. That evidence shows that the resource-poor, rapidly industrializing Japanese economy gradually strengthened its comparative advantage in textiles and clothing from late last century, but subsequently lost this advantage steadily during the postwar period as exports of these items dwindled and imports from newly industrializing neighbors rose. In addition, the Japanese experience supports two cor- Collapse"
106945,1991,economic development and cultural change,the export performance of sub-saharan africa,the export performance of sub saharan africa,"In most sub-Saharan African (SSA) countries there is a strong connection between tradables recovery and the alleviation of rural poverty through higher farm incomes, agricultural wages, rural multiplier effects and inter-sectoral linkages. Liberalisation policies, as elaborated in the Berg report (1991) and subsequent World Bank documents, stressed the importance of reversing stagnation in agricultural tradables by ‘getting prices right’ and removing state monopolies on marketing. Increasing producer prices and devaluing overvalued exchange rates was seen as the means of increasing export revenues and reducing current account deficits. This paper examines the performance of agricultural tradables through the reform phase for a selection of sub-Saharan African countries to see how far recovery has taken place. The descriptive statistics show that agricultural tradables recovery has not been sustained and that in real terms, prices have fallen rather than risen, while the relative prices of non-tradables have increased. A simple model of the determinants of tradables output is tested using data for 14 SSA countries and six major tradables. The results show that, contrary to conventional theory, none of the price variables underlying liberalisation are significantly associated with tradables output, indicating the importance of non-price factors such as the reform of institutions. The final part of the paper examines variations in the relevant institutional arrangements and evidence for ‘export pessimism’ attitudes which may have hindered sustained tradables recovery and the alleviation of rural poverty. Collapse"
106946,1991,economic development and cultural change,labor markets in northeast brazil:  does the dual market model apply?,labor markets in northeast brazil does the dual market model apply,"The economy of Northeast Brazil, despite an impressive 7% real annual growth rate in recent years, lags behind the rest of the country, and far behind the dynamic south in particular.' With more than half of its 40 million residents living in poverty, the region contains the largest concentration of the poor in Latin America. Agriculture has traditionally been the dominant industry there, but its importance is being eroded by the growth of manufacturing and services. This shift has been aided by federal government policies, which have provided jobs through direct government employment and have offered tax breaks, subsidies, advice, and encouragement to large manufacturing firms in the region. Urban growth, meanwhile, especially in the major regional centers of Recife, Fortaleza, and Salvador, has swelled because of inflows of migrants. By 1980, more than half of the population of what had been a predominantly rural region now lived in urban Collapse"
106947,1991,economic development and cultural change,"labor market segmentation and the determination of wages in the public, private-formal, and informal sectors in san jose, costa rica",labor market segmentation and the determination of wages in the public  private formal  and informal sectors in san jose  costa rica,"A striking feature of many cities in less developed countries (LDCs) is the coexistence of large, well-organized, high-technology firms with the small street vendors who own little but a wooden box to sit on. This apparent dualism in the urban economy has been a concern of students of economic development for many years. Several international agencies, notably the International Labour Organisation (ILO) and the World Bank, have focused attention on this phenomenon. The ILO popularized the terms ""formal sector"" and ""informal sector"" to describe this dichotomy. To many economists, one consequence of this dualism is labor market segmentation.' Labor market segmentation plays a crucial role in explanations provided for some important issues, such as rural-urban migration, urban poverty, unemployment, underemployment, and growth.2 In this article I examine the hypothesis that labor market segmentation exists in San Jos6, Costa Rica, between the public and private-formal sectors and between the privateformal and informal sectors (defined in the Appendix). I define labor market segmentation as a situation where, because of institutional barriers to occupational mobility between sectors, a worker in the lower sector has less than full access to a job in the upper sector held by an observationally identical worker.3 If there were no barriers, workers in the low wage sector would enter the high wage sector and force the wages in that sector down until wages across sectors were equalized. That is, labor market segmentation implies differences in wages for observationally identical workers. Relying on this inference, most of the empirical literature on labor market segmen- Collapse"
106949,1991,economic development and cultural change,education and technical efficiency during the green revolution in pakistan,education and technical efficiency during the green revolution in pakistan,"tion is more likely to arise during disequilibrium caused by technical change. In agriculture this may be because technical change renders the existing cultural practices obsolete or inadequate and calls for an adjustment. A more educated farmer is supposed to make the required adjustment more quickly. This article tests the foregoing hypothesis in Pakistan agriculture during the Green Revolution when introduction of new crop varieties disturbed the prevailing equilibrium. For this purpose a production function approach is used and the analysis is conducted not only for the new crop varieties but also for their traditional counterparts. The results lend support to the hypothesis by showing that the worker effect is stronger for new crop varieties than it is for traditional ones. The article is divided into five sections. In the next section a brief review of the literature is presented. Section III presents the hypothesis and discusses the methodology used for the analysis. The results and their policy implications are discussed in Section IV, and Section V concludes the article. II. Brief Literature Review In a pioneering study exploring the economic effects of education, Z. Griliches used production function analysis to highlight the contri Collapse"
106951,1991,economic geography,limits of statutory responses to corporate restructuring illustrated with reference to plant closing legislation,limits of statutory responses to corporate restructuring illustrated with reference to plant closing legislation,"The accumulated evidence on geographical restructuring in the United States suggests that the current regulatory framework is quite inadequate. There is little in the way of consistent statutory guides for accommodating or rationalizing competing public policy interests. By their actions, corporations are challenging the integrity of existing laws and regulations. In some cases, it is clear that corporations are willing to risk significant legal penalties to achieve their goals. Is a new statute the answer? Is an industrial policy the best way of rationalizing corporations' economic imperatives with their social obligations? It is argued here that advocates of a statutory response are overly idealistic about the potential of law to deliver a determinate solution to the regulatory dilemmas posed by restructuring. In this respect, the argument is pessimistic, but not conservative, recognizing that many believe that statutes are the only way of achieving social goals. Nevertheless, it is clear that legislati... Collapse"
106953,1991,economic geography,"corporate headquarters relocation and changes in metropolitan corporate dominance, 1980-1987",corporate headquarters relocation and changes in metropolitan corporate dominance  1980 1987,"During the last 30 years, U.S. metropolitan economies have experienced tremendous restructuring, and the locations of corporate headquarters have increasingly exhibited spatial shifts, both deconcentrating and dispersing. Theoretical explanations have suggested that the United States is entering the third of four stages, in which we are now witnessing the drive to regional maturity with no dominant regional center. Changes in the distribution of metropolitan corporate dominance between 1980 and 1987 are examined and related to two sets of explanatory frameworks, one spatial and the other structural. Changes in metropolitan corporate dominance were strongly related to spatial shifts in headquarters and asset location, especially shifts due to merger and acquisition activity. Changes in dominance were less strongly related to structural factors reflecting the degree of transition to the emerging service-based economy, even though population and location relative to New York were important. Finally, the effe... Collapse"
106967,1991,economic inquiry,teaching tools:  a public goods experiment for the classroom,teaching tools a public goods experiment for the classroom,"TEACHING TOOLS A PUBLIC GOODS EXPERIMENT FOR THE CLASSROOM George Stigler wrote in the American Economic Review twenty-seven years ago, that the typical student in an introductory college economics course would ""memorize a few facts, diagrams, and policy recommendations, and ten years later...be as untutored in economics as the day he entered the class"" [1963, 657]. And later, in the Journal of Economic Education, Stigler argued that ""economics belongs in everyone's education once we have learned how to teach it"" [1970, 80]. As we enter the nineties, academic economists continue groping for methods and techniques that will improve the way in which principles courses are taught. The proliferation of commercially available materials in the form of textbooks, workbooks, software, telecourses (e.g., Economics USA), and transparencies provides evidence of the increasing interest and expenditure aimed at improving the teaching of economics. However, there exists at least one very promising teaching technique that continues to see relatively limited action in the typical economics classroom. While experimental economic methods have become quite familiar to academic economists as a legitimate and powerful research tool, their usefulness for the economics classroom has not yet been generally recognized.(1) In this article, I describe a simple, yet powerful, public goods experiment for use in microeconomics or public finance courses. Since it is likely that many instructors in economics are reluctant to use experimental methods without sufficient preparation,(2) enough detail on experiment administration and suggested post-experiment discussion is provided to assist even the novice experimenter with getting started. Instructors interested in additional information on experimental economics or experiments for the classroom should consult the references listed in the concluding section. I. EXPERIMENT DESCRIPTION(3) A theoretical concept that introductory students often find hard to understand is the vertical summation of individual demand curves to obtain the market demand for a public good. The typical student also fails to fully appreciate how a free rider can cause underinvestment in public goods. The experiment outlined below provides a simple, yet powerful, technique to teach not only the vertical addition and the free rider problems, but also an ideal way to construct a prisoner's dilemma game and discuss the resulting underinvestment in the public good. Students are placed into N teams, where a ""team"" may be as few as one person. Each team is endowed with T tokens and must choose, in each of the decision-making rounds, whether to ""invest"" these tokens in the ""private account"" or the ""group account."" The private account will pay the team $0.30 per token invested each round. The group account will earn, for the entire ""society,"" $0.50 per token invested each round. From the group account, each team will then receive ($0.50) [G.sub.i]/N where [G.sub.i] equals the total number of tokens invested in the group account in round i by all N teams. Note that regardless of how many tokens any single team places into the group account, all N teams receive the same payout from the group account. Teams must invest all T tokens in each round and tokens cannot carry over from one round to the next. Finally, earnings from previous rounds are not allowed to be reinvested in the future. II. EXAMPLE EXPERIMENT Assume a representative class of sixty students.(4) Assume the instructor divides the class into twenty teams (N = 20) of three students each. Each team is endowed with T = 25 tokens per round. The teams read the ""INSTRUCTIONS"" overhead transparency while the instructor distributes to each team: (i) the ""TEAM RECORD SHEET,"" and (ii) the ""TEAM DECISION BALLOTS"" (one for each round).(5) For each round, once all the team ballots have been submitted, the instructor sums the tokens invested in the group account and reports this number and the dollar payout per team to the entire class. … Collapse"
106969,1991,economic journal,economics beyond the horizon,economics beyond the horizon,"Background: Entrepreneurship capabilities are considered a key element for the economic and sustainable development of countries, which must be understood as a complex phenomenon beyond enterprise generation, so it is necessary to start its development from an early age. Taking this in consideration, the purpose of this research is to propose a development model of entrepreneurship capabilities for the teenage population of the Atlántico Department, looking for the acknowledgment and strengthening of the capabilities on the different social contexts, starting from the revision of the conceptual basis that describe, analyze and interpret entrepreneurship as a defining element for the economic and social development of the country. Objective:Explain the interaction between potential and effective entrepreneurial abilities in adolescents aged 13 to 19 in the Atlantic department. Materials and Methods: To achieve this purpose, an empirical analytic research, of a projective type, with a non-experimental design transactional analysis was performed, to a sample of 1.579 young people with ages between 13 and 19 years old that were part of Educational Institutions of the municipalities of Atlántico. An assessment instrument was designed as a survey that would evaluate the defined categories for entrepreneurship capabilities: personal factors, entrepreneurship formation, and socio-cultural factors. After the collection and validation of information performed by Educational Institutions, we went on to identify the methodology of operationalization of the variables for their posterior use in the model proposal, from the answers to the survey, with the construction of complex indicators and the modeling of the structure of entrepreneurship for teenagers of Atlántico, by a Structural Equations System. Results: The main results of this research show how the potential entrepreneurship capability affects in a direct and positive way the effective entrepreneurship capability, this last one being determined by personal aspects, defined in terms of personal features like life skills, and observing a moderating role for family factors and a partial mediation effect between the potential entrepreneurship capability, the entrepreneurship formation, and the entrepreneurship capability. Conclusions: The theoretical model of the interaction of entrepreneurship capabilities in teenagers of the Atlántico Department, the result of this research, highlights the complexity of the effective entrepreneurship capability, since this one is affected by multiple personal, cognitive, familial, educational and cultural factors, clearly showing the effect that in a direct or indirect way these factors may have in the others and a potentializer of success in entrepreneurship initiatives. This study has important contributions for researchers and for the creation of public policies that allow strengthening the potential entrepreneurship capabilities in the different development environments of teenagers. KeyWords:Entrepreneurship, entrepreneurship capabilities, psychosocial factors, entrepreneurship formation. REFERENCIAS (colocar a cada artículo el DOI o la URL en caso de no tener DOI) Ajzen, I. (1991). The theory of planned behavior.Organizational Behavior and Human Decision Processes, 50(2), 179-211. Recuperado el 8 de mayo de 2017 de:https://doi.org/10.1016/0749-5978(91)90020-T Aktouf, O. (2001). La administración: entre tradición y renovación. Cali: Universidad del Valle – Gaetan Moerin. Alcaraz Rodríguez, R. E. (2004). Efectividad del Curso de Emprendedores, en el desarrollo del perfil emprendedor del alumno. Monterrey. Recuperado el 14 de abril de 2017 de: http://www.mty.itesm.mx/rectoria/dda/rieee/pdfII/s1/01DAFRafaelAlcarazFinal.pdf Alonso, C; Fracchia, E. (2009). El emprendedor Schumpeteriano aportes a la teoría económica moderna. Asociación Argentina de Economía Política. Recuperado el 5 de febrero de 2015 de: http://www.aaep.org.ar/anales/works/works2009/alonso.pdf Álvarez, S. y Busenitz, L. (2001). The entrepreneurship of resource-based theory.Journal of management, 27, 755–775. Recuperado el 7 de mayo de 2018 de: http://journals.sagepub.com/doi/pdf/10.1177/014920630102700609 Aparicio, R. E. (2012). Crear valor por medio del emprendimiento estratégico. Revista OIKONOMOS, 2(2), 32-51. Recuperado el 22 de Septiembre de 2015 de: https://revistaelectronica.unlar.edu.ar/index.php/oikonomos/article/ Arenius, P; Minniti, M. (2005).Perceptual Variables and Nascent Entrepreneurship.Small Business Economics, 24, 233247.DOI: 10.1007/s11187-005-1984-x Arocena, R. y Sutz, J. (2001). Sistemas de innovación y países en desarrollo. Organización de los estados Iberoamericanos. Recuperado el 9 de Septiembre de 2015, de:http://www.oei.es/salactsi/arocenasutz.htm. Asamblea del departamento del Atlántico. (1981). Ordenanza 028 del 11 de diciembre de 1981. Recuperado el 12 de octubre de 2019, de: http://www.ambq.gov.co/wp-content/uploads/2016/09/ordenanza-028-del-11de-diciembre-de-1981.pdf Asamblea Nacional de la Naciones Unidas. (2015). Transformar nuestro mundo: la Agenda 2030 para el Desarrollo Sostenible. Recuperado el 12 de febrero de 2019 de: http://www.un.org/ga/search/view_doc.asp?symbol=A/70/L.1&Lang=S Atienza, F. L; Pons, D; Balaguer, I; García-Merita, M. (2000). Propiedades psicométricas de la escala de satisfacción con la vida en adolescentes. Psicothema, 12(2), 314–320. Recuperado el 20 de mayo de 2019 de: https://www.uv.es/lisis/sofia/sofia satisfac.pdf Bagozzi, R. P; Tybout, A. M; Craig, C. S; Sternthal, B. (1979).The construct validity of the tripartite classification of attitudes.Journal of Marketing Research, 16, 8895.Recuperado el 20 de mayo de 2019 de: https://www.scholars.northwestern.edu/en/publications/the-construct-validity-ofthe-tripartite-classification-of-attitu Bandura, A. (1986). The social foundations of thought and action.Englewood Cliffs: Prentice Hall. Bandura, A. (1999). Auto-eficacia: Cómo afrontamos los cambios de la sociedad actual. Bilbao, España: Desclée de Brouwer. Barba-Sánchez, V. (2007). La necesidad de logro y la experiencia del emprendedor: elementos clave en el crecimiento de la nueva empresa. Revista de Contabilidad y Dirección, 5, 121-138. Recuperado el 22 de octubre de 2015, de:http://www.accid.org/revista/documents/RCD5_castellano_121.pdf Barba-Sánchez, V; Atienza-Sahuquillo, C. (2011). Reasons to create a new venture: A determinant of entrepreneurial profiles. African Journal of Business Management, 5(28), 11497-11504. Recuperado el 22 de octubre de 2015, de: http://www.academicjournals.org/app/webroot/article/article1380620515_BarbaSanchez%20and%20Atienza-Sahuquillo.pdf Baron, R.A. (2006).Opportunity recognition as pattern recognition. How entrepreneurs ‘connect the dots’ to identify new business opportunities. Academy of Management Perspectives, 20(1), 104-119. Recuperado el 10 de marzo de 2019 de: https://journals.aom.org/doi/abs/10.5465/amp.2006.19873412 Baron, R. A. (2000). Psychological Perspectives on Entrepreneurship.Current Directions in Psychological Science, 9(1), 15–18. Recuperado el 10 de marzo de 2019 de: doi:10.1111/1467-8721.00050 Barreto, K; Zuniga-Jara, S; Ruiz-Campo, S. (2016). Educación e Intención Emprendedora en Estudiantes Universitarios: Un Caso de Estudio. Formación Universitaria. 9(1), 25-34. DOI: 10.4067/S0718-50062016000100004. Recuperado el 25 de junio de 2017 en:http://www.scielo.cl/pdf/formuniv/v9n1/art04.pdf Bilbao, A; Pachano, S. (2002). Rasgos y actitudes de los emprendedores. Informe final. Venezuela: Corporación Andina de Fomento. Recuperado el 14 de abril de 2017 de: http://www.cid.harvard.edu/archive/andes/documents/workingpapers/razgosyacti tudes/rasgos_actitudes_venezuela_competitiva.pdf Blanchflower, D. (2004). Self-employment: More may not be better (NBER Working Paper No.10286, February). Cambridge: National Bureau of Economic Research. Recuperado el 7 de mayo de 2018 de: http://www.nber.org/papers/w10286.pdf Bonilla, A. (2010).Variables psicológicas y socio culturales involucradas en el proceso de emprendimiento empresarial. Poliantea, 6(11), 83 – 101. Recuperado el 19 de marzo de 2017 de: file:///C:/Users/Admin/Downloads/DialnetvariablesPsicologicasYSocioculturalesInvolucradasE-4784482.pdf Bosma, N; Jones, K; Autio, E; Levie, J. (2008). Global Entrepreneurship Monitor: 2007. Executive Report.Londres: London Business School. Recuperado el 7 de mayo de 2018 de: https://www.babson.edu/Academics/centers/blankcenter/global-research/gem/Documents/gem-2007-executive-report.pdf Boyd, N; Vozikis, G. (1994).The influence of self-efficacy on the development of entrepreneurial intentions and actions.Entrepreneurship Theory and Practice, 63-77. Recuperado el 9 de abril de 2017 de:http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.477.4240&rep=rep 1&type=pdf Braidot, N. B; Chiodi, F; Gonzales Pedraza, J; César, R. (2008). Fomento de las capacidades emprendedoras en estudiantes avanzados de Ingeniería Industrial de UNICEN. Experiencia piloto. Buenos aires. Universidad Nacional de Gral. Sarmiento. Argentina. Recuperado el 9 de abril de 2017 de:https://www.researchgate.net/profile/Gonzalez_Pedraza/publication/2283630 20_Fomento_de_las_capacidades_emprendedoras_en_estudiantes_avanzados _de_Ingenieria_Industrial_de_UNICEN/links/09e4150d0c67e0d225000000.pdf Brush, C. (2008).Pioneering strategies for entrepreneurial success. Business Horizons, 51(1), 21-27. Recuperado el 25 de junio de 2017 en: http://www.sciencedirect.com/science/journal/00076813/51/1 Bueno, E. (2013). El capital intelectual como sistema generador de emprendimiento e innovación. Lima: Universidad San Martín de Porres. Recuperado el 19 de marzo de 2017 de: http://www.usmp.edu.pe/vision2013/pdf/Diapos_confe/El_capital.pdf Bussey, K; Bandura, A. (2004).Social cognitive theory of gender development and functioning. En A. H. Eagly, A. E. Beall, y R. J. Sternberg (Eds.), The Psychology of Gender (pp. 92–119). Nueva Y Collapse"
106971,1991,economic journal,"nutrition, non-convexities and redistributive policies",nutrition  non convexities and redistributive policies,"Of the varying motivations underlying research in economics, forecasting has proved to be the most discomfiting. It is not so much that economists are not good at it; it is more that we do not have any firm idea if we are bad at it. We do not yet have a consensus on how to judge the matter. What I shall therefore do in this note is avoid even trying to make a prediction of which way we will be moving in economics research. Instead, I will outline the kind of work I would like to see come to fruition in the field in which my current research interest lies; namely, the economics of destitution. We need a redirection in how we go about our research in this area. My motivation here springs from an unease I have over methodological matters in current studies of poverty in poor countries. In what follows, I will first present in abstract terms how I think we are going wrong. I will then exemplify this with a key example. Collapse"
106982,1991,economic journal,economics in 2090: the views of an experimentalist,economics in 2090 the views of an experimentalist,No Result.
106984,1991,economic journal,game theory as a part of empirical economics,game theory as a part of empirical economics,"A large part of economics, and economic theory in particular, relies on such solution concepts as Nash equilibrium and its refinements. Unfortunately, it is difficult to provide a solid theoretical or empirical justification for Nash equilibrium behaviour. ‘Rationality’, or even ‘common knowledge of rationality’, is not enough to generate such behaviour. Among other things, one also needs to assume that the players coordinate their beliefs about each other’s actions (Armbruster and Boege, 1979; Johansen, 1982; Bernheim, 1984; Pearce, 1984; Bernheim, 1986; Binmore, 1987; Aumann, 1987; Tan and Werlang, 1988; Aumann and Brandenburger, 1991). Moreover, in games with a dynamic structure, the very notion of rationality becomes problematic, and common knowledge of rationality may even lead to logical contradictions (Rosenthal, 1981; Binmore, 1987; Bicchieri, 1989; Basu, 1988, 1990). Collapse"
106987,1991,economic journal,another century of economic science,another century of economic science,"There is a widespread consensus that during the past century, economic science has come of age. It has developed powerful statistical tools to analyse economic data, to make forecasts, and to test alternative hypotheses, and it has employed sophisticated mathematical techniques to articulate its theories and to prove basic theorems characterising the economy. Samuelson (1947) encapsulated the mid-century enthusiasm for the scientific method in his classic Foundations of Economic Analysis, arguing that economics should be based on observable behaviour and testable hypotheses, and, with his theory of revealed preference, showing how this could be done with the theory of consumers' behaviour.^ Collapse"
106991,1991,economic journal,the interface between environmental and trade policies,the interface between environmental and trade policies,"This paper argues that the achievement of sustainable development requires, inter alia, the development of a framework of appropriate 'tools' to aid project, programme and policy development and implementation. Such a framework could usefully be called 'sustainability analysis'. It is likely to comprise a harmonised range of existing methodologies, some modified to address changing requirements (eg, modified EIA procedures to effectively include social, participatory and economic issues in order to address the key links between environmental impact and sustainable development), and some new elements (eg, indicators of sustainability). The need for sustainability analysis and particularly for indicators of sustainability is a key requirement to implement and monitor the development of national sustainable development plans, as required by Agenda 21 agreed at UNCED in June 1992. 1. EIA A Tool in Need of Overhaul Since its introduction in 1969 in the USA, Environmental Impact Assessment (EIA) has become widely used as a tool for project analysis. An increasing number of countries are introducing legislation which requires EIAs for certain categories of development. In line with this trend, bilateral and multilateral donors have introduced procedures, manuals and guidelines for the environmental assessment of projects. In 1985, the OECD introduced recommendations for its members on the environmental assessment of development assistance projects and programmes, followed in 1986 by recommended measures to facilitate such EIAs. Principles for project appraisal were developed in 1988 by the OECD Development Assistance Committee (DAC) and recommendations were agreed by the OECD Council in 1989 concerning an environmental checklist for high-level decision-makers in aid agencies. The essence of these various recommendations has been incorporated within the new DAC guidelines for ""Good Practice for the EIA of Development Projects"" (OECD, 1991). The World Bank has also published recently a very useful three-volume Environmental Assessment Sourcebook (World Bank, 1991). However, despite the increasing adoption and formalisation of EIA procedures internationally, EIA is still seen by many principally as a technical instrument, and it is wrongly assumed by some to be useful only for application at a relatively advanced stage in the project cycle, or as a disasteror crisismanagement tool. This is unfortunate because the potential of EIA is really as an 'attitude' and as a process which can be applied to all projects and programmes throughout their development and implementation and, with modification or adaptation, to policies. EIA has suffered a bad image amongst many of those involved in promoting sustainable development. This is because the tool has been used inappropriately or on limited components of the sustainable development system (see Figure 1). Those who have employed EIA have rarely integrated successfully environmental, social and economic issues the three most vital components of the equation. In Figure 1, sustainable development takes place within the central interactive zone (shaded) between the economic, the environmental/biological and the social/cultural systems. It is subject to a continual process of trade-offs between these systems. Intuitively, it is evident that development cannot be sustainable if one of these systems is not incorporated. In the main, current EIA practice identifies and estimates the environmental impacts of development projects in physical terms. Techniques of social cost-benefit analysis (SCBA) are sometimes included in an EIA to widen its terms of reference. These are aimed at assessing the costs and benefits (including Figure 1: The Sustainable Development System INTERFACE EXAMPLES OF ANALYTICAL TOOL EXAMPLES OF PLANNING PROCESSES A Traditional EIA National Conservation Strategies Sectoral Natural Resource Strategies B Social Cost Benefit Analysis Traditional Development Plans C Some Human Ecology & Anthropology Studies UNESCO Man and the Biosphere Programme D Sustainability Analysis (modified EIA) National Sustainable Development Plans/Strategies Note: This diagram is illustrative only and is not intended to imply that all EIA and SCBA studies are narrowly focussed. Whilst current EIA approaches are focussed primarily at the interface between the environmental and economic systems (eg, the impact of enterprises on habitat), it is recognised that other forms of EIA have incorporated some social dimensions. environmental impacts) in monetary terms as far as possible, and in qualitative terms where this is not possible. But SCBA techniques have largely been developed independently of EIA, although the demarcation lines between them are often blurred. As Lee (1991) points out, ""some SCBA studies have to ignore environmental impacts or express them in physical units, where monetary measures are unavailable"", whilst some EIA studies include ""employment and other socio-economic impacts Alongside the environmental impacts"". Whilst SCBA has been used as a valid tool for assessing the social and economic 'worth' of project or policies, the quantification of the costs and benefits of environmental impacts is often (some might say usually) not undertaken, mainly due to the insufficiency of required data and to continuing controversy over some of the techniques available. However, it is not always easy to compare environmental, social and economic impacts. Lee (1991) quite correctly asks, ""how should the overall environmental impact of development be evaluated or how can the aggregate environmental impact be compared with the economic and social impacts when reaching a decision on a project ?"". It is argued by some that the task is greatly simplified by expressing environmental impacts in monetary terms whilst others find monetary valuation unacceptable and argue that other means are required. The prominence that this debate has now achieved arises due to several factors, including recent advances in the methodology of monetary valuation (e.g., OECD, 1989; Pearce et.al.,1989; Winpenny, 1991) and official encouragement for the use of monetary valuation (World Bank, 1991; Department of the Environment, 1991). Admittedly, EIAs carried out in the last few years what we might call 'new generation' EIAs are giving more attention to social issues. But, whilst public consultation is now a recognised norm in full EIAs, EIAs are rarely conducted in a truly participatory way, and project beneficiaries are seldom involved adequately in the process. Although EIAs are being extended to include measures of the damage costs associated with adverse environmental impacts, comprehensive monetary valuation and economic analysis of environmental costs and benefits are seldom included (Barbier, et.al., 1991). There is, however, a widening recognition of the urgent need to modify EIA approaches, to place greater emphasis on its practical application, and to address the links between EIA and sustainable development. It has to recognised, however, that EIA alone cannot make development sustainable although ""it can help push decisionmakers along the path towards sustainability"" (Clarke, 1991). EIA will be an important tool in the 'kit bag' required to back up the development of national sustainable development plans. 2. National Sustainable Development Plans One of the main outcomes of UNCED was ratification of Agenda 21 which set priorities for achieving sustainable development. Real action, particularly in the context of international conventions and the priorities contained in agenda 21, takes place at the local and national level. This is where there is knowledge, concern, involvement and the capacity to act. This is where the focus of strategic planning processes should lie. It is a major encouragement, therefore, that Agenda 21 rightly recommends that translation of its priorities for action be focussed at the national level through sustainable development plans. Planning for sustainable development at the national level is nothing new, of course. There are many different approaches, many of which are advocated by different aid agencies in different contexts (see Box 1). Box 1: Some examples of national planning approaches § National Conservation Strategies, led by IUCN; § National Environmental Action Plans, led by the World Bank; § National Tropical Forest Action Plans, led by FAO; § National Plans to Combat Desertification, led by UNEP/UNSO; § National Energy Assessments, led by the World Bank; § Economic-cum-environmental development planning, led by the Asian Development Bank; § A variety of environmental strategies and country environment profiles prepared by bilateral aid donors; and § The UNCED national reports on sustainable development. To this list may shortly be added the ""national plans"" that will arise out of the international climate and biodiversity It can be assumed that the proponents of each of the various approaches listed in Box 2 will be promoting them vigorously as 'the way' towards sustainable development planning. Some of the initiatives listed have enjoyed considerable success. A number of them aim to deal with the wide range of issues implicit in the concept of sustainable development. But again, it needs to be recognised that most are weak on environmental economics and whilst they usually purport to be participatory and country-oriented, they tend, in fact, to be driven largely by outside agencies (Sandbrook et.al.,1992). Many of the approaches overlap in subject and geographical scope, and some have been duplicated in the same country. An analysis of the documents on any one country arising from these national initiatives indicates that they have used essentially the same data, but rarely have the results of any public consultations been included. Their preparation usually involves foreign consultants talking to the same people in the governments who, in turn, have to meet conflicti Collapse"
107099,1991,economica,inward versus outward growth orientation in the presence of country risk,inward versus outward growth orientation in the presence of country risk,"The purpose of this paper is to model the role of trade dependency in determining the access of a developing economy to the international credit market, and its desirable growth strategy. With full integration of capital markets the choice with respect to the inwardness of a technology is irrelevant: investment will be channeled to the more productive sectors, independently of their trade inwardness, With limited capital market integration a given investment will generate two effects. The first is the standard, direct productivity effect that is associated with the change in future output. The second is the trade dependency externality, generated by the change in future bargaining outcomes due to the change in the trade dependency of the nation. With partial integration, investment that increases trade dependency is desirable. If the credit markets are disjoint due to partial defaults, higher trade dependency is disadvantageous. Thus, higher trade dependency generates a positive externality with partial integration of capital markets, and a negative externality with disjoint credit markets. We show that credit market integration is determined by the size of the indebtedness relative to the trade dependency, as reflected by the repayment burden that is supported by the bargaining outcome. The repayment bargaining outcome is determined by the sectoral composition of the economy and by the effective size of the developing and the developed economies. Collapse"
107101,1991,economica,does long-term unemployment reduce a person's chance of a job?  a time-series test,does long term unemployment reduce a person s chance of a job   a time series test,"Long-term unemployed are less likely to leave unemployment than short-term unemployed. To what extent is this due to an effect of duration and to what extent to heterogeneity? On reasonable assumptions, heterogeneity on its own (with no duration-dependence) would imply that, if over time the exit rate of new entrants fell by a given multiple, the overall exit rate from unemployment should fall by the same multiple. However, over the last twenty years in the UK the overall exit rate has fallen by 60 per cent more than the fall in the exit rate of new entrants. Another test of 'pure heterogeneity', based on more general assumptions, also fails. Hence we explain the overall fall in exit rates from unemployment by the combined effect of (1) a fall in the ratio of vacancies to unemployed, and (2) a higher proportion of the unemployed being long-term unemployed, and hence demoralized and stigmatized in the eyes of employers. Collapse"
107127,1991,energy economics,election returns as a signal of changing regulatory climate,election returns as a signal of changing regulatory climate,"Abstract This paper considers the effects of changes in regulatory climate on the expected future profitability of regulated electric utilities. Previous researchers addressing issues relating to regulatory environment have examined the determinants of investor perceptions of regulatory climate, the effect that the method of commissioner selection in particular has on regulatory climate, and the effects of regulatory climate on the cost of capital to the regulated firm. This paper seeks to ascertain the importance of regulatory climate by examining the stock market performance of regulated electric utilities at the time of an unanticipated change in regulatory environment. These results indicate that in those states in which the change in regulatory climate was most clearly unanticipated, an adverse change in the regulatory environment did result in negative abnormal returns to utilities operating in those states, thereby indicating a reduction in the expected future profitability of the regulated firms. In addition our results provide support for the proposition that the firm's systematic risk is lower when enforcement of regulation is more stringent. Collapse"
107151,1991,energy journal,the place of economics in decommissioning policy,the place of economics in decommissioning policy,No Result.
107154,1991,energy journal,federal regulation of decommissioning economics,federal regulation of decommissioning economics,"As we move deeply into the new territory of large-scale decommissioning, countries around the world are looking to the United States for guidance and experience in establishing their own approaches. In the U.S., the Nuclear Regulatory Commission is the lead agency responsible for public health and safety issues linked to commercial nuclear power. This responsibility includes assuring adequate funds for decommissioning. In this chapter, Robert Wood introduces us to the regulations and positions of the NRC regarding decommissioning financing. The issues include why the NRC chose external funding mechanisms, how the funds should be collected and invested, the relationship between the NRC and state agencies, and fund assurance in a variety of cases including possible accident and bankruptcy. While this discussion will serve as an overview of the most significant aspects of decommissioning financing, it also introduces us to other chapters which focus on the relationships of the NRC with the states and electric utility companies. Collapse"
107160,1991,energy journal,estimating the costs for japan's jpdr project,estimating the costs for japans jpdr project,No Result.
107162,1991,energy journal,a private contractor's approach to decommissioning costs,a private contractors approach to decommissioning costs,No Result.
107165,1991,energy journal,managing qualified nuclear decommissioning trust funds under uncertainty,managing qualified nuclear decommissioning trust funds under uncertainty,"Funds for the eventual decommissioning and removal of nuclear power plants are accumulating. The amount will total many tens, perhaps hundreds of billions, of dollars. One of the ingredients in setting aside these funds is managing them so as to assure that just enough cash is on hand at the time of decommissioning to meet all required expenses at the lowest possible net present value cost to utility ratepayers. As with any investment, there can be a variety of opinions. For this reason, it is important to consult several sources for advice on the investment of such nuclear decommissioning trust funds (NDTs). The next three chapters provide such advice from the perspective of three different firms. The first, by Howard Hiller, stresses the importance of an adaptive approach, pointing out that the most difficult question is the choice of investment maturities. Hiller employs a simulation methodology to quantify the riskcost characteristics of strategies along the maturity spectrum--from short to long-term. He identifies some of the unique uncertainties inherent in decommissioning and brings these uncertainties into his analysis. He concludes that the steepness of the municipal yield curve can be exploited even in the presence of inflationary uncertainty. Collapse"
107170,1991,energy journal,a cost/benefit perspective of extended unit service as a decommissioning alternative,a costbenefit perspective of extended unit service as a decommissioning alternative,"Some people consider life extension (and its cousin, license renewal) an alternative to decommissioning. The reasons for the popularity of such alternatives include presumed cost effectiveness, retention of scarce power plant sites, and the continued ability to pass on waste storage expenses as a cost of service. In this chapter, James Hewlett addresses nuclear power plant life extension- -which he calls NUPLEX--in its economic garb, starting with a look at the common utility presumption that life extension of a nuclear plant will allow it to produce electricity at a lower rate than new coal generation. This presumption, he argues, may not be supportable by analysis. He concludes that the deferral of constructing new replacement capacity would result in cost savings only if both the level and escalation rate of the operating costs for the refurbished unit fall substantially from 1986 levels. Therefore, it is unclear whether the deferral of the construction of new capacity would result in the cost savings, although it definitely shifts the financial burden into the future. Collapse"
107172,1991,energy journal,financial implications of early decommissioning,financial implications of early decommissioning,"There are three generalized timing possibilities for decommissioning: at the end of the original operating license, after some period of life extension, or sometime before the end of normal service. In this companion piece to his discussion of life extension in Chapter 20, James Hewlett addresses the third option. Premature decommissioning can arise--as it did for Unit 2 of Three Mile Island--from an accident, or--more commonly-because it may be cheaper to close the facility than to have it continue in operation. The economic implication of premature closure is one of potentially insufficient fund accumulation. As Hewlett points out, the decision to close a plant is complicated by decommissioning considerations; for example, the decision could depend in part on the status of the accumulated funds. Such a decision also can influence plans for new construction since it may delay the time of closure for existing plants. State regulatory bodies in the United States influence such decisions through their control over rates of return. In the end, decommissioning cost considerations often influence decommissioning timing. Collapse"
107307,1991,inquiry,the role of local hospitals in physician rural location decisions,the role of local hospitals in physician rural location decisions,"This study has two objectives. First, it identifies how much the presence of community hospital beds adds to the stock of physicians practicing in nonmetropolitan counties. Second, it estimates the impact hospital closures or other reductions in beds have on the net flow of physicians into rural counties. The study relies primarily on data from the 1981 and 1986 Physician Masterfiles of the American Medical Association. We find that hospital bed reductions and closures essentially do not affect the availability of physicians--they have apparently already left by the time the downsizing occurs. Further, population change and the number of physicians already in the county determine for the most part the entry or exit of physicians. Collapse"
107308,1991,inquiry,trends in length of stay and rates of readmission in massachusetts: implications for monitoring quality of care,trends in length of stay and rates of readmission in massachusetts implications for monitoring quality of care,"In this study, we examined lengths of stay and readmission rates for all Medicare patients discharged from Massachusetts acute care hospitals from October 1982 through September 1986. Using multivariate time series models, we controlled for case mix and assessed trends over time and the impact of prospective payment on lengths of stay and rates of readmission within 7, 14, and 30 days of discharge. We examined patterns for patients overall and for those admitted initially with one of several specific medical conditions or for a surgical procedure. Over the four years, lengths of stay decreased by 25% overall and by 12% to 38% for the individual conditions studied (all p less than .05). A small part of this decrease was associated with prospective payment. Overall readmission rates within 7 and 14 days increased by approximately 10% (p less than .05), although the increase was not statistically associated with prospective payment. Readmission rates for individual medical and surgical conditions were not significantly changed. Further study should assess whether the change in overall rates reflects lower quality care. Collapse"
107309,1991,inquiry,the relationship between quality and cost: pure and simple?,the relationship between quality and cost pure and simple,"This study focuses on the relationship between the quality and the cost of hospital care. I report the findings of variable cost regressions that include outcome indicators of hospital quality: risk-adjusted mortality and readmission indices. Hospital level data are aggregated from a modified 1985 Medicare Provider Analysis File (MEDPAR) and the Commission of Professional and Hospital Activities. Results show several quality measures to be statistically significant determinants of cost. With each measure, the cost-quality relationship is nonlinear but not monotonically increasing throughout the entire range of quality. Collapse"
107311,1991,inquiry,a simultaneous equations model of employer strategies for controlling health benefit costs,a simultaneous equations model of employer strategies for controlling health benefit costs,"We estimated a simultaneous equations model of employer health insurance cost control strategies and their effectiveness in reducing health plan premiums. We hypothesized that as premiums increase, employers will shop more actively for health plans, place incentives on providers to control medical care costs, increase employee cost sharing for medical care, and be more likely to offer an HMO. In turn, we expected each of these strategies, except offering an HMO, to reduce average health plan premiums. The model was estimated with 1985 data from a sample of 922 Minnesota employers. We found that high premiums are related to three of the proposed cost control strategies. Employers with higher premiums shop more, are more likely to seek provider incentives, and offer an HMO. However, these employers appear to have lower employee cost sharing. Employers with higher employee cost sharing and those that offer an HMO had lower health plan premiums. Collapse"
107312,1991,inquiry,"the marginal effect of bond insurance on hospital, tax-exempt bond yields",the marginal effect of bond insurance on hospital  tax exempt bond yields,"In response to changes in the health care environment and the tax-exempt bond market, many hospitals have purchased bond insurance and other forms of credit enhancement to lower the yields on their debt financings. This study of tax-exempt revenue bonds issued by hospitals from 1982-84 estimates that bond insurance lowers yields on hospital bonds by approximately 87 basis points and that bond insurance serves as a substitute measure of creditworthiness. The findings also suggest that the insured group of hospital bonds is more homogeneous than the uninsured group in terms of characteristics that affect the risks associated with hospital investments. Insured bonds seem to represent hospitals in an intermediate risk group. Collapse"
107313,1991,inquiry,how profitable is medical malpractice insurance?,how profitable is medical malpractice insurance,"High medical malpractice insurance premiums charged to physicians have created the perception of a crisis. Some researchers have expressed the concern that medical malpractice insurance companies are making excessive profits. In this paper I compute the actual and allowable normal underwriting profit rates in medical malpractice, as well as in other liability lines, for six large insurance companies. These allowable, normal profit rates are then compared with actual profit rates to evaluate the relative profitability of each line of insurance. Data from 1978 through 1986 show that medical malpractice insurance ranked medium in underwriting profitability compared with other lines of insurance, and during 1985-86 it was the least profitable insurance business. Collapse"
107315,1991,inquiry,comparing hospital length of stay in independent practice association hmos and traditional insurance programs,comparing hospital length of stay in independent practice association hmos and traditional insurance programs,"This study compares length of hospital stay in Independent Practice Association (IPA) HMOs and traditional insurance programs. Hospital admissions from 10 IPAs are compared with admissions to the same hospital of persons covered by Blue Cross and Blue Shield Plans or commercial insurance programs. Admissions of patients under age 65 to the adult medical service for the 10 most frequently occurring DRGs are included. Regression equations are estimated using length of stay as the dependent variable and IPA membership and hospital and patient characteristics as control variables. All 10 IPAs exhibit shorter lengths of stay as indicated by negative beta coefficients, and in 6 of the 10 IPAs this coefficient is statistically significant (p less than .05). This IPA effect occurs for 7 of the 10 study DRGs, and for MedisGroups Admission Severity Groups 0, 1, and 2. Collapse"
107339,1991,international journal of industrial organization,regulatory reform and liberalization: a european perspective,regulatory reform and liberalization a european perspective,"The reform of European railroads is a time-consuming process strongly charac-terized by its path-dependency. Firstly, a short outline of the historical roots of the controversial debates on the role of the state and the markets, and the organi-zation of competition in European railroad industries is provided. Secondly, the opening of the market for train services in the context of the liberalization of European transport markets since 1985 is characterized and the regulatory pre-conditions for competition on the tracks are presented. Thirdly, the evolution of track access regulation in Europe during the last decades is analyzed, differenti-ating between the period of negotiated third party access since 1991, the intro-duction of ex ante regulation by the first railroad infrastructure package in 2001, and the danger of overregulation posed by the recent Draft Directive of July 2012 establishing a single European railway area. Fourthly, the role of competi-tion on the markets for rail services and the reform process of interoperability requirements are considered. Finally, competition on the markets for rail ser-vices and public subsidies for rail infrastructures as well as subsidies for train services are evaluated. Collapse"
107340,1991,international journal of industrial organization,auction design and favoritism,auction design and favoritism,No Result.
107342,1991,international journal of industrial organization,does ownership always matter?,does ownership always matter,Abstract The objective of this paper is to explain and illustrate the complex relationship between ownership arrangements and enterprise performance. It is commonly argued that efficiency will be lower in the public sector than the private because enterprise objectives deviate from maximisation of profits and because monitoring arrangements are inadequate due to the absence of capital market discipline. We argue that public ownership does make the owner-manager relationship more complicated because the chain of principals and agents is expanded; objectives are politically determined; and these are conveyed by a policy-making administrative structure to management. But the relative efficiency of public as against private ownership actually depends on the efficacy of capital market monitoring: on the political and constitutional system; on the information and sanctions available to policy makers; and on the nature of the management market. Variation in these factors can help to explain the different natures and roles of the public sector between countries. Collapse
107343,1991,international journal of industrial organization,the theory and measure of cross-subsidies: an application to the telecommunications industry,the theory and measure of cross subsidies  an application to the telecommunications industry,"Abstract The paper is made of two sections, the first one dealing with cross-subsidies in general and the second one referring specifically to the telecommunications industry. In the first section, we attempt to clarify the relationship between the theoretical approach to cross-subsidies, as it derives on the one hand from Faulhaber's seminal definition and the corpus of cooperative game theory, and from the accounting approach on the other hand, which consists in computing revenue trade-offs across types of products or types of customers. We emphasize the conditions under which a revenue trade-off signals a cross-subsidy in Faulhaber's sense. We finally show how revenue trade-offs across consumers during a price transition are first order approximations of welfare variations. In the second section, telecommunications are chosen as an application case. We first show that most of the issues presently at stake in this industry can helpfully be analysed in terms of cross-subsidies. We then examine the principles and methods which are utilized for costing and pricing by telecommunications companies in Canada and the United States. Finally, we present the results of a study assessing the revenue trade-offs across categories of services and across customer segments in the French telecommunications market. Collapse"
107344,1991,international journal of industrial organization,liberalizing european airlines: cost and factor productivity evidence,liberalizing european airlines cost and factor productivity evidence,"Abstract European countries do not all share the same view concerning the benefits of increased competition among airlines in Europe. To explain this difference in attitudes, the paper presents evidence on differences in costs and global factor productivity between the main European national flag carriers. The data reveal a clear distinction between the main European and North Atlantic routes. Unit costs per passenger-kilometer are lower in North Atlantic routes and unit profits are higher in European routes. The first result reflects the economies of scale due to higher distances and traffic density on North Atlantic routes. The second result is an indication of the lower level of competition between airlines in Europe. Finally, Tornqvist factor productivity growth indexes (1981–1986) show that the initial gap which existed between these productivity measures at the beginning of the period is shrinking. This suggests that the European airline industry is currently in a more favorable situation to support more competition. Collapse"
107361,1991,international organization,"can orthodox stabilization and adjustment work? lessons from new zealand, 1984-90",can orthodox stabilization and adjustment work  lessons from new zealand  1984 90,"Most debate about the efficacy of orthodox stabilization programs, such as those of the International Monetary Fund, has been fruitless. First, rarely are these programs fully implemented or sustained for long periods. Second, defenders and critics of the programs hold differing premises about the nature of capitalist economies. The debate is therefore not about the appropriate balance of supply-and demand-side measures but, rather, about what sort of supply- and demand-side measures will address the supply- and demand-side problems that each group perceives. The results of an orthodox stabilization program which incorporated demand- and supply-side elements and which was fully implemented and sustained by the New Zealand government from 1984 to 1990 reveal the limits to orthodox programs. New Zealand, a primary product exporter, suffers from a structural imbalance of payments and from an external debt burden equal in scale to that of the Latin American and other highly indebted less developed countries (LDCs), but it does not have the serious supplyside constraints on growth that critics claim typify underdeveloped economies. This makes New Zealand an appropriate test of the typical orthodox stabilization program. Despite the fact that its administrative capacity, political will, domestic support, and access to external resources were far in excess of those of the typical would-be LDC stabilizer, New Zealand achieved only a precarious macroeconomic and international payments stability. Moreover, as the case of New Zealand demonstrates, inflation control and financial liberalization policy components of orthodox plans have contradictory consequences for payments balance. This suggests that long-term stabilization, in New Zealand and elsewhere, cannot be achieved solely by internal reforms. Collapse"
107391,1991,journal of the american statistical association,multiple imputation of industry and occupation codes in census public-use samples using bayesian logistic regression,multiple imputation of industry and occupation codes in census public use samples using bayesian logistic regression,"Abstract We describe methods used to create a new Census data base that can be used to study comparability of industry and occupation classification systems. This project represents the most extensive application of multiple imputation to date, and the modeling effort was considerable as well—hundreds of logistic regressions were estimated. One goal of this article is to summarize the strategies used in the project so that researchers can better understand how the new data bases were created. Another goal is to show how modifications of maximum likelihood methods were made for the modeling and imputation phases of the project. To multiply-impute 1980 census-comparable codes for industries and occupations in two 1970 census public-use samples, logistic regression models were estimated with flattening constants. For many of the regression models considered, the data were too sparse to support conventional maximum likelihood analysis, so some alternative had to be employed. These methods solve existence and ... Collapse"
107392,1991,journal of the american statistical association,generalized linear models with random effects;  a gibbs sampling approach,generalized linear models with random effects a gibbs sampling approach,"1 Fractals and Multifractals: The Interplay of Physics and Geometry (With 30 Figures).- 1.1 Introduction.- 1.2 Nonrandom Fractals.- 1.3 Random Fractals: The Unbiased Random Walk.- 1.4 The Concept of a Characteristic Length.- 1.5 Functional Equations and Fractal Dimension.- 1.6 An Archetype: Diffusion Limited Aggregation.- 1.7 DLA: Fractal Properties.- 1.8 DLA: Multifractal Properties.- 1.8.1 General Considerations.- 1.8.2 ""Phase Transition"" in 2d DLA.- 1.8.3 The Void-Channel Model of 2d DLA Growth.- 1.8.4 Multifractal Scaling of 3d DLA.- 1.9 Scaling Properties of the Perimeter of 2d DLA: The ""Glove"" Algorithm.- 1.9.1 Determination of the l Perimeter.- 1.9.2 The l Gloves.- 1.9.3 Necks and Lagoons.- 1.10 Multiscaling.- 1.11 The DLA Skeleton.- 1.12 Applications of DLA to Fluid Mechanics.- 1.12.1 Archetype 1: The Ising Model and Its Variants.- 1.12.2 Archetype 2: Random Percolation and Its Variants.- 1.12.3 Archetype 3: The Laplace Equation and Its Variants.- 1.13 Applications of DLA to Dendritic Growth.- 1.13.1 Fluid Models of Dendritic Growth.- 1.13.2 Noise Reduction.- 1.13.3 Dendritic Solid Patterns: ""Snow Crystals"".- 1.13.4 Dendritic Solid Patterns: Growth of NH4Br.- 1.14 Other Fractal Dimensions.- 1.14.1 The Fractal Dimension dw of a Random Walk.- 1.14.2 The Fractal Dimension dmin ? 1/?? of the Minimum Path.- 1.14.3 Fractal Geometry of the Critical Path: ""Volatile Fractals"".- 1.15 Surfaces and Interfaces.- 1.15.1 Self-Similar Structures.- 1.15.2 Self-Affine Structures.- 1.A Appendix: Analogies Between Thermodynamics and Multifractal Scaling.- References.- 2 Percolation I (With 24 Figures).- 2.1 Introduction.- 2.2 Percolation as a Critical Phenomenon.- 2.3 Structural Properties.- 2.4 Exact Results.- 2.4.1 One-Dimensional Systems.- 2.4.2 The Cayley Tree.- 2.5 Scaling Theory.- 2.5.1 Scaling in the Infinite Lattice.- 2.5.2 Crossover Phenomena.- 2.5.3 Finite-Size Effects.- 2.6 Related Percolation Problems.- 2.6.1 Epidemics and Forest Fires.- 2.6.2 Kinetic Gelation.- 2.6.3 Branched Polymers.- 2.6.4 Invasion Percolation.- 2.6.5 Directed Percolation.- 2.7 Numerical Approaches.- 2.7.1 Hoshen-Kopelman Method.- 2.7.2 Leath Method.- 2.7.3 Ziff Method.- 2.8 Theoretical Approaches.- 2.8.1 Deterministic Fractal Models.- 2.8.2 Series Expansion.- 2.8.3 Small-Cell Renormalization.- 2.8.4 Potts Model, Field Theory, and ? Expansion.- 2.A Appendix: The Generating Function Method.- References.- 3 Percolation II (With 20 Figures).- 3.1 Introduction.- 3.2 Anomalous Transport in Fractals.- 3.2.1 Normal Transport in Ordinary Lattices.- 3.2.2 Transport in Fractal Substrates.- 3.3 Transport in Percolation Clusters.- 3.3.1 Diffusion in the Infinite Cluster.- 3.3.2 Diffusion in the Percolation System.- 3.3.3 Conductivity in the Percolation System.- 3.3.4 Transport in Two-Component Systems.- 3.3.5 Elasticity in Two-Component Systems.- 3.4 Fractons.- 3.4.1 Elasticity.- 3.4.2 Vibrations of the Infinite Cluster.- 3.4.3 Vibrations in the Percolation System.- 3.4.4 Quantum Percolation.- 3.5 ac Transport.- 3.5.1 Lattice-Gas Model.- 3.5.2 Equivalent Circuit Model.- 3.6 Dynamical Exponents.- 3.6.1 Rigorous Bounds.- 3.6.2 Numerical Methods.- 3.6.3 Series Expansion and Renormalization Methods.- 3.6.4 Continuum Percolation.- 3.6.5 Summary of Transport Exponents.- 3.7 Multifractals.- 3.7.1 Voltage Distribution.- 3.7.2 Random Walks on Percolation.- 3.8 Related Transport Problems.- 3.8.1 Biased Diffusion.- 3.8.2 Dynamic Percolation.- 3.8.3 The Dynamic Structure Model of Ionic Glasses.- 3.8.4 Trapping and Diffusion Controlled Reactions.- References.- 4 Fractal Growth (With 4 Figures).- 4.1 Introduction.- 4.2 Fractals and Multifractals.- 4.3 Growth Models.- 4.3.1 Eden Model.- 4.3.2 Percolation.- 4.3.3 Invasion Percolation.- 4.4 Laplacian Growth Model.- 4.4.1 Diffusion Limited Aggregation.- 4.4.2 Dielectric Breakdown Model.- 4.4.3 Viscous Fingering.- 4.4.4 Biological Growth Phenomena.- 4.5 Aggregation in Percolating Systems.- 4.5.1 Computer Simulations.- 4.5.2 Viscous Fingers Experiments.- 4.5.3 Exact Results on Model Fractals.- 4.5.4 Crossover to Homogeneous Behavior.- 4.6 Crossover in Dielectric Breakdown with Cutoffs.- 4.7 Is Growth Multifractal?.- 4.8 Conclusion.- References.- 5 Fractures (With 18 Figures).- 5.1 Introduction.- 5.2 Some Basic Notions of Elasticity and Fracture.- 5.2.1 Phenomenological Description.- 5.2.2 Elastic Equations of Motion.- 5.3 Fracture as a Growth Model.- 5.3.1 Formulation as a Moving Boundary Condition Problem.- 5.3.2 Linear Stability Analysis.- 5.4 Modelisation of Fracture on a Lattice.- 5.4.1 Lattice Models.- 5.4.2 Equations and Their Boundary Conditions.- 5.4.3 Connectivity.- 5.4.4 The Breaking Rule.- 5.4.5 The Breaking of a Bond.- 5.4.6 Summary.- 5.5 Deterministic Growth of a Fractal Crack.- 5.6 Scaling Laws of the Fracture of Heterogeneous Media.- 5.7 Hydraulic Fracture.- 5.8 Conclusion.- References.- 6 Transport Across Irregular Interfaces: Fractal Electrodes, Membranes and Catalysts (With 8 Figures).- 6.1 Introduction.- 6.2 The Electrode Problem and the Constant Phase Angle Conjecture.- 6.3 The Diffusion Impedance and the Measurement of the Minkowski-Bouligand Exterior Dimension.- 6.4 The Generalized Modified Sierpinski Electrode.- 6.5 A General Formulation of Laplacian Transfer Across Irregular Surfaces.- 6.6 Electrodes, Roots, Lungs,.- 6.7 Fractal Catalysts.- 6.8 Summary.- References.- 7 Fractal Surfaces and Interfaces (With 27 Figures).- 7.1 Introduction.- 7.2 Rough Surfaces of Solids.- 7.2.1 Self-Affine Description of Rough Surfaces.- 7.2.2 Growing Rough Surfaces: The Dynamic Scaling Hypothesis.- 7.2.3 Deposition and Deposition Models.- 7.2.4 Fractures.- 7.3 Diffusion Fronts: Natural Fractal Interfaces in Solids.- 7.3.1 Diffusion Fronts of Noninteracting Particles.- 7.3.2 Diffusion Fronts in d = 3.- 7.3.3 Diffusion Fronts of Interacting Particles.- 7.3.4 Fluctuations in Diffusion Fronts.- 7.4 Fractal Fluid-Fluid Interfaces.- 7.4.1 Viscous Fingering.- 7.4.2 Multiphase Flow in Porous Media.- 7.5 Membranes and Tethered Surfaces.- 7.6 Conclusions.- References.- 8 Fractals and Experiments (With 18 Figures).- 8.1 Introduction.- 8.2 Growth Experiments: How to Make a Fractal.- 8.2.1 The Generic DLA Model.- 8.2.2 Dielectric Breakdown.- 8.2.3 Electrodeposition.- 8.2.4 Viscous Fingering.- 8.2.5 Invasion Percolation.- 8.2.6 Colloidal Aggregation.- 8.3 Structure Experiments: How to Determine the Fractal Dimension.- 8.3.1 Image Analysis.- 8.3.2 Scattering Experiments.- 8.3.3 Sacttering Formalism.- 8.4 Physical Properties.- 8.4.1 Mechanical Properties.- 8.4.2 Thermal Properties.- 8.5 Outlook.- References.- 9 Cellular Automata (With 6 Figures).- 9.1 Introduction.- 9.2 A Simple Example.- 9.3 The Kauffman Model.- 9.4 Classification of Cellular Automata.- 9.5 Recent Biologically Motivated Developments.- 9.A Appendix.- 9.A.1 Q2R Approximation for Ising Models.- 9.A.2 Immunologically Motivated Cellular Automata.- 9.A.3 Hydrodynamic Cellular Automata.- References.- 10 Exactly Self-similar Left-sided Multifractals with new Appendices B and C by Rudolf H. Riedi and Benoit B. Mandelbrot (With 10 Figures).- 10.1 Introduction.- 10.1.1 Two Distinct Meanings of Multifractality.- 10.1.2 ""Anomalies"".- 10.2 Nonrandom Multifractals with an Infinite Base.- 10.3 Left-sided Multifractality with Exponential Decay of Smallest Probability.- 10.4 A Gradual Crossover from Restricted to Left-sided Multifractals.- 10.5 Pre-asymptotics.- 10.5.1 Sampling of Multiplicatively Generated Measures by a Random Walk.- 10.5.2 An ""Effective"" f(?).- 10.6 Miscellaneous Remarks.- 10.7 Summary.- 10.A Details of Calculations and Further Discussions.- 10.A.1 Solution of (10.2).- 10.A.2 The Case ?min = 0.- 10.B Multifractal Formalism for Infinite Multinomial Measures, by R.H. Riedi and B.B. Mandelbrot.- 10.C The Minkowski Measure and Its Left-sided f(?), by B.B. Mandelbrot.- 10.C.1 The Minkowski Measure on the Interval [0,1].- 10.C.2 The Functions f(?) and f?(?) of the Minkowski Measure.- 10.C.3 Remark: On Continuous Models as Approximations, and on ""Thermodynamics"".- 10.C.4 Remark on the Role of the Minkowski Measure in the Study of Dynamical Systems. Parabolic Versus Hyperbolic Systems.- 10.C.5 In Lieu of Conclusion.- References. Collapse"
107393,1991,journal of the american statistical association,"semiparametric estimation in the rasch model and related exponential response models, including a simple latent class model for item analysis",semiparametric estimation in the rasch model and related exponential response models including a simple latent class model for item analysis,"Abstract The Rasch model for item analysis is an important member of the class of exponential response models in which the number of nuisance parameters increases with the number of subjects, leading to the failure of the usual likelihood methodology. Both conditional-likelihood methods and mixture-model techniques have been used to circumvent these problems. In this article, we show that these seemingly unrelated analyses are in fact closely linked to each other, despite dramatic structural differences between the classes of models implied by each approach. We show that the finite-mixture model for J dichotomous items having T latent classes gives the same estimates of item parameters as conditional likelihood on a set whose probability approaches one if T ≥ (J + 1)/2. Unconditional maximum likelihood estimators for the finite-mixture model can be viewed as Keifer-Wolfowitz estimators for the random-effects version of the Rasch model. Latent-class versions of the model are especially attractive when T is... Collapse"
107394,1991,journal of the american statistical association,a nonparametric method for dealing with mismeasured covariate data,a nonparametric method for dealing with mismeasured covariate data,"Abstract Mismeasurement of covariate data is a frequent problem in statistical data analysis. However, when true and mismeasured data are obtained for a subsample of the observations, it is possible to estimate the parameters relating the outcome to the covariate of interest. Maximum likelihood methods that rely on parametric models for the mismeasurement have not met with much success. Realistic models for the mismeasurement process are difficult to construct; the form of the likelihood is often intractable and, more important, such methods are not robust to model misspecification. We propose an easily implemented method that is nonparametric with respect to the mismeasurement process and that is applicable when mismeasurement is due to the problem of missing data, errors in variables, or use of imperfect surrogate covariates. Specifically, denote the outcome variable by Y, the covariate data subject to mismeasurement by X, and the remaining covariates, including perhaps surrogates or mismeasured values ... Collapse"
107396,1991,journal of the american statistical association,nonparametric estimation from cross-sectional survival data,nonparametric estimation from cross sectional survival data,"Abstract In many follow-up studies survival data are often observed according to a cross-sectional sampling scheme. Data of this type are subject to left truncation in addition to the usual right censoring. A number of characteristics and properties of the product-limit estimate, for left-truncated and right-censored data, have been explored and found to be similar to those of the Kaplan-Meier estimate. Under the stationarity assumption, however, it is believed that an alternative estimate has much better efficiency. In this article the conditional maximum likelihood estimate (MLE) property of the product-limit estimate is visited. The non-parametric MLE of the truncation distribution is derived. Use of this estimate includes testing the stationarity assumption, estimating the proportion of truncated data, and other applications in prevalent cohort studies. The analysis of the estimation is based on a “working data” approach. The asymptotic properties of the proposed estimates are developed through nonpar... Collapse"
107398,1991,journal of the american statistical association,a general method for approximating tail probabilities,a general method for approximating tail probabilities,"Abstract Gray and Wang's (1991) general method for approximating tail probabilities is applied to the cases of the noncentral χ 2 , F and t distributions. The validity of such applications is established. The resulting approximations are easy to compute. Numerical results show the great accuracy of the approximations for all the three most commonly used noncentral distributions. Collapse"
107399,1991,journal of the american statistical association,on weak convergence of an estimator of the survival function when new is better than used of a specified age,on weak convergence of an estimator of the survival function when new is better than used of a specified age,"Abstract A survival function S is said to be in the New Better than Used of age t 0 (NBU-t 0) class if S(x + t 0) ≤ S(t 0)S(x) for all x ≥ 0. Reneau and Samaniego proposed an estimator Ŝ n for S when S is known as a member of the NBU-t 0 class. Many properties of Ŝ n were studied by Reneau and Samaniego. The problems of the weak convergence of Wn = √n (Ŝ n − S), however, was not solved. In this article, we show that the weak convergence of Wn does not hold in general and establish sufficient conditions for the weak convergence to hold. Three important cases are as follows: (1) If the underlying survival function S is from the subclass of New Strictly Better than Used of age t 0 (e.g., gamma and Weibull distributions), then Wn converges weakly with the same limiting distribution as that of the empirical process. In this case, confidence bands for S can be easily constructed. (2) If S is from the subclass of New is the Same as Used of age t 0 (e.g., exponential distributions), then the weak convergence of W... Collapse"
107401,1991,journal of the american statistical association,recent developments in nonparametric density estimation,recent developments in nonparametric density estimation,"Advances in computation and the fast and cheap computational facilities now available to statisticians have had a significant impact upon statistical research, and especially the development of nonparametric data analysis procedures. In particular, theoretical and applied research on nonparametric density estimation has had a noticeable influence on related topics, such as nonparametric regression, nonparametric discrimination, and nonparametric pattern recognition. This article reviews recent developments in nonparametric density estimation and includes topics that have been omitted from review articles and books on the subject. The early density estimation methods, such as the histogram, kernel estimators, and orthogonal series estimators are still very popular, and recent research on them is described. Different types of restricted maximum likelihood density estimators, including order-restricted estimators, maximum penalized likelihood estimators, and sieve estimators, are discussed, where restrictions are imposed upon the class of densities or on the form of the likelihood function. Nonparametric density estimators that are data-adaptive and lead to locally smoothed estimators are also discussed; these include variable partition histograms, estimators based on statistically equivalent blocks, nearest-neighbor estimators, variable kernel estimators, and adaptive kernel estimators. For the multivariate case, extensions of methods of univariate density estimation are usually straightforward but can be computationally expensive. A method of multivariate density estimation that did not spring from a univariate generalization is described, namely, projection pursuit density estimation, in which both dimensionality reduction and density estimation can be pursued at the same time. Finally, some areas of related research are mentioned, such as nonparametric estimation of functionals of a density, robust parametric estimation, semiparametric models, and density estimation for censored and incomplete data, directional and spherical data, and density estimation for dependent sequences of observations. Collapse"
107430,1991,journal of business and economic statistics,estimation and seasonal adjustment of population means using data from repeated surveys:  comment,estimation and seasonal adjustment of population means using data from repeated surveys comment,"I consider estimation and seasonal adjustment of population means based on rotating panel surveys carried out at regular time intervals. The analysis uses a dynamic structural model that assumes a decomposition of the mean into a trend-level component and a seasonal component. The model accounts for the correlations between individual panel estimators and for possible rotation group effects. It can be applied to general rotation patterns using either the individual panel estimates or the aggregate sample estimates, depending on data availability. Empirical results illustrating the important features of the procedure are presented. Collapse"
